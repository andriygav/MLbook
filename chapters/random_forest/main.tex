\section*{Случайный лес}

\subsection*{Что такое случайный лес}
Случайный лес - это алгоритм машинного обучения, заключающийся в улучшении идеи бэггинга решающих деревьев путем уменьшение количества признаков в расщепляемой подвыборке. Основная идея заключается в использовании большого ансамбля решающих деревьев, что позволяет уменьшить дисперсию, но случайные леса обеспечивают улучшение по сравнению с баггингом деревьев благодаря случайному небольшому изменению, которое декоррелирует деревья.  

Как и в баггинге, мы строим множество деревьев решений на бутстрепированных обучающих выборках. Однако при построении деревьев решений каждый раз, когда рассматривается расщепление в дереве, случайным образом выбирается подмножество из \(k\) признаков из полного набора из \(n\) признаков. Расщепление допускается только по одному из этих \(k\) признаков. Для каждого расщепления производится новый случайный выбор \(k\) признаков, и обычно выбирается \(k \approx \sqrt{n}\) для задач классификации и \(k \approx n/3\) для задач регрессии. Иными словами, при построении случайного леса на каждом шаге алгоритму запрещено учитывать большинство доступных признаков.

К сожалению, усреднение большого количества сильно коррелированных величин не приводит к значительному снижению дисперсии по сравнению с усреднением большого количества некоррелированных величин. Случайные леса решают эту проблему, заставляя каждое разделение учитывать только подмножество признаков. Таким образом, в среднем доля разделений, которые вообще не будут учитывать сильный признак, составляет  
\[
\frac{p - m}{p}.
\]  
Это даёт шанс другим признакам быть использованными. Такой процесс можно рассматривать как декорреляцию деревьев, что делает среднее предсказаний полученных деревьев менее вариативным и, следовательно, более надёжным.

\subsection*{Алгоритм обучения и параметры модели}
\subsubsection*{Настраиваемые параметры модели}
\begin{itemize}
    \item Число T деревьев
    \item Число k случайно выбираемых признаков
    \item Максимальная глубина деревьев max depth
    \item Минимальное число объектов в расщепляемой подвыборке
    \item Минимальное число объектов в листьях
    \item Критерий расщепления
\end{itemize}

\subsubsection*{Алгоритм построения случайного леса}

\begin{enumerate}
    \item \textbf{Инициализация параметров (T, k, max depth, min samples plit, min samples leaf)}

    \item \textbf{Обучение деревьев:}
    \begin{enumerate}
        \item Для каждого \(i \in \{1, 2, \dots, T\}\):
        \begin{enumerate}
            \item Сформировать обучающую выборку \(D_i\) путём бутстрепа (случайная подвыборка с возвратом из исходного набора данных).
            \item Построить дерево решений на \(D_i\):
            \begin{itemize}
                \item На каждом шаге разделения узла случайным образом выбрать \(k\) признаков из общего числа \(n\).
                \item Найти лучший признак и точку разделения среди выбранных \(n\) признаков, оптимизируя критерий разделения (например, индекс Джини для классификации или дисперсию для регрессии).
                \item Повторять, пока не будет достигнут критерий остановки (например, максимальная глубина дерева или минимальное число объектов в узле).
            \end{itemize}
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Предсказание:}
    \begin{enumerate}
        \item Для нового объекта \(x\):
        \begin{itemize}
            \item Провести \(x\) через каждое дерево \(T_i\), \(i \in \{1, 2, \dots, B\}\), и получить предсказания \(y_i\).
            \item Объединить предсказания:
            \begin{itemize}
                \item Для задач классификации: итоговый класс определяется голосованием большинства:
                \[
                \hat{y} = \text{argmax}_k \sum_{i=1}^T \mathbb{I}(y_i = k),
                \]
                где \(\mathbb{I}(\cdot)\) — индикатор принадлежности классу \(k\).
                \item Для задач регрессии: итоговое предсказание вычисляется как среднее:
                \[
                \hat{y} = \frac{1}{T} \sum_{i=1}^T y_i.
                \]
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{enumerate}


\subsection*{OOB и оценка важности признаков}
Оказывается, существует очень простой способ оценить ошибку теста для модели, использующей метод бутстрэппинга (случайный лес), без необходимости выполнять кросс-валидацию. Можно показать, что в среднем каждое дерево, созданное методом бутстрэппинга, использует около двух третей всех наблюдений (см. задачу далее). Оставшаяся треть наблюдений, которая не использовалась для подгонки конкретного дерева, называется вневыборочными (out-of-bag, OOB) наблюдениями.

Мы можем предсказать ответ для i-го объекта, используя каждое из деревьев, где этот объект был вне выборочной совокупности. Это даст примерно T/3 прогнозов для i-го объекта. Чтобы получить одно предсказание для i-го объекта, мы можем усреднить эти предсказанные ответы (если целью является регрессия) или принять решение большинством голосов (если цель – классификация). Это приводит к одному вневыборочному предсказанию для i-го объекта. Вневыборочное предсказание может быть получено таким образом для каждого из N объектов, на основе которых затем можно вычислить общую среднеквадратичную ошибку (MSE) вне выборочных данных (для задачи регрессии) или ошибку классификации (для задачи классификации). Полученная ошибка вне выборочных данных является достоверной оценкой ошибки тестирования для модели, построенной методом бутстрэппинга, поскольку ответ для каждого объекта предсказывается только теми деревьями, которые не были построены с использованием этого объекта.

Хотя совокупность деревьев, созданных методом бутстрэппинга, гораздо сложнее интерпретировать, чем одно дерево, можно получить общее представление об относительной важности каждого признака, используя остаточную сумму квадратов (RSS) (в случае использования деревьев регрессии) или индекс Джини (Gini index) (в случае использования деревьев классификации). В случае деревьев регрессии мы можем записать общее уменьшение RSS, вызванное разбиением по данному признаку, усредненное по всем T деревьям. Большое значение указывает на важный признак. Аналогично, в контексте деревьев классификации мы можем сложить общее уменьшение индекса Джини, вызванное разбиениями по данному признаку, усреднённое по всем T деревьям.


\subsection*{Преимущества и недостатки}
\subsubsection*{Преимущества:}
\begin{itemize}
    \item Метод обертка над базовым методом обучения
    \item Подходит для классификации, регрессии и других задач
    \item Простая реализация и простое распараллеливание
    \item Возможность получения несмещённых оценок OOB
\end{itemize}
\subsubsection*{Недостатки:}
\begin{itemize}
    \item Требуется очень много базовых алгоритмов
    \item Трудно агрегировать устойчивые базовые методы обучения
\end{itemize}
\subsection*{Задачи}

\subsubsection*{Задача 1.} 

Пусть $N \rightarrow\infty$. Оцените аналитически количество объектов, которое не будет входить в выборку после процедуры бутстрэпа. 

\textbf{Решение:}

Бутстрэп выбирает объект рандомно из выборки и заменяет его своей копией, оставляя изначальную выборку неизменной. Такой выбор проводится $N$ раз, создавая выборку из $N$ объектов, некоторые из которых могут повторяться. Таким образом, вероятность выбора каждого объекта составляет $\dfrac{1}{N}$, а вероятность не быть выбранным соответственно $1 - $\dfrac{1}{N}$. Таким образом, после N выборов мы имеем:

$$\lim_{N\rightarrow\infty} (1-\frac{1}{N})^N = \frac{1}{e}$$

Таким образом, бутстрэп игнорирует больше чем $\frac{N}{3}$ объектов в выборке, что позволяет использовать методы OOB.

\textbf{Ответ:} $\dfrac{N}{e}$

\subsubsection*{Задача 2.} 

Как известно, случайный лес позволяет оценить важность признаков. Что произойдет с важностью признака, если добавить в датасет его точную копию, т.е. к множеству всех признаков добавляется ещё один, являющийся точной копией другого (т.е. принимает те же значения)?

\textbf{Решение:}

Важность признаков оценивается путем измерения их вклада в снижение дисперсии (в случае регрессионных задач) или показателя неопределенности (например индекса Джини в случае классификационных задач). Если добавить точную копию признака, то оба этих признака будут иметь одинаковую информацию, поэтому они будут одинаково полезны для разделения данных. Поскольку они будут абсолютно скоррелированы, случайный лес не сможет различить, какой из них лучше и в результате важность будет равномерно распределяться между этими двумя признаками. То есть вместо того чтобы один признак имел полную значимость, теперь у вас будет два признака с половинной значимостью каждый. Т.е. важность отдельного признака уменьшится.


\textbf{Ответ:} Уменьшится

\subsubsection*{Задача 3.}

Предположим, у вас есть искусственный датасет для классификации с $n = 10$ признаками и $N = 1000$ объектов. Датасет генерируется следующим образом:

$$
y = \begin{cases}
1 & \text{если } x1 + x2 > 1.5 \\
0 & \text{иначе}
\end{cases}
$$

Признаки $x_1$ и $x_2$ сгенерированы равномерно в диапазоне $[0, 1]$.

Остальные признаки ($x_3$ – $x_{10}$) генерируются из нормального распределения $N(0, 1)$.

\begin{enumerate}
\item  Какой вклад в предсказания случайного леса будут вносить шумовые признаки 
($x_3$ – $x_{10}$) по сравнению с информативными ($x_1$ и $x_2$)? Обоснуйте ваш ответ.

\item Как изменится качество модели, если:
\begin{enumerate}
    \item Увеличить число деревьев в случайном лесе?
    \item Уменьшить число признаков, рассматриваемых при разбиении (\(\text{max\_features}\))?
\end{enumerate}

\item Что изменится, если y будет определяться как
$x_1 \cdot x_2 > 0.5$,
а не:
$x_1 + x_2 > 1.5$?

Как случайный лес будет справляться с такой задачей?

\end{enumerate}

\textbf{Решение:}

\begin{enumerate}

\item Шумовые признаки не несут полезной информации для предсказания класса. Благодаря большому количеству деревьев и процессу агрегации результатов, влияние шумовых признаков должно уменьшаться. То есть даже если отдельные деревья используют шумовые признаки, итоговая модель, основанная на голосовании множества деревьев, должна давать правильный результат, опираясь преимущественно на информативные признаки ($x1$ и $x2$). Следовательно, вклад шумовых признаков в итоговую модель будет незначительным.

\item (а) Увеличение количества деревьев в случайном лесе обычно ведет к улучшению качества модели, так как увеличивается разнообразие моделей и уменьшается вариация ошибок. Таким образом, увеличение числа деревьев обычно повышает стабильность и точность модели. Но стоит учесть, что прирост точности начинает замедляться после достижения определенного числа деревьев.

    (б) Параметр max\_features определяет максимальное количество признаков, которое рассматривается при каждом разбиении узла дерева. Чем меньше это значение, тем сильнее снижается сложность модели и уменьшается риск переобучения. С другой стороны, слишком маленькое значение может привести к потере важной информации и ухудшению качества модели.
    
    В нашей задаче, если уменьшить max\_features, то модель может начать чаще выбирать шумовые признаки, что снизит ее эффективность. Особенно критичным это станет, если max\_features окажется равным 1, тогда модель будет строить деревья, основываясь исключительно на одном признаке, который может оказаться шумовым.

\item Теперь давайте рассмотрим ситуацию, когда правило определения $y$ меняется на следующее:

$$
y = \begin{cases}
1 & \text{если } x1 \cdot x2 > 0.5 \\
0 & \text{иначе}
\end{cases}
$$

Это условие нелинейно. Случайный лесу намного труднее справляться со сложными нелинейными границами, потому что каждый отдельный решающий узел в дереве случайного леса делает бинарное разделение по одному признаку. Т.о., для разделения пространства по условию $x1 \cdot x2 > 0.5$ потребуется несколько таких узлов, что сделает дерево глубоким и сложным.

Но несмотря на это, случайный лес всё равно способен приблизиться к правильной границе, особенно если построить много деревьев. Тем не менее, его производительность может ухудшаться по сравнению с предыдущим условием.

\end{enumerate}

\section{Критерии качества кластеризации}

Давайте детально разберем основные метрики, используемые для оценки качества кластеризации данных.  

Выбор подходящей метрики напрямую зависит от наличия или отсутствия предварительной разметки данных и от того, задано ли количество кластеров априори или оно подбирается в процессе кластеризации.

\subsection{Критерии, не требующие разметки выборки}

\subsubsection{Среднее внутрикластерное расстояние} \hfill\\

Название этой метрики говорит само за себя: она отражает среднее расстояние между всеми парами точек внутри одного кластера.  Иными словами, мы подсчитываем сумму расстояний между всеми парами точек в каждом кластере и делим на общее количество таких пар.  

Формула метрики выглядит так:
\begin{equation*}
     F_0 = \cfrac{\displaystyle\sum_{i=1}^n \sum_{j=i}^n \rho(x_i,  x_j) I[a(  x_i)=a(x_j)]}{\displaystyle\sum_{i=1}^n \sum _ {j=i}^n I[ a(x_i)=a(x_j)]}.
\end{equation*}

В формуле учитываются и пары вида $(x_i, x_i)$, что позволяет избежать неопределенности $\frac{0}{0}$ в случае, если кластер состоит всего из одной точки.  Однако, иногда для упрощения вычислений суммирование ведется только по парам $(x_i, x_j)$, где $i < j$, при этом для случая одноточечных кластеров значение метрики полагается равным нулю.

Вычисление этого критерия достаточно трудоёмко, поэтому можно также ввести средний квадрат внутрикластерного расстояния, если нам известные представители, или центры масс, кластеров $\mu_k$:
\begin{equation*}
     \Phi_0 = \displaystyle\frac{1}{nK} \sum_{k=1}^K \sum_{i=1}^n \rho(\mu_k,  x_i)^2 I[a(x_i)=k].
\end{equation*}

Наша цель при кластеризации -- получить максимально компактные кластеры, поэтому мы стремимся минимизировать значение этой метрики.  Чем меньше среднее внутрикластерное расстояние, тем лучше.

\subsubsection{Среднее межкластерное расстояние} \hfill\\

В отличие от предыдущей метрики, среднее межкластерное расстояние оценивает среднее расстояние между точками из разных кластеров.  

Формула выглядит следующим образом:
\begin{equation*}
     F_1 = \cfrac{\displaystyle\sum_{i=1}^n \sum_{j=i}^n \rho(x_i,  x_j) I[a(  x_i)\ne a(x_j)]}{\displaystyle\sum_{i=1}^n \sum _ {j=i}^n I[ a(x_i)\ne a(x_j)]}.
\end{equation*}

Здесь, наоборот, мы стремимся к максимизации этого значения.  Чем больше расстояние между кластерами, тем лучше разделение.  

\subsection{Критерии, требующие разметки выборки}

Следующие метрики требуют, чтобы мы заранее знали, к какому классу принадлежит каждый объект в наборе данных.  Это позволяет сравнить результаты кластеризации с истинным распределением данных.

Мы будем обозначать кластеры, полученные в результате кластеризации, как $k \in \{1, \ldots, K\}$, а истинные классы -- как $c \in \{1, \ldots, C\}$.

\subsubsection{Гомогенность} \hfill\\

Если у нас есть разметка, то можно свести задачу кластеризации к использованию методов классификации. Если размеченных данных достаточно много, то обучение классификатора -- более подходящий подход. Однако часто возникает ситуация, когда данных достаточно для оценки качества кластеризации, но всё ещё не хватает для использования методов обучения с учителем.

Пусть $n$ -- общее количество объектов в выборке, $n_k$ -- количество объектов в кластере номер $k$, $m_c$ -- количество объектов в классе номер $c$, а $n_{c,k}$ количество объектов из класса $c$ в кластере $k$. Рассмотрим следующие величины:
\begin{gather*}
    H_{class} = -\displaystyle\sum_{c=1}^C \cfrac{m_c}{n} \log\cfrac{m_c}{n}, \\
    H_{clust} = -\displaystyle\sum_{k=1}^K \cfrac{n_k}{n} \log\cfrac{n_k}{n}, \\
    H_{class \vert clust} = -\displaystyle\sum_{c=1}^C \sum_{k=1}^K \cfrac{n_{c,k}}{n} \log\cfrac{n_{c,k}}{n_k}.
\end{gather*}

Несложно заметить, что эти величины соответствуют формуле энтропии и условной энтропии для мультиномиальных распределений $\cfrac{m_c}{n}, \cfrac{n_k}{n}, \cfrac{n_{c,k}}{n_k}$ соответственно.

Гомогенность кластеризации определяется такой формулой:
\begin{equation*}
    Homogeneity = 1 - \cfrac{H_{class \vert clust}}{H_{class}}.
\end{equation*}

Отношение $\cfrac{H_{class \vert clust}}{H_{class}}$ показывает, насколько уменьшается неопределенность в распределении классов (измеряемая энтропией), если мы знаем, к какому кластеру относится каждый объект. 

Худший сценарий -- когда отношение равно единице, то есть знание о кластерной принадлежности никак не помогает определить истинный класс объекта (энтропия не изменилась). 

Лучший случай -- когда каждый кластер содержит только объекты одного класса, и, следовательно, зная номер кластера, мы точно знаем истинный класс (гомогенность равна 1). Заметим, что тривиальный (и бессмысленный) способ добиться максимальной гомогенности -- это выделить каждый объект в отдельный кластер.

\subsubsection{Полнота} \hfill\\

Метрика полноты аналогична гомогенности, но использует условную энтропию $H_{clust \vert class}$, которая симметрична $H_{class \vert clust}$:
\begin{equation*}
    Completeness = 1 - \cfrac{H_{clust \vert class}}{H_{clust}}.
\end{equation*}

Полнота равна единице, когда все объекты, принадлежащие одному и тому же истинному классу, находятся в одном и том же кластере.

Тривиальный, но непрактичный способ получить максимальную полноту -- это объединить все объекты выборки в один большой кластер.

\subsubsection{V-мера} \hfill\\

Метрики гомогенности и полноты в кластеризации аналогичны точности и полноте в классификации.  V-мера, в свою очередь, аналогична F-мере и представляет собой гармоническое среднее гомогенности и полноты. Пусть $\beta$ - весовой параметр, тогда формула выглядит следующим образом:
\begin{equation*}
    V_\beta = \cfrac{(1+\beta) \cdot Homogeneity \cdot Completeness}{\beta \cdot Homogeneity + Completeness}.
\end{equation*}

В случае $\beta = 1$ получаем, что $V_1$-мера является просто средним гармоническим гомогенности и полноты:
\begin{equation*}
    V_\beta = \cfrac{2 \cdot Homogeneity \cdot Completeness}{Homogeneity + Completeness}.
\end{equation*}

Использование V-меры позволяет избежать тривиальных решений, таких как присвоение каждого объекта в отдельный кластер (максимальная гомогенность) или объединение всех объектов в один кластер (максимальная полнота).  V-мера обеспечивает сбалансированную оценку качества кластеризации, учитывая как гомогенность, так и полноту.

\subsubsection{Коэффициент силуэта} \hfill\\

Коэффициент силуэта — метрика качества кластеризации, не требующая наличия меток классов. Сначала он вычисляется для каждого объекта, а затем итоговая метрика для всей выборки определяется как среднее значение коэффициентов силуэта всех объектов.

Для вычисления коэффициента силуэта $S(x_i)$ нужны две величины:

\begin{itemize}
    \item $A(x_i)$ — среднее расстояние от объекта $x_i$ до всех других объектов в том же кластере.
    \item $B(x_i)$ — среднее расстояние от объекта $x_i$ до объектов ближайшего соседнего кластера.
\end{itemize}

Сам коэффициент силуэта вычисляется по формуле:
\begin{equation*}
    S(x_i) = \cfrac{B(x_i)-A(x_i)}{\max (B(x_i), A(x_i))}.
\end{equation*}

В идеале, точки внутри кластера должны быть ближе друг к другу, чем к точкам ближайшего соседнего кластера $A(x_i) < B(x_i)$. Однако это не всегда так.  Например, если кластер сильно вытянут или большой, а рядом находится небольшой кластер, то среднее расстояние до точек своего кластера ($A(x_i)$) может оказаться больше, чем до точек соседнего ($B(x_i)$).  Поэтому разность $B(x_i) - A(x_i)$ может быть как положительной, так и отрицательной, хотя в идеале ожидается положительное значение.  Коэффициент силуэта $S(x_i)$, изменяющийся от -1 до +1, максимизируется, когда кластеры компактны и хорошо разделены.

Главное преимущество коэффициента силуэта — он не требует меток классов и позволяет оценивать качество кластеризации при разных количествах кластеров.

\subsection{Различия и выбор метрик качества кластеризации}

Выбор метрики качества кластеризации зависит от нескольких факторов. Если число кластеров известно и разметка данных отсутствует, то целесообразно использовать среднее внутрикластерное или среднее межкластерное расстояние для оптимизации качества кластеризации. 

Если же доступна разметка данных, то для оценки качества можно использовать гомогенность и полноту, а V-мера, сочетающая эти две метрики, позволяет также подбирать оптимальное число кластеров.

В случае отсутствия разметки и неизвестного числа кластеров, коэффициент силуэта является наиболее подходящей метрикой на практике. Исключение составляет ситуация, когда результаты кластеризации используются как промежуточный этап в более сложной задаче. В таких случаях качество кластеризации оценивается косвенно, по качеству решения конечной задачи, и выбор алгоритма кластеризации и его параметров подчиняется этой цели.

\subsection{Задачи на понимание}
\subsubsection{Задача 1}

Представьте, что у вас есть два набора данных с одинаковым средним внутрикластерным расстоянием. Может ли это означать, что качество кластеризации в обоих наборах одинаково? Объясните, почему да или нет.

\subsubsection{Ответ}

Нет, одинаковое среднее внутрикластерное расстояние не гарантирует одинаковое качество кластеризации. Эта метрика отражает только компактность кластеров, игнорируя другие важные аспекты, такие как разделение между кластерами, форма кластеров и наличие выбросов. Например, в одном наборе данных кластеры могут быть компактными и хорошо разделены, а в другом -- компактными, но сильно перекрывающимися. Среднее внутрикластерное расстояние будет одинаковым, но качество кластеризации -- разным.

\subsubsection{Задача 2}

У вас есть данные, где границы между кластерами размыты, и некоторые точки могут принадлежать нескольким кластерам одновременно. Какая метрика качества кластеризации наименее подходит для оценки результатов в этом случае, и почему?

\subsubsection{Ответ}

Метрики, основанные на жестком распределении точек по кластерам (например, среднее внутрикластерное расстояние, среднее межкластерное расстояние), наименее подходят. Это связано с тем, что они предполагают, что каждая точка строго принадлежит только одному кластеру. В случае нечетких кластеров более подходящими могут быть метрики, учитывающие степень принадлежности точки к каждому кластеру.

\subsubsection{Задача 3}

Опишите сценарий, в котором высокая гомогенность, но низкая полнота. И наоборот.

\subsubsection{Ответ}

Высокая гомогенность, низкая полнота. Представим, что у нас есть два истинных класса A и B. Алгоритм кластеризации создал много маленьких кластеров, каждый из которых содержит преимущественно объекты одного класса (высокая гомогенность). Однако объекты одного и того же класса (например, класса A) разбросаны по множеству разных кластеров. В этом случае полнота будет низкой, так как объекты одного класса не собраны вместе.

Высокая полнота, низкая гомогенность. В этом случае объекты одного класса сгруппированы в одном кластере (высокая полнота). Однако этот кластер содержит значительное количество объектов из других классов, что приводит к низкой гомогенности. Например, один большой кластер может содержать значительное количество объектов класса A и меньшее -- класса B. Полнота для класса A высокая, а гомогенность низкая, потому что кластер "загрязнен" объектами класса B.

\section{$K$--means++}

Алгоритм $K$ средних не всегда хорошо справляется с кластеризацией при неудачном выборе начальных точек.
К примеру, даже при наличии трёх хорошо разделимых равномощных групп точек если две начальные начальные точки были выбраны в одной группе,
а третья --- в любой из двух оставшихся, то алгоритм кластеризует группу, в которую попало две начальные точки, в два класса, а две другие группы объединит в один класс.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{poor_kmeans_clustering.png}
\end{figure}

Во избежание подобных случаев предлагается подбирать начальные точки не случайно, а по некоторому алгоритму.

\subsection{Алгоритм}

\textbf{$K$--means++} --- алгоритм кластеризации, оптимизирующий выбор начальных точек для алгоритма $K$ средних.

Интуитивно, чем более удалены друг от друга начальные точки, тем менее вероятен <<конфликт классов>>, описанный выше. Тогда реализуем буквально эту логику.

Выберем первую начальную точку произвольным образом. Важно, чтобы она лежала в границах датасета, поэтому предлагается
в качестве первой начальную точки выбрать случайную из датасета.

Далее будем итеративно добавлять в множество $\mu$ начальные точки, пока не наберём требуемое количество $K$.
Каждую новую точку будем определять как $\arg\max_{x\in X} \rho(x, \mu)$ --- точку из датасета, наиболее удалённую от всех ранее выбранных точек.

После выбора $K$ начальных точек запускаем классический алгоритм $K$ средних.

\subsection{Задачи на понимание}

\textbf{Задача 1.} Какие ограничения алгоритма $K$--средних присутствуют и у алгоритма $K$--means++? Появляются ли новые ограничения? Какие?

\textit{\textbf{Подсказка:} Рассмотрите датасет для кластеризации <<два полумесяца>>.
Что можно сказать о характере распознаваемых алгоритмом кластеров?
Как поведёт себя алгоритм, если в данных будут явные выбросы или фон?
Какой алгоритм в таком случае даст лучший результат: <<классический>> алгоритм $K$ средних или описанная модификация с подбором начальных точек?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{twoo_half_moons.png}
\end{figure}

\textbf{Задача 2.} Оцените асимптотику подбора начальных точек. Сравните её с асимптотикой шага алгоритма $K$ средних.
Что будет, если производить выбор начальных точек, ограничившись случайным подмножеством датасета?
Какую долю датасета вы бы использовали для поиска начальных точек? В каких случаях и почему?

\textbf{Задача 3.} Предположим, что при работе с зашумленными данными студент
решил вместо наиболее удалённых точек датасета брать $p$--квантиль ($p\in [0, 1]$).
Будет ли его идея работать при некотором $p$ (снизится ли влияние шумовых точек на работу алгоритма)?
Будет ли выбор точек "портится" с каждой итерацией алгоритма? Как следует менять $p$ на каждой итерации?
Предложите студенту альтернативные методы решения проблемы. В чём их достоинства/недостатки?

\section{Алгоритм k-средних}

Алгоритм k-средних (k-means) — это метод машинного обучения, предназначенный для решения задачи кластеризации. Он относится к неиерархическим и итеративным методам кластеризации, благодаря чему получил широкую популярность за счет своей простоты, визуальной понятности и хорошего качества работы. Алгоритм был разработан в 1950-х годах математиком Гуго Штейнгаузом(и почти одновременно Стюартом Ллойдом) и представляет собой разновидность EM-алгоритма. 

Суть данного алгоритма заключается в минимизации суммы квадратов внутрикластерных расстояний. 

\[
\sum_{i=1}^\ell \|x_i - \mu_{a_i}\|^2 \to \min_{\{a_i\}, \{\mu_a\}}
\]

\subsection{Математическое описание работы алгоритма}
\textbf{Обозначения}
\begin{itemize}
    \item \( X \) — пространство объектов
    \item \( X_\ell = \{ x_1, x_2, \ldots, x_\ell \} \) — обучающая выборка
    \item \( Y \) — множество идентификаторов (меток) кластеров, где каждая метка соответствует определённому кластеру, в который попадает объект.
    \item \( \rho(x, x') \) — евклидово расстояние между объектами.
\end{itemize}

\textbf{Алгоритм}
\begin{enumerate}
    \item Сформировать начальное приближение центров всех кластеров  $\mu_a$ для всех ${a \in Y}$.
    \item Повторять:
    \begin{enumerate}
        \item Отнести каждый объект к ближайшему центру:  
        \[
        a_i := \arg \min_{a \in Y} \rho(x_i, \mu_a), \quad i = 1, \ldots, \ell;
        \]
        \item Вычислить новое положение центров:  
        \[
        \mu_{a} := \frac{\sum_{i=1}^\ell [a_i = a] x_i}{\sum_{i=1}^\ell [a_i = a]}, 
        \quad a \in Y, \; 
        \]
    \end{enumerate}
    Пока $a_i$ не перестанут изменяться.
\end{enumerate}

\textbf{Модификация алгоритма 
при наличии размеченных объектов}

Если даны размеченные объекты $\{x_1, x_2, \ldots, x_k\}$, то алгоритм будет уже выглядеть по-другому:

\begin{enumerate}
    \item Сформировать начальное приближение центров всех кластеров  $\mu_a$ для всех ${a \in Y}$.
    \item Повторять:
    \begin{enumerate}
        \item Отнести \textbf{каждый $x_{i}$ из неразмеченных объектов} к ближайшему центру:  
        \[
        a_i := \arg \min_{a \in Y} \rho(x_i, \mu_a), \quad i = \textbf{k+1}, \ldots, \ell;
        \]
        \item Вычислить новое положение центров:  
        \[
        \mu_{a} := \frac{\sum_{i=1}^\ell [a_i = a] x_i}{\sum_{i=1}^\ell [a_i = a]}, 
        \quad a \in Y, \; 
        \]
    \end{enumerate}
    Пока $a_i$ не перестанут изменяться.
\end{enumerate}

\subsection{Недостатки алгоритма k-средних}
\begin{itemize}
    \item \textbf{Чувствительность к выбору начальных центров} \\
    Алгоритм k-means крайне чувствителен к выбору начальных приближений центров кластеров. Случайная инициализация центров на первом шаге может приводить к плохим результатам кластеризации. Для формирования более устойчивых начальных приближений предлагается выделять \( k \) наиболее удалённых точек выборки. Первые две точки выбираются по максимуму всех попарных расстояний, а каждая следующая точка выбирается так, чтобы расстояние от неё до ближайшей уже выделенной точки было максимальным. Однако даже с этой стратегией нет гарантии, что выбранные центры будут оптимальными.
    \item \textbf{Неправильное определение числа кластеров} \\
    Кластеризация может оказаться неадекватной, если число кластеров \( k \) будет изначально выбрано неверно. Стандартная рекомендация заключается в проведении кластеризации при различных значениях \( k \) и выборе того значения, при котором достигается резкое улучшение качества кластеризации по заданному функционалу. Однако этот процесс может быть трудоёмким и не всегда приводит к однозначным результатам, так как выбор оптимального числа кластеров часто требует дополнительного анализа и экспериментов.
\end{itemize}

\subsection{Задачи}
\textbf{Задача 1.} Дан набор точек в двумерном пространстве:
\[
X = \{(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)\}.
\]
Начальные центроиды кластеров заданы как:
\[
C = \{(1, 1), (10, 3)\}.
\]

Необходимо выполнить одну итерацию алгоритма \(k\)-means и найти координаты новых центров.

\textbf{Ответ. }Новые центроиды: \(C_0 = (1.0, 2.0)\), \(C_1 = (10.0, 2.0)\).

\textbf{Решение.}
Евклидово расстояние между точкой \(x = (x_1, x_2)\) и центроидом \(c = (c_1, c_2)\) вычисляется по формуле:
\[
d(x, c) = \sqrt{(x_1 - c_1)^2 + (x_2 - c_2)^2}.
\]
Для каждой точки из \(X\) вычислим расстояние до двух центроидов \((1, 1)\) и \((10, 3)\), чтобы определить, к какому кластеру она относится. Результаты расчетов приведены в таблице.

\[
\begin{array}{|c|c|c|c|}
\hline
\text{Точка } x & d(x, C_1 = (1, 1)) & d(x, C_2 = (10, 3)) & \text{Кластер} \\
\hline
(1, 2) & \sqrt{(1-1)^2 + (2-1)^2} = 1 & \sqrt{(1-10)^2 + (2-3)^2} \approx 9.05 & 0 \\
(1, 4) & \sqrt{(1-1)^2 + (4-1)^2} = 3 & \sqrt{(1-10)^2 + (4-3)^2} \approx 9.05 & 0 \\
(1, 0) & \sqrt{(1-1)^2 + (0-1)^2} = 1 & \sqrt{(1-10)^2 + (0-3)^2} \approx 9.49 & 0 \\
(10, 2) & \sqrt{(10-1)^2 + (2-1)^2} \approx 9.06 & \sqrt{(10-10)^2 + (2-3)^2} = 1 & 1 \\
(10, 4) & \sqrt{(10-1)^2 + (4-1)^2} \approx 9.49 & \sqrt{(10-10)^2 + (4-3)^2} = 1 & 1 \\
(10, 0) & \sqrt{(10-1)^2 + (0-1)^2} \approx 9.05 & \sqrt{(10-10)^2 + (0-3)^2} = 3 & 1 \\
\hline
\end{array}
\]

Центроид каждого кластера пересчитывается как среднее значение координат точек, принадлежащих этому кластеру. Для кластера \(k\) с точками \(x_1, x_2, \dots, x_n\):
\[
C_k = \left(\frac{1}{n} \sum_{i=1}^n x_{i1}, \frac{1}{n} \sum_{i=1}^n x_{i2}\right).
\]

Точки: \((1, 2), (1, 4), (1, 0)\).  
\[
C_0 = \left(\frac{1+1+1}{3}, \frac{2+4+0}{3}\right) = (1.0, 2.0).
\]
 
Точки: \((10, 2), (10, 4), (10, 0)\).  
\[
C_1 = \left(\frac{10+10+10}{3}, \frac{2+4+0}{3}\right) = (10.0, 2.0).
\]

\textbf{Задача 2.} 
Дан набор точек в двумерном пространстве:
\[
X = \{(3, 6), (3, 8), (4, 7), (7, 3), (7, 6), (8, 5), (8, 1), (9, 4)\}
\]
с размеченными объектами:
\[
y = \{0, 0, \text{неизвестно}, 1, \text{неизвестно}, 1, 1, \text{неизвестно}\}.
\]
Центроиды кластеров на начальный момент:
\[
C = \{(3, 7), (8, 5)\}.
\]

Необходимо провести одну итерацию алгоритма \(k\)-средних  и найти координаты новых центров.

\textbf{Ответ.}Новые центроиды: \(C_0 = (3.33, 7)\), \(C_1 = (7.8, 3.8)\).

\textbf{Решение}
Для точек с известными метками \(y = 0\) и \(y = 1\) (то есть точек \((3, 6), (3, 8), (7, 3), (7, 6), (8, 5)\)) уже известен их кластер.

Для точек с неизвестными метками, например, \((4, 7)\), \((8, 1)\), и \((9, 4)\), вычислим расстояния до центроидов и назначим ближайший кластер.

1. Для точки \((4, 7)\):
\[
\begin{aligned}
d((4, 7), (3, 7)) &= \sqrt{(4-3)^2 + (7-7)^2} = 1, \\ d((4, 7), (8, 5)) &= \sqrt{(4-8)^2 + (7-5)^2} = \sqrt{16 + 4} = \sqrt{20} \approx 4.47.
\end{aligned}
\]
Таким образом, точка \((4, 7)\) будет отнесена к кластеру 0.

2. Для точки \((8, 1)\):
\[
\begin{aligned}
d((8, 1), (3, 7)) &= \sqrt{(8-3)^2 + (1-7)^2} = \sqrt{25 + 36} = \sqrt{61} \approx 7.81, \\
d((8, 1), (8, 5)) &= \sqrt{(8-8)^2 + (1-5)^2} = 4.
\end{aligned}
\]
Таким образом, точка \((8, 1)\) будет отнесена к кластеру 1.

3. Для точки \((9, 4)\):
\[
\begin{aligned}
d((9, 4), (3, 7)) &= \sqrt{(9-3)^2 + (4-7)^2} = \sqrt{36 + 9} = \sqrt{45} \approx 6.71, \\
d((9, 4), (8, 5)) &= \sqrt{(9-8)^2 + (4-5)^2} = \sqrt{2} \approx 1.41.
\end{aligned}
\]
Таким образом, точка \((9, 4)\) будет отнесена к кластеру 1.

Теперь, имея все метки, можем определить распределение точек по кластерам:
\begin{itemize}
    \item Кластер \(0\): \((3, 6), (3, 8), (4, 7)\).
    \item Кластер \(1\): \((7, 3), (7, 6), (8, 5), (8, 1), (9, 4)\).
\end{itemize}
Пересчитываем центроиды для каждого кластера.

1. Для кластера 0 (точки \((3, 6), (3, 8), (4, 7)\)):
\[
C_0 = \left( \frac{3+3+4}{3}, \frac{6+8+7}{3} \right) = \left( \frac{10}{3}, \frac{21}{3} \right) = (3.33, 7).
\]

2.Для кластера 1 (точки \((7, 3), (7, 6), (8, 5), (8, 1), (9, 4)\)):
\[
C_1 = \left( \frac{7+7+8+8+9}{5}, \frac{3+6+5+1+4}{5} \right) = \left( \frac{39}{5}, \frac{19}{5} \right) = (7.8, 3.8).
\]

\textbf{Задача 3.}
Оцените асимптотическую сложность алгоритма \(k\)-средних, если:
\begin{itemize}
    \item \(n\) — количество точек данных,
    \item \(k\) — количество кластеров,
    \item \(d\) — размерность данных (количество признаков).
    \item \(N\) — размерность данных (количество признаков).
\end{itemize}

\textbf{Решение}

1. Шаг 1: Назначение точек кластерам.
   Для каждой точки нужно вычислить расстояние до каждого из \(k\) центроидов, что даёт сложность \(O(n \cdot k)\).

2. Шаг 2: Пересчёт центроидов.
   Для пересчёта центроида необходимо найти среднее значение всех точек в кластере. Для каждого кластера сложность составляет \(O(n \cdot d)\).

Итак, сложность одной итерации алгоритма \(k\)-средних: 
\[
O(n \cdot k \cdot d)
\]

Если алгоритм выполняет \(N\) итераций, общая сложность будет:
\[
O(N \cdot n \cdot k \cdot d)
\]
\textbf{Ответ.} $O(N \cdot n \cdot k \cdot d)$


\section{DBSCAN}

\textbf{DBSCAN} (Density-Based Spatial Clustering of Applications with Noise) - алгоритм кластеризации, решающий проблему сО сферичностью кластеров, он не делает никаких предположений о форме кластеров. Также он довольно быстрый и подходит для кластеризации больших данных.
\\
Он основан на понятии {\textit{окрестности}}.

\textbf{Определение 1.} Задан объект $x \in U$, его $\varepsilon$-окрестность $U_\varepsilon (x) = \{\;u\in U:\; \rho (x,u) \leq \varepsilon \;\}$ - это множество объектов, которые находятся на расстоянии не больше $\varepsilon$ от заданного объекта $x$.

\newcommand{\abs}[1]{\left|#1\right|}

Тогда каждый объект может быть отнесен к одному из трёх типов:
\begin{itemize}
    \item \textit{корневой}: имеющий плотную окрестность,  {$\lvert U_\varepsilon (x) \rvert \geq m$}, т.е. $\varepsilon$ содержит $\geq m$ объектов.
    \item \textit{граничный}: не корневой, но в окрестности корневого.
    \item \textit{шумовой (выброс)}: не корневой и не граничный.
\end{itemize}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{chapters/clustering/images/An-Example-Illustrating-the-Density-Based-DBSCAN-Clustering-Method-Applied-to-SMLM-Data.png}
    \caption{An Example Illustrating the Density-Based DBSCAN Clustering Method Applied to SMLM Data}
    \label{fig:enter-label-2}
\end{figure}
Возникает 2 параметра: $\varepsilon$ и $m$. Других параметров не будет. От этих параметров и будет зависеть то, какой картина кластеризации получится. Также к преимуществам этого метода относится то, что он не задает заранее количество кластеров, в отличие, например, от k-means, причём количество кластеров будет зависеть от $\varepsilon$ и $m$. 

Как работает алгоритм: берётся произвольная точка, если она имеет плотную окрестность, то дальше рассматривается каждая точка этой плотной окрестности, и вокруг неё также строится $\varepsilon$-окрестность, и так пока не будет достигнута граница некоторого множества объектов. 

Хорошей аналогией может служить лес: один лес - это один кластер, через опушку, второй лес, - другой кластер, мы находимся в лесу. Смотрим, в нашей окрестности деревьев много, это значит, что мы в корневой точке находимся, и дальше мы идём, пока не выйдем на опушку леса, там мы окажемся в граничной точке - она уже не корневая, вокруг деревьев меньше. А где-то могут расти отдельно стоящие деревья - это шумовые выбросы. И вот так ходим по лесу, пока его весь не обойдём, и как только мы обошли весь лес, назовем его кластером. После чего случайно выбираем новое дерево и начинаем строить другой кластер.

Формализуем алгоритм в виде псевдокода:\\
\begin{tabularx}{\linewidth}{lX}
\textbf{вход:} выборка $X^l - \{x_1,...,x_l\}$; параметры $\varepsilon$ и $m$\\
\textbf{выход:} разбиение выборки на кластеры и шумовые выбросы;\\\hspace*{7mm}\hspace*{9mm}$U := X^l$ - не помеченные точки, $a := 0$\\
\textbf{пока} в выборке есть непомеченные точки, $U \neq \emptyset$:\\
\hspace*{7mm} взять случайную точку $x \in U$; \\
\hspace*{7mm} \textbf{если} $\lvert U_\varepsilon (x) \rvert < m$ \textbf{то} \\
\hspace*{7mm}\hspace*{7mm} пометить $x$ как, возможно, шумовой;\\
\hspace*{7mm}\textbf{иначе} \\
\hspace*{7mm}\hspace*{7mm} создать новый кластер: $K:=U_\varepsilon (x); \; a:=a+1;$ \\
\hspace*{7mm}\hspace*{7mm} \textbf{для всех} $x' \in K$, не помеченных или шумовых \\
\hspace*{7mm}\hspace*{7mm}\hspace*{7mm} \textbf{если} $\lvert U_\varepsilon (x') \rvert \geq m$,  \textbf{то} $K := K \cup U_\varepsilon (x')$; \\
\hspace*{7mm}\hspace*{7mm}\hspace*{7mm} \textbf{иначе} поментить $x'$ как граничный кластера $K$;\\
\hspace*{7mm}\hspace*{7mm} $a_j := a$ для всех $x_i \in K$;\\
\hspace*{7mm}\hspace*{7mm} $U := U \backslash K$;\\
\vspace{5mm}
\end{tabularx}

В таком виде алгоритм обладает следующими \textbf{свойствами}:
\begin{itemize}
    \item быстрая кластеризация больших данных: \\$O(l^2)$ в худшем случае, \\ $O(l \mathrm{ln} l)$ при эффективной реализации $U_\varepsilon (x)$;
    \item кластеры произвольной формы
    \item деление объектов на корневые, граничные, шумовые.
\end{itemize}

При этом важно понимать, что граничные объекты не выстраивают в точности границу каждого кластера. Практически это означает, что не стоит всерьез рассматривать граничные объекты, в отличие от шумовых, которые действительно можно в дальнейшем анализировать.

\subsection{Примечание о HDBSCAN} 
От гиперпараметра $\varepsilon$ можно избавиться, используя дивизивную кластеризацию. Такая модификация называется HDBSCAN. Его суть проста: необходимо построить дендрограмму, где по $Oy$ будет отложен $\varepsilon$ (на рис.\ref{fig:hdbdendro} снизу distance). Так мы сможем явно видеть вложенные кластеры. Алгоритм затем сам вычисляет оптимальное количество кластеров на основе метрики "стабильности кластеров".

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{chapters/clustering/images/hdbscan_dendrogramm.png}
    \caption{К примечанию о HDBSCAN}
    \label{fig:hdbdendro}
\end{figure}
\subsection{Задачи}
\textbf{Задача 1.}

\textbf{Условие.} Применить DBSCAN для выборки из таблицы с $m=4,\;\varepsilon=1.9$. Метрика евклидова.

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 P1(3,7) & P5(7,3) & P9(3,3) \\ 
 P2(4,6) & P6(6,2) & P10(2,6) \\ 
 P3(5,5) & P7(7,2) & P11(3,5) \\ 
 P4(6,4) & P8(8,4) & P12(2,4) \\ 
 \hline
\end{tabular}
\end{center}

\textbf{Решение.}
Запишем матрицу, составленную из соответственных расстояний между точками выборки:
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c|c|} 
 \hline
dot & P1 & P2 & P3 & P4 & P5 & P6 & P7 & P8 & P9 & P10 & P11 & P12 \\ \hline
P1 & 0 &  &  &  &  &  &  &  &  &  &  &   \\ \hline
P2 & 1.41 & 0 &  &  &  &  &  &  &  &  &  &   \\ \hline
P3 & 2.83 & 1.41 & 0 &  &  &  &  &  &  &  &  &   \\ \hline
P4 & 4.24 & 2.83 & 1.41 & 0 &  &  &  &  &  &  &  &   \\ \hline
P5 & 5.66 & 4.24 & 2.83 & 1.41 & 0 &  &  &  &  &  &  &   \\ \hline
P6 & 5.83 & 4.47 & 3.16 & 2.00 & 1.41 & 0 &  &  &  &  &  &   \\ \hline
P7 & 6.40 & 5.00 & 3.61 & 2.24 & 1.00 & 1.00 & 0 &  &  &  &  &   \\ \hline
P8 & 5.83 & 4.47 & 3.16 & 2.00 & 1.41 & 2.83 & 2.24 & 0 &  &  &  &   \\ \hline
P9 & 4.00 & 3.16 & 2.83 & 3.16 & 4.00 & 3.16 & 4.12 & 5.10 & 0 &  &  &   \\ \hline
P10& 1.41 & 2.00 & 3.16 & 4.47 & 5.83 & 5.83 & 5.66 & 6.40 & 6.32 & 0 &  &   \\ \hline
P11& 2.00 & 1.41 & 2.00 & 3.16 & 4.47 & 4.24 & 5.00 & 5.10 & 2.00 & 1.41 & 0 &   \\ \hline
P12& 2.83 & 3.16 & 4.00 & 5.10 & 4.47 & 5.39 & 6.00 & 1.41 & 2.00 & 2.00 & 1.41 & 0  \\ \hline
\end{tabular}
\end{center}
Сравнивая значения в каждом столбце матрицы с $\varepsilon$ и отбирая те, что меньше этого значения, находим окрестности каждой точки.

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 точка & окрестность \\\hline
 P1 & P2, P10\\ 
 P2 & P1, P3, P11\\ 
 P3 & P2, P4\\ 
 P4 & P3, P5\\
 P5 & P4, P6, P7, P8\\
 P6 & P5, P7\\
 P7 & P5, P6\\
 P8 & P5\\
 P9 & P12\\
 P10 & P1, P11\\
 P11 & P2, P10, P12\\
 P12 & P9, P11\\
 \hline
\end{tabular}
\end{center}

Если в окрестности больше $m=4$ точек (включая ее саму), то отнесем эту точку к корневой, иначе - к шумовой.

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 точка & тип \\\hline
 P1 & шум\\ 
 P2 & корневая\\ 
 P3 & шум\\ 
 P4 & шум\\
 P5 & корневая\\
 P6 & шум\\
 P7 & шум\\
 P8 & шум\\
 P9 & шум\\
 P10 & шум\\
 P11 & корневая\\
 P12 & шум\\
 \hline
\end{tabular}
\end{center}

Уточним классификацию, учтя граничные точки, т.е. точки, лежащие в окрестности корневых, но при этом не являющимися корневыми:
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 точка & тип \\\hline
 P1 & граничная\\ 
 P2 & корневая\\ 
 P3 & граничная\\ 
 P4 & граничная\\
 P5 & корневая\\
 P6 & граничная\\
 P7 & граничная\\
 P8 & граничная\\
 P9 & шум\\
 P10 & граничная\\
 P11 & корневая\\
 P12 & граничная\\
 \hline
\end{tabular}
\end{center}

К первому кластеру отнесем окрестность корневой точки 2, причем в ее окрестности находится еще одна корневая точка 11, так что отнесем и ее окрестность к первому кластеру. Ко второму кластеру отнесем корневую точку 5 и ее окрестность. Осталась лишь одна точка P9, которая не относится ни к какому кластеру и является шумовой.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\linewidth]{chapters/clustering/images/task1dbs_plot.png}
    \caption{Кластеризация в задаче 1}
    \label{fig:task1dbs}
\end{figure}

\begin{minipage}{.5\textwidth}
\textbf{Задача 2.}\\
\textbf{Условие.}
  Сравните результаты кластеризации с помощью k-means и с помощью DBSCAN и объясните их.\\
\textbf{Решение.}
Объяснение различий:
\begin{itemize}
\item \textit{Форма кластера}:
K-средние: стремится найти сферические или выпуклые кластеры. Предполагается, что кластеры изотропны (однородны во всех направлениях) и имеют схожий размер.
DBSCAN: может обнаруживать кластеры произвольной формы и размера. Не делает предположений о форме кластеров.
\item \textit{Обработка шума}:
K-средние: плохо справляется с шумом. Точки шума могут быть назначены кластерам, что может повлиять на центры кластеров.
DBSCAN: может идентифицировать и маркировать точки шума, которые не назначены ни одному кластеру.
\end{itemize}
\end{minipage}% This must go next to `\end{minipage}`
\begin{minipage}{.4\textwidth}
      \includegraphics[width=0.95\linewidth]{chapters/clustering/images/task2dbs_plot.png}
\end{minipage}
\begin{itemize}
\item \textit{Плотность кластера}:
K-средние: не учитывает плотность точек. Каждый кластер представлен центроидом.
DBSCAN: учитывает плотность точек. Кластеры формируются на основе плотности точек в окрестности.
\item \textit{Чувствительность параметров}:
K-средние: требует предварительного указания количества кластеров (K), так что, если если заранее указать 3 кластера, то алгоритм и найдет три кластера, даже если он всего один, как на последней паре картинок.
\end{itemize}

\textbf{Задача 3.}\\
\textbf{Предисловие.}
При решении задачи 1 использовалась матрица, состоящая из расстояний между парами точек (\textit{матрица смежности}). Понятием, противоположным расстоянию, является понятие сходства между объектами. Неотрицательная вещественная функция $S(x_i,x_j) = S_{ij}$ называется \textit{мерой сходства}, если:
\begin{itemize}
    \item $0 \leq S(x_i,x_j) < 1$, для $x_i \neq x_j$
    \item $S(x_i,x_j)=1$
    \item $S(x_i,x_j)=S(x_j,x_i)$
\end{itemize}
Пары значений мер сходства можно объединить в \textit{матрицу сходства} $S$, симметричную и единичной диагональю.
\textbf{Условие.}
Применить DBSCAN с пороговым значением \textit{меры сходства} 0.8 и $m = 2$ и заданной матрицей сходства между точками выборки:

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|} 
 \hline
dot & P1 & P2 & P3 & P4 & P5  \\ \hline
P1 & 1.0 &  &  &  &     \\ \hline
P2 & 0.10 & 1.0 &  &  &  \\ \hline
P3 & 0.41 & 0.64& 1.0 &  & \\ \hline
P4 & 0.55 & 0.47 & 0.44 & 1.0 & \\ \hline
P5 & 0.35 & 0.98 & 0.85 & 0.76 & 1.0 \\ \hline
\end{tabular}
\end{center}

Сравнивая значения в каждом столбце матрицы с $\varepsilon$ и выбирая те точки, для которых значение сходства выше, чем порог, формируем окрестности всех точек.

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 точка & окрестность \\\hline
 P1 & -\\ 
 P2 & P5\\ 
 P3 & P5\\ 
 P4 & -\\
 P5 & P2, P3\\
 \hline
\end{tabular}
\end{center}

Если в окрестности больше $m=2$ точек (включая ее саму), то отнесем эту точку к корневой, иначе - к шумовой.

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 точка & тип \\\hline
 P1 & шум\\ 
 P2 & корневая\\ 
 P3 & корневая\\ 
 P4 & шум\\
 P5 & корневая\\
 \hline
\end{tabular}
\end{center}

Уточнение классификации, путем учитывания граничных точек, т.е. точек, лежащие в окрестности корневых, но при этом не являющимися корневыми, ничего не дает, т.к. в окрестности точек, определенных как шумовые вообще нет других точек, так что они действительно являются шумом.

К первому кластеру отнесем окрестность корневой точки P2, причем в ее окрестности находятся еще краевая точка P5, так что отнесем ее к этому же кластеру. В окрестности точки P5 помимо уже классифицированной P2 находится еще корневая точка P3, которую также отнесем к первому кластеру. Остальные точки классифицированы как шумовые. Таким образом в данной задаче всего один кластер, состоящий из точек P2, P3, P5.


\section{Affinity Propagation}

\subsection{Введение}

Метод распространения близости (affinity propagation) основан на концепции соотношения между данными и выборе из них экземпляров — наиболее репрезентативных образцов, которые представляются центроидами кластеров и группируют возле себя все остальные данные. 

Соотношения между данными описываются с помощью матрицы сходства (similarity matrix), матрицы доступности (availability matrix) и матрицы ответственности (responsibility matrix), а наиболее важными параметрами при настройке алгоритма являются damping (фактор затухания, который не дает алгоритму слишком быстро менять своё мнение о том, какие точки данных лучше всего подходят друг другу) и preference (мера предпочтения точки быть экземпляром для себя или для других точек: чем больше это значение, тем больше вероятность быть экземпляром)

\subsection{Структура алгоритма}

\begin{enumerate}
    \item на основе отрицательного квадратичного расстояния находится матрица сходства, а также нулями инициализируются матрицы доступности и ответственности;

    \item из матрицы сходства удаляются вырождения, а также добавляется небольшой шум;

    \item далее итеративно обновляются значения матриц доступности и ответственности на основе временной матрицы, представленной изначально как сумма матриц сходства и доступности, чтобы найти максимальное сходство между точками данных и потенциальными экземплярами;

    \item из этой матрицы вычисляются максимальные значения по столбцам, а также вторые по величине значения, которые используются для вычитания из матрицы сходства, чтобы получить новую матрицу ответственности;

    \item при обновлении значений матрицы доступности, временная матрица заполняется положительными значениями матрицы ответственности, чтобы посчитать сумму сообщений между точками о том, насколько сильно они хотят быть экземплярами.

    \item затем из данной матрицы вычитается её сумма по столбцам, чтобы получить отрицательную матрицу доступности, а также временная матрица обрезается по нулю для получения положительной матрицы доступности;

    \item также при обновлении значений матрицы доступности и ответственности к временной матрице применяется коэффициент затухания для избегания численных колебаний при обновлении значений;

    \item экземпляры с положительной суммой ответственности и доступности становятся центрами кластеров;

    \item после каждой итерации проверяется условие сходимости алгоритма с использованием матрицы сходимости экземпляров: если число экземпляров не меняется в течение заданного числа итераций или если число итераций достигает максимума, то алгоритм останавливается;

    \item в конце уточняется итоговый набор экземпляров и меток кластеров через выбор лучших экземпляров для каждого кластера из его членов на основе максимальной суммы сходств между ними;

    \item полученные метки кластеров и будут итоговым прогнозом.

\end{enumerate}

\subsection{Преимущества Affinity Propagation}
- высокая точность
\newline
- автоматическое определение кол-ва кластеров
\newline
- небольшое кол-во гиперпараметров, подлежащих настройке
\newline
- не чувствителен к выбору начальных значений

\subsection{Недостатки Affinity Propagation}
- возможная чувствительность к шуму в данных
- возможная чувствительность к выбросам в данных
\newline
- высокая сложность по времени и памяти, а значит плохо применим для данных большого размера

\subsection{Модификации алгоритма}

на данный момент существует несколько модификаций, способных значительно повысить его скорость. Среди наиболее интересных модификаций можно выделить следующие:
\newline
\newline
- Hierarchical Affinity Propagation (HAP) — алгоритм кластеризации с использованием Affinity Propagation несколько раз на разных выборках данных. Сначала находятся экземпляры на небольшом подмножестве данных, которые после применяются ко всему набору данных. Такой подход позволяет сократить время выполнения и уменьшить количество сообщений, передаваемых между точками, однако это может повлиять на качество кластеризации.
\newline
\newline
 - Fast Affinity Propagation — модификация на основе обрезаний сообщений, которая базируется на основе нескольких основных идей. 
 \begin{enumerate}
     \item сокращается размер матрицы сходства путем выбора небольшого подмножества экземпляров в качестве кандидатов в прототипы, а также вычисления сходства только между ними и всеми остальными экземплярами.

     \item используется многомерный поиск для определения параметра предпочтения путем разбиения интервала возможных значений предпочтения на несколько подынтервалов и запуска алгоритма на каждом из них параллельно. Затем выбирается тот подынтервал, который даёт наилучшее качество кластеризации по некоторому установленному критерию.

 \end{enumerate} 
 
- Affinity Propagation на основе пиков плотности — двухэтапный алгоритм кластеризации (DDAP), который сначала определяет пики плотности в данных с помощью алгоритма DDC (Density peaks and Distance-based Clustering), а затем использует стандартный Affinity Propagation для поиска экземпляров, которые близки к этим пикам. Такой подход позволяет значительно уменьшить количество вычислений при сопоставимом качестве кластеризации.

\subsection{Задачи}

\textbf{Задача 1}

У вас есть набор данных, содержащий двумерные точки, которые образуют кластеры различной формы, размера и плотности. Некоторые кластеры компактные и плотно расположенные, другие — вытянутые и разреженные. Вы хотите выполнить кластеризацию этого набора данных. Сравните алгоритм Affinity Propagation с алгоритмом k-средних (K-means). Какой из них может быть более подходящим?


\textbf{Решение}

Почему Affinity Propagation может быть более подходящим:

- Форма кластеров: алгоритм k-средних предполагает, что кластеры имеют сферическую форму и примерно одинаковый размер, поскольку он минимизирует внутрикластерное расстояние до центроидов. Это может привести к неправильной кластеризации при наличии кластеров неправильной формы или разной плотности.

- Количество кластеров: к-средних требует заранее задавать количество кластеров k, что не всегда возможно, если структура данных неизвестна. Affinity Propagation автомат {автоматически} определяет количество кластеров на основе данных.

- Чувствительность к начальному положению центроидов: к-средних чувствителен к начальному положению центроидов и может сходиться к локальным минимумам. Affinity Propagation не требует задания начальных центроидов.


\textbf{Задача 2}

Вычислительная сложность алгоритма Affinity Propagation. Как она зависит от количества точек данных n? Сравните её с вычислительной сложностью алгоритма иерархической кластеризации и k-средних. В каких ситуациях использование Affinity Propagation может быть нецелесообразным с точки зрения производительности?

\textbf{Решение}
\begin{enumerate}
    \item Вычислительная сложность Affinity Propagation: Алгоритм Affinity Propagation имеет временную сложность $O(n^2 \cdot T)$, где n — количество точек данных, T — количество итераций до сходимости.

    \item Причина квадратичной сложности:
        - Требуется вычислить и хранить матрицу сходства размером $n \cdot n$.
        - Передаются сообщения между всеми парами точек, что приводит к квадратичному росту объёма вычислений с увеличением n.

    \item Сравнение с другими алгоритмами:

- K-средних:**
  - Временная сложность $O(n \cdot k \cdot T)$, где k — количество кластеров, T — количество итераций.
  - Алгоритм масштабируется линейно по числу точек n, что делает его подходящим для больших наборов данных.

- Иерархическая кластеризация:
  - Агломеративные методы имеют временную сложность $O(n^2)$.
  - Как и Affinity Propagation, не подходят для очень больших наборов данных из-за
квадратичного роста вычислений.

Когда Affinity Propagation нецелесообразен:

- Большие наборы данных: При большом количестве точек (например, несколько десятков тысяч и более) вычислительная нагрузка становится очень высокой.
- Ограничения по памяти: Хранение матрицы сходства и сообщений требует значительного объёма оперативной памяти.
- Время вычислений: Время, необходимое для сходимости алгоритма, может быть неприемлемо большим.
\end{enumerate}


\textbf{Задача 3}

Вы применяете алгоритм Affinity Propagation для кластеризации изображений по их визуальному сходству. Вы используете признаки, извлечённые из нейронной сети, и вычисляете сходство между изображениями с помощью косинусного расстояния.
Объясните, как выбор параметра "предпочтения" (preference) влияет на количество полученных кластеров в Affinity Propagation. Как вы можете определить подходящий уровень "предпочтения" для вашей задачи?

\textbf{Решение}

\begin{enumerate}
    \item Влияние параметра "предпочтения" на количество кластеров:

- Определение "предпочтения": Параметр "предпочтения" задаёт склонность точек данных быть выбранными в качестве "экземпляров" (центров кластеров).

- Высокие значения "предпочтения":
  - Увеличивают вероятность того, что больше точек будут выбраны как экземпляры.
  - Приводят к большему количеству кластеров.

- Низкие значения "предпочтения":
  - Снижают вероятность выбора точек в качестве экземпляров.
  - Приводят к меньшему количеству кластеров, вплоть до одного общего кластера при очень низких значениях.

\item Определение подходящего уровня "предпочтения":

- Использование статистики сходств:
  - Установить "предпочтения" равными медиане или среднему значению сходства между точками. Это обеспечивает сбалансированное количество кластеров.

- Эмпирический подбор:
  - Запустить алгоритм с разными значениями "предпочтения" и наблюдать за количеством кластеров и их качеством.
  - Выбрать значение, при котором полученное разбиение соответствует требованиям задачи.

- Оценка качества кластеризации:
  - Использовать метрики, такие как коэффициент силуэта, для количественной оценки качества кластеров при разных значениях "предпочтения".
  - Выбрать значение, при котором метрика достигает максимума.

- Учёт специфики задачи:
  - Если известно приблизительное количество ожидаемых кластеров или имеются внешние ограничения, можно настроить "предпочтения" в соответствии с этими знаниями.

\end{enumerate}


\section{Агломеративная иерархическая кластеризация}
Иерархические алгоритмы кластеризации, также известные как алгоритмы таксономии, создают не одно, а несколько вложенных разбиений выборки на непересекающиеся классы. Результаты такой кластеризации обычно визуализируются в форме дендрограммы — дерева, которое иллюстрирует структуру кластеров и их взаимосвязи. Классическим примером может служить иерархическая классификация живых организмов, таких как животные и растения. \\
Среди алгоритмов иерархической кластеризации выделяются два основных типа. Дивизивные или нисходящие алгоритмы начинают с одного общего кластера и последовательно разбивают его на более мелкие подмножества. В отличие от них, агломеративные или восходящие алгоритмы работают по принципу объединения объектов в всё более крупные кластеры, начиная с того, что каждый объект представляет собой отдельный кластер.


\subsection{Описание алгоритма}
\begin{enumerate}
    \item Инициализировать множество кластеров $C_1$:  
    \[
    t := 1; \quad C_t = \{\{x_1\}, \{x_2\}, \ldots, \{x_\ell\}\}.
    \]
    \item Для всех $t = 2, \ldots, \ell$ (где $t$ — номер итерации):
    \begin{enumerate}
        \item Найти в $C_{t-1}$ два ближайших кластера $U, V$ и вычислить расстояние между ними $R(U, V)$:
        \[
        (U, V) := \arg \min_{U \neq V} R(U, V);
        \]
        \[
        R_t := R(U, V).
        \]
        \item Изъять кластеры $U$ и $V$, добавить объединённый кластер $W = U \cup V$:
        \[
        C_t := C_{t-1} \cup \{W\} \setminus \{U, V\}.
        \]
        \item Для всех $S \in C_t$ вычислить R(W, S) по формуле Ланса-Уильямса:
        \[
R(W, S) = \alpha_U R(U, S) + \alpha_V R(V, S) + \beta R(U, V) + \gamma |R(U, S) - R(V, S)|
        \]
    \end{enumerate}
\end{enumerate}

\subsection{Описание алгоритма для частичного обучения}
Если даны размеченные объекты $\{x_1, x_2, \ldots, x_k\}$, то алгоритм будет уже выглядеть по-другому:
\begin{enumerate}
    \item Инициализировать множество кластеров $C_1$:  
    \[
    t := 1; \quad C_t = \{\{x_1\}, \{x_2\}, \ldots, \{x_\ell\}\}.
    \]
    \item Для всех $t = 2, \ldots, \ell$ (где $t$ — номер итерации):
    \begin{enumerate}
        \item Найти в $C_{t-1}$ два ближайших кластера $U, V$ и вычислить расстояние между ними $R(U, V)$, \textbf{при условии, что в $U \cup V$ нет объектов с разными метками}:
        \[
        (U, V) := \arg \min_{U \neq V} R(U, V);
        \]
        \[
        R_t := R(U, V).
        \]
        \item Изъять кластеры $U$ и $V$, добавить объединённый кластер $W = U \cup V$:
        \[
        C_t := C_{t-1} \cup \{W\} \setminus \{U, V\}.
        \]
        \item Для всех $S \in C_t$ вычислить R(W, S) по формуле Ланса-Уильямса:
        \[
R(W, S) = \alpha_U R(U, S) + \alpha_V R(V, S) + \beta R(U, V) + \gamma |R(U, S) - R(V, S)|
        \]
    \end{enumerate}
\end{enumerate}

\subsection{Построение дендрограммы}
Дендрограммы графически иллюстрируют иерархическую кластеризацию. По горизонтали располагаются кластеры, а по вертикали — расстояния \( R_t \). 

При построении дендрограммы важно учитывать, что линии не должны нигде пересекаться. 

Определение числа кластеров наиболее удобно осуществлять путем отсечения правой части дендрограммы. На горизонтальной оси находится интервал максимальной длины \( |R_{t+1} - R_t| \).

Количество кластеров определяется по формуле:
\[
K = \ell - t + 1,
\]
где \( \ell \) — общее количество итераций.

Ниже представлен пример  дендрограммы:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.2\textwidth]{png/example.jpg} % Укажите путь к картинке
    \caption{Пример дендрограммы}
\end{figure}


\subsection{Частные случаи формулы Ланса-Уильямса}

На практике используются следующие способы вычисления расстояний \( R(W, S) \) между кластерами \( W \) и \( S \).

\textbf{1. Расстояние ближнего соседа}
\[
R_b(W, S) = \min_{w \in W, s \in S} \rho(w, s),
\]
где \( \alpha_U = \alpha_V = \frac{1}{2}, \beta = 0, \gamma = -\frac{1}{2} \).

\textbf{2. Расстояние дальнего соседа}
\[
R_d(W, S) = \max_{w \in W, s \in S} \rho(w, s),
\]
где \( \alpha_U = \alpha_V = \frac{1}{2}, \beta = 0, \gamma = \frac{1}{2} \).

\textbf{3. Среднее расстояние}
\[
R_c(W, S) = \frac{1}{|W||S|} \sum_{w \in W} \sum_{s \in S} \rho(w, s),
\]
где \( \alpha_U = \frac{|U|}{|W|}, \alpha_V = \frac{|V|}{|W|}, \beta = 0, \gamma = 0 \).

\textbf{4. Расстояние между центрами}
\[
R_c(W, S) = \rho^2\left(\frac{\sum_{w \in W} w}{|W|}, \frac{\sum_{s \in S} s}{|S|}\right),
\]
где \( \alpha_U = \frac{|U|}{|W|}, \alpha_V = \frac{|V|}{|W|}, \beta = -\alpha_U\alpha_V, \gamma = 0 \).

\textbf{5. Расстояние Уорда}
\[
R_u(W, S) = |S||W|\frac{\rho^2\left(\frac{\sum_{w \in W} w}{|W|}, \frac{\sum_{s \in S} s}{|S|}\right)}{|S| + |W|},
\]
где \( \alpha_U = \frac{|S| + |U|}{|S| + |W|},\,  \alpha_V =  \frac{|S| + |V|}{|S| + |W|},\,  \beta = -\frac{|S|}{|S| + |W|},\,  \gamma = 0. \)

\subsection{Свойства иерархической кластеризации}

\begin{itemize}

    \item \textbf{Сжатие}:
    \[
R_t \leq \rho(\mu_U, \mu_V), \quad \forall t.
\]
    \item \textbf{Растяжение}:
    \[
R_t \ge \rho(\mu_U, \mu_V), \quad \forall t.
\]
    \item \textbf{Монотонность}: Дендрограмма не имеет самопересечений, при каждом слиянии расстояние между объединяемыми кластерами только увеличивается

    Оказывается, не любое сочетание коэффициентов в формуле Ланса-Вильямса приводит к монотонной кластеризации. \\
    \textbf{Теорема Миллигана} \\
    Если выполняются следующие три условия, то кластеризация является монотонной:

\begin{enumerate}
    \item \( \alpha_U > 0, \quad \alpha_V > 0; \)
    \item \( \alpha_U + \alpha_V + \beta > 1; \)
    \item  \( \min\{\alpha_U, \alpha_V\} + \gamma > 0. \)
\end{enumerate}
\end{itemize}

Из перечисленных выше расстояний только \( R_c \) не является монотонным. 

\subsection{Задачи}
\textbf{Задача 1.}
Даны точки:
\[
X = \{(1, 2), (1, 4), (2, 3), (8, 8), (8, 9)\}.
\]
Необходимо провести агломеративную иерархическую кластеризацию с использованием метода дальнего соседа и построить дендрограмму.

Примечание: Используем метрику Евклида для вычисления расстояний между точками

\textbf{Решение}
\[
d((x_1, y_1), (x_2, y_2)) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}.
\]

\begin{itemize}
    \item \( d((1, 2), (1, 4)) = 2 \)
    \item \( d((1, 2), (2, 3)) \approx 1.41 \)
    \item \( d((1, 2), (8, 8)) \approx 9.22 \)
    \item \( d((1, 2), (8, 9)) \approx 9.90 \)
    \item \( d((1, 4), (2, 3)) \approx 1.41 \)
    \item \( d((1, 4), (8, 8)) \approx 8.06 \)
    \item \( d((1, 4), (8, 9)) \approx 8.60 \)
    \item \( d((2, 3), (8, 8)) \approx 7.81 \)
    \item \( d((2, 3), (8, 9)) \approx 8.49 \)
    \item \( d((8, 8), (8, 9)) = 1 \)
\end{itemize}

На первом шаге находим пару точек с минимальным расстоянием: \( (8, 8) \) и \( (8, 9) \), с расстоянием 1. Объединяем их в кластер \( C_1 = \{(8, 8), (8, 9)\} \).

Теперь пересчитываем расстояния между новым кластером и остальными точками:
\begin{itemize}
    \item \( d(C_1, (1, 2)) = \max(d((8, 8), (1, 2)), d((8, 9), (1, 2))) \approx 9.90 \)
    \item \( d(C_1, (1, 4)) = \max(d((8, 8), (1, 4)), d((8, 9), (1, 4))) \approx 8.60 \)
    \item \( d(C_1, (2, 3)) = \max(d((8, 8), (2, 3)), d((8, 9), (2, 3))) \approx 8.49 \)
\end{itemize}

Теперь выбираем пару с минимальным расстоянием: \( (1, 2) \) и \( (2, 3) \), с расстоянием \( \sqrt{2} \approx 1.41 \). Объединяем их в кластер \( C_2 = \{(1, 2), (2, 3)\} \).

Теперь пересчитываем расстояния между этим кластером и остальными:
\begin{itemize}
    \item \( d(C_2, (1, 4)) = \max(d((1, 2), (1, 4)), d((2, 3), (1, 4))) = 2 \)
    \item \( d(C_2, C_1) = \max(d((1, 2), (8, 8)), d((1, 2), (8, 9)), d((2, 3), (8, 8)), d((2, 3), (8, 9))) = 9.90 \)
\end{itemize}

Теперь объединяем \( C_2 = \{(1, 2), (2, 3)\} \) и \( (1, 4) \), так как их расстояние равно 2.

Теперь у нас есть два кластера:
\[
C_3 = \{(1, 2), (2, 3), (1, 4)\}, \quad C_1 = \{(8, 8), (8, 9)\}.
\]

Наконец, объединяем два оставшихся кластера \( C_3 \) и \( C_1 \), так как расстояние между ними равно 9.90.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{png/figure1.jpg} % Укажите путь к картинке
    \caption{Дендрограмма методом дальнего соседа}
\end{figure}


\textbf{Задача 2.} Доказать, что расстояние между центрами не является монотонным
\textbf{Доказательство.}
Формула Ланса-Уильямса при вычислении расстояния между центрами: 
$R_{\text{ц}}(W, S) = \rho^2 \left( \sum_{w \in W} w \frac{1}{|W|} , \sum_{s \in S} s \frac{1}{|S|} \right)$
\[
\alpha_U = \frac{|U|}{2}, \quad \alpha_V = \frac{|V|}{2}, \quad \beta = -\alpha_U \cdot \alpha_V, \quad \gamma = 0.
\]

Теорема Миллигана утверждает, что кластеризация является монотонной, если выполняются следующие три условия для параметров \( \alpha_U \), \( \alpha_V \) и \( \beta \):

\begin{enumerate}
    \item \( \alpha_U \geq 0 \), \( \alpha_V \geq 0 \)
    \item \( \alpha_U + \alpha_V + \beta \geq 1 \)
    \item \( \min(\alpha_U, \alpha_V) + \gamma \geq 0 \)
\end{enumerate}

Рассмотрим первую итерацию, когда $|U|=1$, $|V|=1$, а значит $|W|=2$.

Тогда:
\[
\alpha_U = \frac{1}{2}, \quad \alpha_V = \frac{1}{2}, \quad \beta = -\frac{1}{2} \cdot \frac{1}{2}.
\]

Рассмотрим условие 2 теоремы Миллигана:

\[
\alpha_U + \alpha_V + \beta = \frac{1}{2} + \frac{1}{2} - \left( \frac{1}{2} \cdot \frac{1}{2} \right)=1 - \frac{1}{4} = \frac{3}{4}.
\]

Это значение меньше 1, следовательно, второе условие не выполняется, и теорема Миллигана не выполняется для метода центроидного расстояния.

\textbf{Задача 3.}
По данной дендрограмме определить количество кластеров, пояснить свой выбор.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{png/figure2.jpg} % Укажите путь к картинке
    \caption{Дендрограмма методом Уорда}
\end{figure}

\textbf{Решение.}

Для нахождения числа кластеровмы ищем интервал максимальной длины \( |R_{t+1} - R_t| \), где \( R_t \) и \( R_{t+1} \) — расстояния между кластерами на каждом шаге слияния. На основе этих интервалов и максимального изменения расстояний между слияниями можно определить точку \( t \).
Количество кластеров находится по формуле: 
$K=l - t - 1$, где l - количество итераций

В нашем случае t = 12, l = 14, значит K = 14-12+1 = 3

\textbf{Ответ.} 3

\section{Простые эвристические методы частичного обучения}
\subsection{Постановка задачи}
Существует привычная нам задача классификации. Мы имеем $X$ --- множество объектов с известными признаками и $Y$ --- пространство классов. Есть неизвестная функция $X \longrightarrow Y$, сопоставляющая каждому обьекту его класс. Имеется обучающая выборка $\{x_1, x_2, ...\} \subset X$ и соответствующие им известные классы $\{y_1, y_2, ...\} \subset Y$. Задача классификации сводится к построению классификатора --- некой аппроксимации неизвестной функции $X \longrightarrow Y$. \\

С другой стороны существует задача кластеризации. Мы все также имеем $X$ --- множество объектов. И хотим аппроксимировать функцию $X \longrightarrow Y$. Но на этот раз у нас нет обучающей выборки, зато есть функция расстояния между объектами $\rho: X\times X \longrightarrow \mathbb{R}$. И в этом случае кластеризующая функция строится не на основе обучающей выборки, а так чтобы расстояние между объектами одного кластера было мало, а расстояние между объектами разных кластеров было велико. \\

Первый случай называется обучение с учителем,а кластеризация называется обучением без учителя. Где-то по середине между этими задачами находится задача частичного обучения. В этом случае у нас все также есть множество обьектов из $X$ и (возможно) функция $\rho$. При этом только для некоторой доли имеющихся обьектов известна классовая принадлежность. Иначе говоря обучающая выборка размечена \textbf{частично}. \\

Приведем пример (Рис. 1). Допустим известные обьекты в пространстве признаков имеют вид в виде двух бананов. И нам известна принадлежность только двух точек. В таком случае чистая задача кластеризации обучит классификатор только по двум точкам, который будет иметь сомнительное качество на всех остальных данных. При это частичное обучение могло бы учесть явную кластеризацию данных и дать значительно лучшую классификацию.\\

\begin{center}
\includegraphics[width=1.0\textwidth]{chapters/clustering/images/picture_1.png}
\textbf{Рисунок 1.} (\textbf{A}) Пример частично размеченных данных. (\textbf{B}) Классификация обученная на размеченных данных не учитывает кластерную структуру неразмеченных. 
\end{center}

Однако частичное обучение не может сводиться только к кластеризации, представьте теперь, что мы знаем классы уже трех точек на выборке из двух бананов (Рис. 2). В таком случае кластеризация дала бы предсказание которое не может соотноситься с известными классами всех трех точек. Таким образом задача частичного обучения действительно находится посередине между классификацией и кластеризацией, но не является ни тем, ни другим. \\
\begin{center}
\includegraphics[width=1.0\textwidth]{chapters/clustering/images/picture_2.png}
\textbf{Рисунок 2.} (\textbf{A}) Пример частично размеченных данных. (\textbf{B}) Кластеризация не учитывает классовую принадлежность размеченных данных. 
\end{center}

Приведем пример: программист решил сделать классификатор для фотографий котиков и собачек, для этого он скачал по миллиону фотографий и тех и других. Однако, ему хватило сил подписать котик это или собачка только для 1000 фотографий. Классификация не может быть построена только по 1000 фотографиям --- мало данных, а кластеризация может разбить фотографии неизвестным образом --- по цвету фона, по размеру животного. И только частичное обучение может помочь ленивому программисту.
\subsection{Self-training}


Пусть $\mu: X^{\text{k}} \to a$ \textendash{} метод обучения классификации;

Классификаторы имеют вид:

\[
a(x) = \arg \max_{y \: \in \: Y}{\Gamma_{y}(x)}.
\]

Для каждого класса вычисляем оценку степени принадлежности объекта классу. Нам нужна оценка степени уверенности классификации для объекта без разметки. При наличии разметки мы могли бы использовать просто маржин (отступ).

\textbf{Псевдоотступ} \textendash{} степень уверенности классификации $a_i = a(x_i):$
\[
M_i(a) = \Gamma_{a_i(x_i)} - \max_{y \in Y \setminus a_i}{\Gamma_y(x_i)}.
\]

Псевдоотступ позволяет выделить из неразмеченных те объекты, на которых мы настолько уверены, что их реальный маржин, вероятно, был бы того же знака или такой же большой.

Основная идея метода заключается в том, чтобы постепенно увеличивать объём размеченных данных, добавляя к ним объекты, на которых мы достаточно сильно уверены.

\subsection*{Алгоритм}

\begin{algorithm}
\caption{Self-training Algorithm}\label{alg:SelfTraining}
\begin{algorithmic}[1]

\State $Z \gets X^k$
\While{$|Z| < \ell$}
    \State $a \gets \mu(Z)$
    \State $\Delta \gets \{x_i \in U \setminus Z \mid M_i(a) \geq M_0\}$
    \ForAll{$x_i \in \Delta$}
        \State $a_i \gets a(x_i)$
    \EndFor
    \State $Z \gets Z \cup \Delta$
\EndWhile

\end{algorithmic}
\end{algorithm}

\textbf{Пояснение к алгоритму:}
\newline Стартуем с выборки $Z$ \textendash{} размеченные объекты. Обучаем на них классификатор.
Берём из неразмеченных данных небольшое количество объектов, у которых псевдоотступы наибольшие. Считаем, что на них мы смогли определить класс правильно, добавляем их в $Z$.

\subsection*{Пример}

\begin{wrapfigure}{r}{0.4\textwidth}
    \includegraphics[width=0.8\textwidth]{Self_learning.png}
    \caption{Пример работы алгоритма}
    \label{fig:example}
\end{wrapfigure}

Изначально имеем два размеченных объекта \textendash{} синий и красный квадраты. Линия \textendash{} изначально обученный классификатор. Светлосиние и светлокрасные объекты \textendash{} добавляемые на этой итерации. На следующей итерации разделяющая линия будет смещаться по часовой стрелке, и мы будем добавлять новые объекты.

Порог $M_0$ можно определять из условия мощности добавляемого множества. Например, 
\[ \|\Delta\| = 0.05\|U\|| \text{ \textendash{} добавляем $5\%$ от неразмеченных данных.}
\]
\subsubsection{Задачи}
\subsubsection*{Задача 1. Выбор параметра $M_0$}

Дана выборка объектов $X$, разбитая на размеченные $Z$ и неразмеченные $U$. Классификатор $a(x)$ обучен на $Z$. Для каждого объекта $x_i \in U$ вычислены псевдоотступы $M_i(a)$.

\begin{itemize}
    \item (a) Предложите методику выбора порогового значения $M_0$ для добавления объектов в $\Delta$, если известно, что мощность $\|\Delta\|$ должна составлять не более $10\%$ от $\|U\|$.
    \item (b) Реализуйте выбранный метод в виде описания алгоритма.
    \item (c) Проанализируйте, как изменение $M_0$ влияет на количество добавляемых объектов $\|\Delta\|$ и качество классификации.
\end{itemize}

\subsubsection*{Задача 2. Анализ ошибок алгоритма}

Рассмотрим работу self-training алгоритма. Пусть начальная размеченная выборка $Z$ состоит из двух классов (обозначены цветами: синий и красный). Изначально классификатор имеет ошибку, разделяя выборку с погрешностью.

\begin{itemize}
    \item (a) Как выбор начальных объектов в $Z$ может повлиять на итоговую точность классификации?
    \item (b) Приведите пример ситуации, когда добавление объектов с большим псевдоотступом $M_i(a)$ приводит к ошибке классификатора.
    \item (c) Предложите способ уменьшения ошибки, связанной с неправильным добавлением объектов в $Z$.
\end{itemize}

\subsubsection*{Задача 3. Визуализация работы алгоритма}

На двумерной плоскости задано множество объектов $U$: синие точки, красные точки и несколько объектов с неизвестной разметкой (обозначены серым цветом). Изначально обучен классификатор $a(x)$, который проводит разделяющую линию между двумя классами.

\begin{itemize}
    \item (a) Постройте псевдоотступы $M_i(a)$ для всех серых объектов.
    \item (b) Определите объекты, которые следует добавить в $Z$, если порог $M_0$ задан так, что добавляется $5\%$ от всех серых объектов.
    \item (c) На графике покажите изменённую разделяющую линию после добавления объектов и опишите, как она изменяется с каждой итерацией.
\end{itemize}
\subsection{Self-training}
Каким же образом можно реализовать частичное обучение? Рассмотрим подход, который называется self-training (само-обучение). Вернемся к датасету с бананами в котором известны классы двух точек. Как было сказано ранее построение классификатора по двум точкам тут не поможет. Однако допустим, что мы все же построили классификатор по двум точкам, к примеру логистическую регрессию. Вспомним что многие методы классификации могут оценивать свою уверенность в предсказании. Тогда для некоторых точек с наибольшей уверенностью классификации (скажем 5\% самых уверенных) предсказание  классификации может оказаться вполне верным. Давайте же дополним обучающую выборку классификатора этими точками. На второй итерации нашего подхода мы обучим классификатор уже по дополненной обучающей выборке. Выполнив предсказание для остальных данных, выберем еще раз 5\% ранее неразмеченных точек с наибольшей уверенностью и пополним ими обучающую выборку. Будем повторять такие итерации пока не классифицируем все точки в нашем датасета. Как мы видим, результат такого подхода заметно лучше, чем у классификации по двум исходным точкам. 
\begin{center}
\includegraphics[width=1.0\textwidth]{chapters/clustering/images/picture_3.png}
\textbf{Рисунок 3.} Пример работы итераций self-training. 
\end{center}
Оформим описанный процесс более математизировано. Пусть мы строим функцию $a: X \longrightarrow Y$. Пусть у нас есть метод обучения функции $\mu: Z \longrightarrow a$ который принимает на вход размеченную часть выборки $Z \subset X$. Допустим функция $a$ имеет вид
\begin{equation*}
    a(x) = \arg \max_{y \subset Y}\Gamma_y(x)
\end{equation*}
где $\Gamma_y(x)$ это некоторые (к примеру) линейные функции от $x \subset X$ которые обучаются так, чтобы быть большими, если $x$ принадлежит классу $y$ и маленькими в противном случае. В таком случае уверенность классификации в принадлежности элемента $x$ к классу $y_1$ (отступ):
\begin{equation*}
    M_1(x) = \Gamma(x) - \max_{y \subset Y\backslash y_1}\Gamma_{y_1}(x) 
\end{equation*}
Пусть $Z$ это размеченная часть обучающей выборки. Тогда алгоритм выглядит так:
\begin{enumerate}
  \item $a = \mu(Z)$ --- обучить классификатор на размеченной выборке
  \item $\Delta := \{x \subset X\backslash Z \;|\; M(x) \geq M_0 \}$ --- выбрать несколько точек из неразмеченной части выборки которые наиболее уверенно классифицируются. 
  \item $Z := Z\cup\Delta$ --- дополнить размеченную выборку
  \item Если не все элементы выборки размечены, вернуться в начало.
\end{enumerate}
\subsection{Сo-training}
Рассмотрим более узкий кейс, допустим у нас есть не один, а целых два метода обучения классификации $\mu_1, \mu_2$, которые принципиально отличаются друг от друга, например имеют разные парадигмы обучения, и/или используют разные признаки обьектов, и/или имеют разную стартовую выборку. В таком случае мы можем получить преимущество в частичном обучении заставив их учить друг друга по следующему алгоритму:
\begin{enumerate}
  \item $a_1 = \mu_1(Z_1)$\\
  $a_2 = \mu_2(Z_2)$ --- два метода обучают классификаторы на своих размеченных выборках. 
  \item $\Delta_1 := \{x \subset X\backslash Z_1\backslash Z_2 \;|\; M_1(x) \geq M_{01} \}$ --- метод 1 размечает неразмеченные точки, в которых он уверен.\\
  $\Delta_2 := \{x \subset X\backslash Z_1\backslash Z_2 \;|\; M_2(x) \geq M_{02} \}$ --- метод 2 делает тоже самое.
  \item $Z_1 := Z_1\cup\Delta_2$ --- метод 2 дополняет обучающую выборку метода 1 \\
  $Z_2 := Z_2\cup\Delta_1$ --- метод 1 дополняет обучающую выборку метода 2
  \item Если не все элементы выборки размечены, вернуться в начало.
\end{enumerate}
\subsection{Сo-learning}
Идем еще дальше, допустим у нас теперь есть набор методов $\mu_i$ отличающиеся чем-то. Допустим что все эти методы обучились на одной выборке $Z$ и произвели классификаторы $a_i$. Давайте соберем из множества классификаторов $a_i$ один классификатор-мегазорд:
\begin{equation*}
    a(x) = \arg \max_{y \subset Y}\Gamma_y(x)
\end{equation*}
Где на этот раз функции $\Gamma$ определяются как:
\begin{equation*}
    \Gamma_y(x) = \sum_{i = 1}^{I}[a_i(x) = y]
\end{equation*}
Выражение в квадратных скобках равно 1 когда написанное в них верно, и равно 0 в противных случаях. Иначе говоря, классификаторы $a_i$ голосуют за принадлежность к классам, и демократически выбирают к кому классу отнести каждый из обьектов. Далее на основе такого обьединенного классификатора строится Self-training описанный выше. 
\subsection{Задачи}
\subsection*{Задача 1}
Возможно ли при помощи self-training эффективно классифицировать точки частично размеченной выборки представленной ниже:
\begin{center}
\includegraphics[width=0.5\textwidth]{chapters/clustering/images/picture_5.png}
\end{center}
\textit{\textbf{Решение:} Конечно можно! Для этого нужно построить self-training на основе классификатора который может делать разделяющую поверхность в виде окружности, например классификаторы восстанавливающие плотность или логистическая регрессия с добавлением квадратичных признаков}
\subsection*{Задача 2}
Два подхода частичного обучения применяются для классификации обьектов частичной выборки представленной ниже.
\begin{center}
\includegraphics[width=0.5\textwidth]{chapters/clustering/images/picture_4.png}
\end{center}
Первый подход это self-training на основе логичтической регрессии, которая видит только первый признак. Второй подход это co-training на основе двух логистических регрессий, таких что первая видит первый признак, а вторая второй. Какой алгоритм приведет к лучшей классификации точек и почему?\\

\textit{\textbf{Решение:} self-training видящий только первый признак веротянее всего идеально классифицирует точки, так как по этому признаку кластеры разделены и практически не пересекаются. В случае co-training классификатор использующий второй признак видит кластеры пересекающимися и почти наверняка наделает ошибок при классификации}
\subsection*{Задача 3}
Алгоритм co-training построенный на основе трех классификаторов классифицирует точки представленной ниже выборки. На последней итерации работы алгоритма осталась лишь одна неклассифицированная точка. На этот момент классификаторы имеют разделяющие линии представленные на картинке. К какому кластеру будет отнесена оставшаяся точка?
\begin{center}
\includegraphics[width=0.5\textwidth]{chapters/clustering/images/picture_6.png}
\end{center}
\textit{\textbf{Решение:} Судя по разделяющим линиям два классификатора проголосую за красный кластер, и только один за синий, в итоге точка будет отнесена к красному кластеру.}

\section{Алгоритм FOREL}
Рассмотрим алгоритм кластеризации FOREL.
\section*{Необходимые условия работы}

\begin{itemize}
    \item Выполнение гипотезы компактности, предполагающей, что близкие друг к другу объекты с большой вероятностью принадлежат к одному кластеру (таксону).
    \item Наличие линейного или метрического пространства кластеризуемых объектов.
\end{itemize}

\section*{Входные данные}

\begin{enumerate}
    \item Кластеризуемая выборка.
    \begin{itemize}
        \item Может быть задана признаковыми описаниями объектов (линейное пространство) либо матрицей попарных расстояний между объектами.
        \item Замечание: в реальных задачах зачастую хранение всех данных невозможно или бессмыслено, поэтому необходимые данные собираются в процессе кластеризации.
    \end{itemize}
    \item Параметр \( R \) — радиус поиска локальных сгущений.
    \begin{itemize}
        \item Его можно задавать как из априорных соображений (знание о диаметре кластеров), так и настраивать скользящим контролем.
        \item В модификациях возможно введение параметра \( k \) — количества кластеров.
    \end{itemize}
\end{enumerate}

\section*{Выходные данные}

Кластеризация на заранее неизвестное число таксонов.

\section*{Принцип работы}

На каждом шаге мы случайным образом выбираем объект из выборки, раздуваем вокруг него сферу радиуса \( R \), внутри этой сферы выбираем центр тяжести и делаем его центром новой сферы. Таким образом, на каждом шаге мы двигаем сферу в сторону локального сгущения объектов выборки, т.е. стараемся захватить как можно больше объектов выборки сферой фиксированного радиуса. После того как центр сферы стабилизируется, все объекты внутри сферы с этим центром помечаем как кластеризованные и выкидываем их из выборки. Этот процесс повторяется до тех пор, пока вся выборка не будет кластеризована.

\section*{Алгоритм}

\begin{enumerate}
    \item Случайным образом выбираем текущий объект из выборки.
    \item Помечаем объекты выборки, находящиеся на расстоянии менее, чем \( R \) от текущего.
    \item Вычисляем их центр тяжести, помечаем этот центр как новый текущий объект.
    \item Повторяем шаги 2-3, пока новый текущий объект не совпадет с прежним.
    \item Помечаем объекты внутри сферы радиуса \( R \) вокруг текущего объекта как кластеризованные, выкидываем их из выборки.
    \item Повторяем шаги 1-5, пока не будет кластеризована вся выборка.
\end{enumerate}

\subsection{Задачи}

\textbf{Задача 1: Основы работы алгоритма}

\textbf{Условие задачи:}
Какие основные принципы лежат в основе работы алгоритма кластеризации, описанного в тексте? Как алгоритм решает, когда завершить кластеризацию?

\textbf{Решение:}
\begin{itemize}
    \item Центр нового кластера определяется как центр тяжести объектов, находящихся в сфере радиуса \( R \) вокруг выбранного объекта. Алгоритм постепенно двигает сферу в сторону локального сгущения объектов, пока центр тяжести не стабилизируется.
    \item Алгоритм завершает кластеризацию, когда все объекты в выборке оказываются кластеризованными. На каждом шаге выбирается новый объект, формируется его кластер, и эти объекты удаляются из выборки. Процесс повторяется до тех пор, пока не останется необработанных объектов.
\end{itemize}

\vspace{1em}

\textbf{Задача 2: Выбор радиуса}

\textbf{Условие задачи:}
В алгоритме используется параметр \( R \) — радиус поиска локальных сгущений. Как изменение параметра \( R \) повлияет на кластеризацию, если \( R \) слишком или слишком велик?

\textbf{Решение:}
\begin{enumerate}
    \item Если \( R \) слишком мал, то алгоритм будет искать только очень близкие объекты, что может привести к образованию множества маленьких кластеров, каждый из которых содержит лишь несколько объектов. Это приведет к излишней детализированности кластеризации.
    \item Если \( R \) слишком велик, алгоритм будет захватывать слишком много объектов в один кластер, что может привести к неадекватной кластеризации, где несколько разных кластеров окажутся объединены в один.
\end{enumerate}

\vspace{1em}

\textbf{Задача 3: Число кластеров}

\textbf{Условие задачи:}
Какое влияние на эффективность работы алгоритма может оказать использование параметра \( k \) (количества кластеров)? Ответьте на вопросы:
\begin{enumerate}
    \item Что произойдет, если параметр \( k \) установлен заранее и не соответствует истинному числу кластеров в выборке?
    \item Как алгоритм может адаптироваться, если параметр \( k \) не задан и алгоритм должен кластеризовать данные до тех пор, пока не останется необработанных объектов?
\end{enumerate}

\textbf{Решение:}
\begin{enumerate}
    \item Если параметр \( k \) установлен заранее, но не соответствует истинному числу кластеров, то кластеризация будет либо слишком грубой, либо слишком детализированной. Алгоритм может либо объединить несколько кластеров в один, либо разделить один кластер на несколько, что приведет к неадекватной кластеризации.
    \item Если параметр \( k \) не задан, алгоритм будет продолжать кластеризацию до тех пор, пока не обработает все объекты. В этом случае количество кластеров не ограничено, и алгоритм сам находит подходящее число кластеров, основываясь на локальных сгущениях объектов. Этот подход может быть более гибким, но требует большего времени на обработку.
\end{enumerate}


\section{ЕМ-алгоритм для кластеризации на примере смеси нормальных распределений}

В задачах кластеризации мы хотим разбить данные на группы, не имея априорной разметки. Одним из распространённых подходов для решения подобных задач является модель смеси нормальных распределений (Gaussian Mixture Model, GMM) и её оценка методом максимального правдоподобия с использованием ЕМ-алгоритма (Expectation-Maximization).

\subsection{Модель смеси нормальных распределений}

Пусть у нас есть выборка $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\}$, где каждая точка $\mathbf{x}_n \in \mathbb{R}^D$. Предположим, что данные сгенерированы смесью $K$ нормальных распределений, то есть каждое наблюдение порождается из одного из $K$ компонент смеси, но нам неизвестно, из какого именно. Формально:
\begin{equation*}
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k),
\end{equation*} 
где $\pi_k$ --- вероятности принадлежности к компоненте $k$ , удовлетворяющие 
\begin{equation*}
\pi_k \geq 0 \quad \text{и} \quad \sum_{k=1}^K \pi_k = 1.
\end{equation*} 
Математические ожидания $\boldsymbol{\mu}_k \in \mathbb{R}^D$ и ковариационные матрицы $\boldsymbol{\Sigma}_k \in \mathbb{R}^{D \times D}$ --- параметры нормального распределения каждой компоненты.

\subsection{Скрытые переменные и ЕМ-алгоритм}

Для удобства введём скрытые переменные $\mathbf{z}_n \in \{1,\ldots,K\}$, обозначающие индекс компоненты, из которой сгенерировано наблюдение $\mathbf{x}_n$. Тогда совместное распределение:
\begin{equation*}
p(\mathbf{x}_n, z_n) = p(z_n) p(\mathbf{x}_n | z_n) = \pi_{\mathbf{z}_n} \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_{\mathbf{z}_n}, \boldsymbol{\Sigma}_{\mathbf{z}_n}).
\end{equation*}

Правдоподобие для всей выборки $X$:
\begin{equation*}
p(X|\{\pi_k,\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k\}_{k=1}^K) = \prod_{n=1}^N \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\end{equation*}

Максимизация этого правдоподобия напрямую затруднена из-за суммы внутри произведения. ЕМ-алгоритм решает эту проблему, рассматривая скрытые переменные $Z=\{z_n\}$, итеративно улучшая оценки параметров.

\textbf{Идея ЕМ:} 

1. \textbf{E-шаг (Expectation)}: при фиксированных текущих оценках параметров вычисляем апостериорные вероятности (ответственности) «принадлежности» точки $\mathbf{x}_n$ к кластеру $k$:
\begin{equation*}
\gamma_{nk} = p(z_n = k|\mathbf{x}_n) = \frac{\pi_k \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_k,\boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}_n|\boldsymbol{\mu}_j,\boldsymbol{\Sigma}_j)}.
\end{equation*}

2. \textbf{M-шаг (Maximization)}: при фиксированных $\gamma_{nk}$ пересчитываем параметры, максимизируя ожидаемое правдоподобие:
\begin{equation*}
\pi_k^{new} = \frac{1}{N}\sum_{n=1}^N \gamma_{nk}, \quad \boldsymbol{\mu}_k^{new} = \frac{\sum_{n=1}^N \gamma_{nk}\mathbf{x}_n}{\sum_{n=1}^N \gamma_{nk}},
\end{equation*}
\begin{equation*}
\boldsymbol{\Sigma}_k^{new} = \frac{\sum_{n=1}^N \gamma_{nk}(\mathbf{x}_n - \boldsymbol{\mu}_k^{new})(\mathbf{x}_n - \boldsymbol{\mu}_k^{new})^T}{\sum_{n=1}^N \gamma_{nk}}.
\end{equation*}

После чего вычисляем новое правдоподобие и проверяем критерий сходимости. ЕМ-алгоритм гарантированно не уменьшает правдоподобие на каждой итерации и обычно сходится к локальному максимуму.

\subsection{Аналогия с k-means}

Алгоритм $k$-means можно рассматривать как предельный случай GMM с ковариационными матрицами вида $\sigma^2 \mathbf{I}$ при $\sigma^2 \to 0$. В этом случае нормальное распределение вырождается практически в дельта-функцию (очень узкий пик), и $\gamma_{nk}$ превращаются в индикаторы принадлежности точки $\mathbf{x}_n$ к наиболее близкому центру. Тогда M-шаг сводится к пересчёту средних арифметических для каждого кластера, а E-шаг --- к однозначному присвоению точек кластерам. Таким образом, $k$-means можно рассматривать как предельный случай EM для смеси нормальных распределений с нулевой дисперсией.

\subsection{Дополнительный пример: Бернуллиевская смесь}

Вместо смеси нормальных распределений можно рассмотреть смесь Бернуллиевских распределений. Если у нас бинарные данные $\mathbf{x}_n \in \{0,1\}^D$, тогда:
\begin{equation*}
p(\mathbf{x}_n | z_n = k) = \prod_{d=1}^D p_{kd}^{x_{nd}} (1 - p_{kd})^{1 - x_{nd}},
\end{equation*}
где $p_{kd}$ --- параметр Бернуллиевского распределения для компоненты $k$ и признака $d$.

ЕМ-алгоритм в случае Бернуллиевской смеси также состоит из вычисления «ответственностей» $\gamma_{nk}$ и последующего пересчёта параметров $p_{kd}$ как долей единичных значений среди точек, принадлежащих кластеру $k$, взвешенных ответственностями.

\subsection{Задачи для закрепления материала}

\noindent\textbf{Задача 1.} \emph{Переход от GMM к k-means.}  
Пусть мы имеем модель GMM с ковариационными матрицами $\boldsymbol{\Sigma}_k = \sigma^2 \mathbf{I}$. Покажите, что при предельном переходе $\sigma^2 \to 0$ правило обновления кластеризации переходит в правило $k$-means. 

\underline{Краткий ответ:} при $\sigma^2 \to 0$ нормальные распределения вырождаются в дельта-функции, и точка присваивается тому кластеру, для которого расстояние $\|\mathbf{x}_n - \boldsymbol{\mu}_k\|$ минимально, что соответствует однозначному присвоению как в $k$-means.

\noindent\textbf{Задача 2.} \emph{Получение формул для смеси Бернулли.}  
Предположим, что данные бинарные, и модель есть смесь Бернуллиевских распределений:
\begin{equation*}
p(\mathbf{x}_n | z_n = k) = \prod_{d=1}^D p_{kd}^{x_{nd}}(1 - p_{kd})^{1-x_{nd}}.
\end{equation*}
Выпишите формулы для M-шагa, аналогичные случаю смеси нормальных распределений.

\underline{Краткий ответ:}
\begin{equation*}
\pi_k = \frac{1}{N}\sum_{n=1}^N \gamma_{nk}, \quad p_{kd} = \frac{\sum_{n=1}^N \gamma_{nk} x_{nd}}{\sum_{n=1}^N \gamma_{nk}}.
\end{equation*}

\noindent\textbf{Задача 3.} \emph{Инициализация и сходимость.}  
Опишите результат работы ЕМ-алгоритма для смеси нормальных распределений, если при инициализации параметров средние некоторых кластеров совпадут в точности с некоторыми точками выборки.

\underline{Краткий ответ:} Случится вырождение кластеров отвечающих за эти точки, дисперсия будет стремиться к нулю, из-за чего алгоритм будет двигаться в сторону такого вырожденного, а не информативного, максимума правдоподобия.


\section{Нечёткая кластеризация}

\textit{Нечёткой кластеризацией} называется такой вид кластеризации, при котором объекты могут
принадлежать более чем одному кластеру.

Каждому объекту выборки присваивается степень принадлежности каждому кластеру. Так, объекты,
расположенные на границе кластера, имеют меньшую степень принадлежности ему, чем объекты в центре
того же кластера.

Степень принадлежности объекта $x_i \in X$ ($X$ конечно) кластеру $j$ обозначается $w_{ij}$.

Вводится также понятие \textit{центроида} кластера:
\begin{equation}\label{fuzzy-clustering-centroid}
    c_j = \frac{\sum\limits_{i = 1}^{|X|} w_{ij}^{m} x_i}{\sum\limits_{i = 1}^{|X|} w_{ij}^{m}}.
\end{equation}
Число $m > 1$ -- гиперпараметр, определяющий степень нечёткости кластеров: чем он больше, чем более
нечёткими окажутся кластеры. Понятие центроида аналогично центру масс в физике: сходство в
определяющей формуле очевидно.

Одним из наиболее часто используемых алгоритмов нечёткой кластеризации является алгоритм
C-средних (Fuzzy C-means clustering, FCM). Алгоритм был разработан в 1973 году человеком по имени
J.C. Dunn, а позже в 1981 году усовершенствован J.C. Bezdek.

\subsection{Описание алгоритма}

На вход алгоритма поступает конечное множество объектов $X = \{x_1, \ldots, x_n\}$ и число $c$
кластеров, на которые эти объекты следует разбить.

На выходе алгоритма получаем список центроидов кластеров $C = \{c_1, \ldots, c_c\}$ и матрицу
\begin{equation*}
    W = \{w_{ij}\},\ w_{ij} \in [0; 1],\ i \in \{1, \ldots, n\},\ j \in \{1, \ldots, c\},
\end{equation*}
$ij$-й элемент которой является степенью принадлежности $i$-го объекта $j$-му кластеру.

В процессе работы алгоритма оптимизации подлежит целевая функция:
\begin{equation*}
    J(W, C) = \sum\limits_{i = 1}^{n} \sum\limits_{j = 1}^{c} w_{ij}^{m} \lVert x_i - c_j \rVert^2,
\end{equation*}
где
\begin{equation}\label{fuzzy-clustering-weight}
    w_{ij} = \left(\sum\limits_{k = 1}^{c}
        \left(
            \frac{\lVert x_i - c_j \rVert}{\lVert x_i - c_k \rVert}
        \right)^{\frac{2}{m - 1}}
    \right)^{-1}.
\end{equation}

Сам алгоритм имеет следующую структуру:

\begin{itemize}
    \item Каждому объекту присвоить случайные степени принадлежности кластерам $w_{ij}$.
    \item Повторять, пока алгоритм не сойдётся ($J(W, C) > \varepsilon$):
    \begin{itemize}
        \item Для каждого кластера вычислить центроид $c_j$ по формуле
                \eqref{fuzzy-clustering-centroid}.
        \item Пересчитать степени принадлежности кластеру каждого объекта по формуле
                \eqref{fuzzy-clustering-weight}.
    \end{itemize}
\end{itemize}

\subsection{Сравнение с алгоритмом $K$-средних}

Заметим, что степень принадлежности $w_{ij}$ для алгоритма $C$-средних стремится к 0 или 1 при
$m \to 1$. Таким образом, алгоритм $K$-средних является пределом $C$-средних при $m \to 1$.

Рассмотрим одномерные объекты, которые необходимо разделить на 2 кластера. Применение обоих
алгоритмов приводит к следующему результату:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fuzzy-clustering.png}
\end{figure}

Как можно заметить, расстояние между центроидами кластеров, полученных с помощью $C$-средних,
увеличивается с ростом $m$.

\subsection{Применение алгоритма}

FCM нашёл место в обработке изображений. Долгое время алгоритм $K$-средних использовался для
распознавания паттернов. Однако в связи с такими проблемами, преследующими фотосъёмку, как шум и
тени, жёсткая кластеризация не всегда способна показать удовлетворительные результаты. Для лучшего
решения подобных задач и был предложен FCM. Например, он используется при конвертации RGB в HCL.

\subsection{Задачи}

\begin{itemize}
    \item Оценить сложность алгоритма FCM.
    \item Строго показать, что $\lim\limits_{m \to 1 + 0} w_{ij} \in \{0, 1\}$.
    \item Показать, что в качестве начального приближения $w_{ij}$ нельзя брать одинаковые величины.
\end{itemize}

\subsection{Решения}

\begin{itemize}
    \item Первый шаг алгоритма занимает время $O(n)$. Далее, рассмотрим сложность каждой итерации
        цикла. Вычисление одного центроида имеет сложность $O(2n) = O(n)$. Значит, вычисление всех
        центроидов имеет сложность $O(n \cdot c)$. Расчёт одной степени принадлежности $w_{ij}$, как
        видно из формулы \eqref{fuzzy-clustering-weight}, имеет сложность $O(c)$. Тогда всех
        степеней принадлежности -- $O(n \cdot c^2)$. Вычисление целевой функции занимает время
        $O(n \cdot c)$. Таким образом, сложность одной итерации цикла составляет
        $O(n \cdot c) + O(n \cdot c^2) + O(n \cdot c) = O(n \cdot c^2)$. Поскольку условие
        $J(W, C) > \varepsilon$ эквивалентно заданию фиксированного числа итераций, то сложность
        всего цикла также составляет $O(n \cdot c^2)$. Наконец, сложность алгоритма составляет
        $O(n) + O(n \cdot c^2) = O(n \cdot c^2)$.
    \item Рассмотрим отдельное слагаемое в сумме:
        \begin{equation*}
            \lim\limits_{m \to 1 + 0} \xi^{\frac{2}{m - 1}} =
            \lim\limits_{k \to +0} \xi^{\frac{2}{k}} =
            \lim\limits_{y \to +\infty} \xi^{y}
        \end{equation*}
        Из формулы \eqref{fuzzy-clustering-weight} и определения нормы следует, что
        $\xi = \dfrac{\lVert x_i - c_j \rVert}{\lVert x_i - c_k \rVert} \geq 0$. Поэтому
        \begin{equation*}
            \lim\limits_{y \to +\infty} \xi^{y} =
            \begin{cases}
                0,\ \xi \in [0; 1) \\
                1,\ \xi = 1 \\
                +\infty,\ \xi \in (1; +\infty) \\
            \end{cases}
        \end{equation*}
        Итак, в знаменателе правой части формулы \eqref{fuzzy-clustering-weight} имеем сумму
        слагаемых, одно из которых равно 1, а из остальных каждое стремится либо к 0, либо к
        $+\infty$. Таким образом, вся сумма стремится к 1 тогда и только тогда, когда все
        слагаемые, не равные 1, стремятся к нулю, иначе -- стремится к $+\infty$. Следовательно,
        $\lim\limits_{m \to 1 + 0}w_{ij} \in \{0, 1\}$, ч.т.д.
    \item Пусть $\forall i \in \{1, \ldots, n\}, j \in \{1, \ldots, c\} \hookrightarrow w_{ij} = w$.
        Тогда, согласно формуле \eqref{fuzzy-clustering-centroid},
        \begin{equation*}
            \forall j \in \{1, \ldots, c\} \hookrightarrow c_j =
            \frac{1}{|X|}\sum\limits_{i = 1}^{|X|} x_i := z
        \end{equation*}
        Далее, по формуле \eqref{fuzzy-clustering-weight}
        \begin{equation*}
            w_{ij} = \left(\sum\limits_{k = 1}^{c} 1^{\frac{2}{m - 1}}\right)^{-1} = \frac{1}{c}
        \end{equation*}
        Итак, все $w_{ij}$ снова одинаковы, и на следующей итерации ситуация повторится, значит,
        процесс не сойдётся. Таким образом, для сходимости алгоритма необходимо выбирать различные
        начальные приближения $w_{ij}$, ч.т.д.


\section{Сети Кохонена для кластеризации}

\subsection{Задача кластеризации (обучение без учителя)}
Кластеризация — это метод обучения без учителя, при котором требуется разделить данные на группы (кластеры) на основе их сходства.  \\
\textbf{Дано:}  \\
$X_\ell = \{ x_i \}_{i=1}^{\ell}$ — обучающая выборка объектов, $x_i \in \mathbb{R}^n$ — объект с $n$ признаками.\\
$\rho^2(x, w) = || x - w ||^2$ — евклидова метрика в $\mathbb{R}^n$, которая измеряет расстояние между объектом $x$ и центром кластера $w$.\\

\textbf{Найти:} \\
Центры кластеров $w_y \in \mathbb{R}^n$, $y \in Y$ — векторные представления центров кластеров.

Правило жесткой конкуренции (WTA, Winner Takes All) говорит о том, что объект $x$ относится к тому кластеру, чьё расстояние до объекта минимально. Это правило применяется в стандартных алгоритмах кластеризации:
\[
a(x) = \arg \min_{y \in Y} \rho(x, w_y)
\]
Критерий для выбора центров кластеров — минимизация среднего внутрикластерного расстояния:
\[
Q(w; X_\ell) = \sum_{i=1}^{\ell} \rho^2(x_i, w_{a(x_i)}) \to \min_{w_y : y \in Y}
\]
Это означает, что мы стремимся минимизировать сумму квадратов расстояний от каждого объекта до ближайшего кластера.

\subsection{Сеть Кохонена (сеть с конкурентным обучением)}
Сети Кохонена относятся к нейронным сетям с конкурентным обучением. В таких сетях нейроны (кластеры) "соревнуются" за право быть ближе к объектам, и только один или несколько нейронов обновляются на каждом шаге обучения.

Структура модели — двухслойная нейронная сеть. Верхний слой — это входные данные, а нижний — это нейроны, представляющие кластеры.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{scheme_kohonen.jpg}
\caption{Структура сети Кохонена}
\end{figure}

\textbf{Градиентный шаг в методе стохастического градиента (SG):}\\
Для выбранного объекта $x_i \in X_\ell$ выполняется обновление весов центра кластера $w_y$, к которому относится объект:
\[
w_y := w_y + \eta(x_i - w_y)
\]
Здесь $\eta$ — темп обучения, определяющий скорость обновления. Если объект $x_i$ относится к кластеру $y$, то центр этого кластера сдвигается в сторону объекта.

\subsection{Алгоритм обучения}
Алгоритм стохастического градиента (SG) используется для обучения сети Кохонена.\\

\textbf{Вход}: выборка $X_\ell$; темп обучения $\eta$; параметр $\lambda$.  \\
\textbf{Выход}: центры кластеров $w_y \in \mathbb{Р}^n$, $y \in Y$.

\begin{enumerate}
    \item Инициализировать центры кластеров $w_y$, $y \in Y$ случайным образом.
    \item Инициализировать оценку функционала: $Q := \sum_{i=1}^{\ell} \rho^2(x_i, w_{a(x_i)})$.
    \item Повторять:
        \begin{itemize}
            \item Выбрать объект $x_i$ из $X_\ell$ (например, случайно).
            \item Вычислить ближайший кластер: $y := \arg \min_{y \in Y} \rho(x_i, w_y)$.
            \item Градиентный шаг: $w_y := w_y + \eta(x_i - w_y)$, сдвигая центр кластера в сторону объекта.
            \item Оценить значение функционала: $Q := (1 - \lambda) Q + \lambda \rho^2(x_i, w_y)$.
        \end{itemize}
\end{enumerate}
Процесс повторяется, пока значение функционала $Q$ не стабилизируется, указывая на то, что обучение завершилось.

\subsection{Жесткая и мягкая конкуренция}
В модели с жесткой конкуренцией (WTA) только один кластер обновляется:
\[
w_y := w_y + \eta(x_i - w_y), \quad y \in Y
\]
Однако это может приводить к тому, что некоторые центры кластеров никогда не будут обновляться.

Для устранения этого недостатка используется правило мягкой конкуренции (WTM — Winner Takes Most):
\[
w_y := w_y + \eta(x_i - w_y) K(\rho(x_i, w_y)), \quad y \in Y
\]
Ядро $K(\rho)$ — неотрицательная невозрастающая функция, определяющая, насколько сильно каждый кластер обновляется в зависимости от его расстояния до объекта $x_i$. Чем дальше кластер от объекта, тем меньше его обновление.

\subsection{Задача классификации LVQ (Learning Vector Quantization)}
Learning Vector Quantization (LVQ) — это метод обучения с учителем, основанный на сетях Кохонена. Он используется для классификации, когда объекты имеют метки классов.

\textbf{Дано:}\\  
$X_\ell = \{ (x_i, y_i) \}_{i=1}^{\ell}$ — обучающая выборка, где $x_i \in \mathbb{Р}^n$ — объект, а $y_i \in Y$ — метка класса.\\
$C$ — множество кластеров, $y(c) \in Y$ — класс, к которому относится кластер $c \in C$.

\textbf{Найти:}  
Центры кластеров $w_c \in \mathbb{Р}^n$, $c \in C$ и модель классификации:
\[
a(x) = y(c(x))
\]
где $c(x)$ — ближайший кластер к объекту $x$, а $a(x)$ — предсказанный класс.

\textbf{Критерий:} \\
Задача состоит в том, чтобы минимизировать расстояния внутри кластеров для объектов одного класса и максимизировать для объектов разных классов:
\[
Q = \sum_{i=1}^{\ell} \rho^2(x_i, w_{c(x_i)}) \left( \mathbf{1}[y(c(x_i)) = y_i] - \mathbf{1}[y(c(x_i)) \neq y_i] \right) \to \min_{w_c : c \in C}
\]

\subsection{Обучаемое векторное квантование}
Метод обучаемого векторного квантования (Learning Vector Quantization, LVQ) представляет собой модель классификации, основанную на разбиении каждого класса на несколько кластеров. Это позволяет гибко описывать классы, даже если их форма сложна и не линейна. LVQ можно рассматривать как альтернативу байесовским классификаторам или методам, использующим GMM (Gaussian Mixture Model) — смесь гауссовских распределений, которые также позволяют моделировать сложные формы данных.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{clusters_kohonen.jpg}
\caption{Кластеры в сети Кохонена}
\end{figure}

\subsection{Задачи}
\subsubsection{Задача 1: Выбор метода кластеризации WTA или WTM для клиентов интернет-магазина}

\textbf{Дано:}
Рассмотрим задачу кластеризации клиентов интернет-магазина, основанную на данных о покупательском поведении. Имеются два кластера, и магазин хочет классифицировать новых клиентов, основываясь на исторических данных. Каждый клиент описывается двумя признаками: 

\begin{itemize}
    \item \textbf{Средняя сумма покупки} (в диапазоне от 100 до 500 у.е.).
    \item \textbf{Частота покупок} (в диапазоне от 1 до 20 покупок в месяц).
\end{itemize}

Цель состоит в том, чтобы выбрать метод кластеризации с использованием сетей Кохонена: \textbf{WTA (Winner-Takes-All)} или \textbf{WTM (Winner-Takes-Most)}, чтобы наилучшим образом разделить клиентов на два кластера — "активных покупателей" и "редких покупателей", исходя из вышеуказанных признаков.

Дополнительное условие: исторически известно, что у некоторых клиентов наблюдается пограничное поведение — их средняя сумма покупок нестабильна, и она может существенно колебаться, оставаясь между двумя кластерами. Частота покупок также может немного изменяться в разных месяцах.

\textbf{Данные:}
\begin{itemize}
    \item \textbf{Активные покупатели}: частота покупок — от 10 до 20, средняя сумма покупки — от 300 до 500 у.е.
    \item \textbf{Редкие покупатели}: частота покупок — от 1 до 9, средняя сумма покупки — от 100 до 300 у.е.
    \item \textbf{Пограничные клиенты}: частота покупок — от 7 до 10, средняя сумма покупки — от 250 до 350 у.е.
\end{itemize}

\textbfn{Решение:}
\textbf{Winner-Takes-All (WTA)}:
\begin{itemize}
    \item В этом методе ближайший к клиенту кластер побеждает и забирает весь вес. Это означает, что даже небольшой разброс данных не изменит решение, и каждый клиент будет жестко отнесен к тому кластеру, который находится ближе всего.
    \item \textbf{Плюсы}: Быстрое и четкое отнесение клиентов к одному кластеру. Хорошо подходит для случаев, когда данные четко разделены.
    \item \textbf{Минусы}: Не учитываются незначительные колебания данных (особенно у пограничных клиентов). Пограничные клиенты могут быть некорректно отнесены к "активным" или "редким" покупателям, что приведет к неправильной сегментации.
\end{itemize}

\textbf{Winner-Takes-Most (WTM)}:
\begin{itemize}
    \item В этом методе несколько ближайших кластеров получают долю веса в зависимости от их расстояния до клиента. Это приводит к более мягкому учету пограничных случаев.
    \item \textbf{Плюсы}: Позволяет лучше учитывать клиентов с пограничным поведением, давая возможность частичного участия в обоих кластерах. Более гибкая кластеризация, подходящая для случаев, когда данные имеют некоторое размытое распределение.
    \item \textbf{Минусы}: Может усложнить интерпретацию, так как один клиент может быть частично отнесен сразу к нескольким кластерам.
\end{itemize}

\textbf{Оценка данных}: 
Клиенты, которые четко относятся к одной из двух категорий (активные или редкие покупатели), имеют устойчивые значения признаков, что позволяет эффективно использовать метод WTA. 

\textbf{Проблема возникает с пограничными клиентами}, чьи значения частоты покупок и средней суммы находятся между кластерами. В случае использования WTA такие клиенты могут быть жестко отнесены к одному кластеру, даже если их поведение не является достаточно устойчивым для точного отнесения к активным или редким покупателям. Это может исказить результаты сегментации.

\textbf{Заключение:}
В данной задаче \textbf{WTM (Winner-Takes-Most)} является более предпочтительным методом, так как он позволяет учесть пограничное поведение клиентов и сгладить влияние неустойчивых данных. В случае применения WTA, пограничные клиенты будут жестко отнесены к одному кластеру, что приведет к снижению точности кластеризации. Таким образом, метод WTM будет лучшим выбором, поскольку он дает возможность учитывать неопределенность и вариативность данных клиентов.

\subsubsection{Задача 2: Выбор метода кластеризации для различных задач}

\textbf{Дано:}
Перед вами стоит задача выбора метода кластеризации для разделения данных на несколько групп. Рассмотрены следующие методы кластеризации:
\begin{itemize}
    \item \textbf{Нейронная сеть Кохонена}.
    \item \textbf{K-средних (K-means)}.
    \item \textbf{Иерархическая кластеризация (Hierarchical Clustering)}.
\end{itemize}

Для каждого метода необходимо привести его плюсы и минусы, а также обосновать, в каких случаях каждый из методов будет предпочтителен.

\subsubsection{Решение:}

\textbf{1. Нейронная сеть Кохонена}:
\begin{itemize}
    \item \textbf{Плюсы:}
    \begin{itemize}
        \item \textbf{Сокращение размерности:} сеть Кохонена автоматически проецирует многомерные данные на двумерную решетку, что позволяет визуализировать высокоразмерные данные. Это особенно полезно для анализа сложных данных.
        \item \textbf{Нелинейное разделение:} сеть Кохонена может кластеризовать данные, которые не разделяются линейно, что делает его более гибким для задач с неявными границами между кластерами.
        \item \textbf{Интерпретируемость:} сеть Кохонена создает топологическую карту, где схожие объекты оказываются близко друг к другу, что позволяет увидеть, какие данные похожи и как распределены кластеры.
        \item \textbf{Мягкая конкуренция:} Сети Кохонена поддерживают мягкую конкуренцию между нейронами, что позволяет учитывать схожесть объектов в соседних кластерах.
    \end{itemize}
    \item \textbf{Минусы:}
    \begin{itemize}
        \item \textbf{Трудоемкость настройки:} Требует тщательной настройки параметров, таких как размеры решетки и скорость обучения.
        \item \textbf{Чувствительность к выбору начальных весов:} Результаты могут зависеть от начальной инициализации, что иногда приводит к неоптимальным решениям.
        \item \textbf{Трудности с выбором числа кластеров:} Метод не предоставляет четкого способа выбора количества кластеров.
    \end{itemize}
\end{itemize}

\textbf{2. Метод K-средних (K-means)}:
\begin{itemize}
    \item \textbf{Плюсы:}
    \begin{itemize}
        \item \textbf{Простота и быстрота:} K-means прост в реализации и эффективен при больших объемах данных.
        \item \textbf{Легкость интерпретации:} Метод делит данные на четкие кластеры, что упрощает анализ и интерпретацию результатов.
        \item \textbf{Хорошо работает с линейно разделимыми данными:} K-means подходит для данных, где кластеры можно разделить прямыми границами.
    \end{itemize}
    \item \textbf{Минусы:}
    \begin{itemize}
        \item \textbf{Чувствительность к форме кластеров:} Метод плохо работает с данными, где кластеры имеют сложную форму или сильно перекрываются.
        \item \textbf{Чувствительность к выбросам:} Выбросы могут сильно повлиять на центроиды кластеров.
        \item \textbf{Необходимость выбора числа кластеров:} Требуется заранее задавать количество кластеров, что не всегда очевидно.
    \end{itemize}
\end{itemize}

\textbf{3. Иерархическая кластеризация (Hierarchical Clustering)}:
\begin{itemize}
    \item \textbf{Плюсы:}
    \begin{itemize}
        \item \textbf{Гибкость в выборе числа кластеров:} Иерархическая кластеризация позволяет на любом этапе "отрезать" дерево и выбрать нужное количество кластеров.
        \item \textbf{Не требует заранее определенного числа кластеров:} Этот метод не требует от пользователя указания числа кластеров заранее.
        \item \textbf{Дает информацию о структуре данных:} Результаты представлены в виде дендрограммы, что позволяет увидеть иерархию кластеров и их отношения.
    \end{itemize}
    \item \textbf{Минусы:}
    \begin{itemize}
        \item \textbf{Высокая вычислительная сложность:} Алгоритм может быть неэффективен для очень больших наборов данных.
        \item \textbf{Чувствительность к шуму:} Иерархические методы могут быть чувствительны к выбросам и шуму в данных.
        \item \textbf{Сложность в интерпретации:} Дендрограмма может быть трудной для интерпретации, особенно если структура данных сложная.
    \end{itemize}
\end{itemize}

\subsubsection{Задача 3: Сравнение архитектур для кластеризации}

\textbf{Дано:}

Представьте, что вам нужно применить два различных подхода для кластеризации данных: один с использованием сети Кохонена, а другой с использованием автокодировщика (autoencoder). Оба подхода решают задачу кластеризации на данных о изображениях. Задание состоит в том, чтобы указать основные архитектурные отличия между сетью Кохонена и автокодировщиком, которые могут повлиять на результаты кластеризации, и объяснить, как эти различия влияют на процесс обучения и кластеризацию.

\textbf{Решение:}

\textbf{1. Сеть Кохонена:}

\begin{itemize}
    \item \textbf{Архитектура:} Это тип нейронной сети, состоящий из одномерной или двумерной решетки нейронов, каждый из которых имеет веса, соответствующие входным данным. На вход сети подаются данные, и нейрон, чьи веса наиболее похожи на входной вектор, становится «победителем» (наиболее конкурентоспособным). Нейроны, расположенные рядом с победителем, также обновляют свои веса, что стимулирует близкие кластеры.
    \item \textbf{Обучение:} Используется метод обучения без учителя, с конкуренцией между нейронами (жесткая конкуренция). Кластеры формируются на основе минимизации расстояния между нейронами и входными векторами.
    \item \textbf{Отличие:} Основной особенностью сети Кохонена является принцип конкуренции, где нейроны обучаются на основе «победителя», и близкие нейроны обновляют свои веса. Это позволяет сетке создавать топологически осмысленные кластеры.
\end{itemize}

\textbf{2. Автокодировщик (autoencoder):}

\begin{itemize}
    \item \textbf{Архитектура:} Автокодировщик состоит из двух частей: энкодера и декодера. Энкодер принимает входные данные и сжимает их в более низкоразмерное пространство (латентное пространство), а декодер восстанавливает данные обратно в исходное пространство. Автокодировщики могут быть обучены с использованием метода обратного распространения ошибки для минимизации разницы между входом и восстановленным выходом.
    \item \textbf{Обучение:} Автокодировщик использует метод обучения с учителем (по сути, обучается на восстановление данных), что отличается от обучения без учителя в сети Кохонена. Он минимизирует ошибку восстановления, а не конкуренцию между нейронами.
    \item \textbf{Отличие:} Главным отличием является наличие явной цели для восстановления входных данных через латентное пространство. Это приводит к тому, что автокодировщик учит сеть выделять наиболее значимые признаки для восстановления данных, что влияет на кластеризацию.
\end{itemize}

\section*{Трансдуктивный метод опорных векторов TSVM}

\subsection{Общее описание метода}

Трансдуктивный метод опорных векторов (TSVM) является частью методов обучения с учителем, которые применяются к задачам, где часть данных размечена, а часть — нет. В отличие от классического метода опорных векторов (SVM), который использует только размеченные данные, TSVM может эффективно работать как с размеченными, так и с неразмеченными данными.

Пусть у нас есть данные:
\[
    X_k = \{x_1, x_2, \dots, x_k\} \quad \text{— размеченные данные},
\]
\[
    y_k = \{y_1, y_2, \dots, y_k\} \quad \text{— метки для размеченных данных},
\]
\[
    U = \{x_{k+1}, x_{k+2}, \dots, x_\ell\} \quad \text{— неразмеченные данные}.
\]

Цель метода — найти классификатор \( a(x, w) \), который минимизирует общую ошибку на размеченных данных, а также штрафует классификатор за неопределенность на неразмеченных данных.

Определение классификатора:
\[
    a(x, w) = \text{sign}(\langle w, x \rangle - w_0).
\]

Функционал потерь для TSVM включает обе ошибки:
\[
    \sum_{i=1}^{k} L(a(x_i, \textcolor{red}{w}), y_i) + \lambda \sum_{i=k+1}^{\ell} L_U(a(x_i, \textcolor{red}{w})) \to \min_\textcolor{red}{w},
\]
где:\\
 \( L(a(x_i, \textcolor{red}{w}), y_i) \) — функция потерь для размеченных данных,\\
 \( L_U(a(x_i, \textcolor{red}{w})) \) — штрафная функция для неразмеченных данных,\\
 \( \lambda \) — параметр, регулирующий важность штрафа за неопределенность.\\

Суть в том, чтобы параметризовать $\textcolor{red}{w}$ в двух критериях. 
\subsection{Идея для штрафа из классификатора SVM}
    Пусть у нас есть линейный классификатор на два класса \( Y = \{-1, 1\} \):
    \[
        a(x) = \text{sign} \left( \langle w, x \rangle - w_0 \right), \quad w, x \in \mathbb{R}^n
    \]
    
    \textbf{Напоминание: Отступ (margin) объекта \( x_i \)} — это мера того, насколько правильно классифицирован объект, с учетом расстояния до разделяющей гиперплоскости. Для объекта \( x_i \) с меткой \( y_i \) отступ определяется как:
    \[
        M_i(w, w_0) = \left( \langle w, x_i \rangle - w_0 \right) y_i.
    \]
    Если отступ положителен и больше 1, объект классифицирован правильно и лежит за пределами разделяющей полосы, что свидетельствует о хорошей уверенности в классификации. Если отступ меньше 1, объект либо лежит внутри разделяющей полосы, либо ошибочно классифицирован.

    \textbf{Задача обучения} весов \( w, w_0 \) нашего классификатора на размеченной выборке ставится следующим образом:
    \[
        Q(w, w_0) = \sum_{i=1}^{k} \left( 1 - M_i(w, w_0) \right)_+ + \frac{1}{2C} \|w\|^2 \to \min_{w, w_0}.
    \]
    где функция потерь \( L(M) = (1 - M)_+ \) штрафует за объекты, которые оказываются слишком близко к разделяющей гиперплоскости или ошибочно классифицированы (т.е. \( M_i < 1 \)).

    \vspace{1em}
    \textbf{Идея для метода TSVM}

    Таким образом получаем идею, реализованную в методе трансдуктивной SVM (TSVM), где используется штраф для объектов, которые неразмечены. В TSVM требуется учесть неопределенные данные, вводя штраф за попадание объекта внутрь разделяющей полосы, что настраивается параметром \( \gamma \). Для этого используется модифицированная функция штрафа для неразмеченных данных:
    \[
        L_U(M) = \left( 1 - |M| \right)_+,
    \]
    где \( M \) — это отступ объекта. Такой штраф вводится для того, чтобы минимизировать неопределенность классификации и корректно учитывать информацию о неразмеченных данных в процессе обучения модели.
    
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{MLbook/chapters/clustering/png/TSVMLoss.jpg}
    \caption{Вид функции штрафа для неразмеченных данных.}
    \end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{MLbook/chapters/clustering/png/ClassifierData.jpg}
    \caption{Пример распределения точек около границы.}
\end{figure}

\subsection{Математическое описание трансдуктивной задачи}
С математической точки зрения новая трансдуктивная задача описывается следующим образом:

\begin{equation}
    Q(w, w_0) = \sum_{i=1}^{k} \left( 1 - \textcolor{red}{M_i(w, w_0)} \right)_+ + \frac{1}{2C} \|w\|^2 + \gamma \sum_{i=k+1}^{\ell} \left( 1 - |\textcolor{red}{M_i(w, w_0)}| \right)_+ \to \min_{\textcolor{red}{w, w_0}}.
\end{equation}

Данный подход позволяет учитывать как известные, так и неизвестные (неразмеченные) данные для более точного обучения модели.

\textbf{Основные преимущества и недостатки TSVM}:
\begin{itemize} 
    \item \textbf{Преимущества:} 
    \begin{itemize} 
        \item Как и в обычном SVM, можно использовать ядра для работы с нелинейными задачами. 
        \item Имеются эффективные реализации, которые позволяют работать с большими объемами данных. 
    \end{itemize}
        
    \item \textbf{Недостатки:}
    \begin{itemize}
        \item Задача оптимизации TSVM не является выпуклой, что делает методы оптимизации более сложными.
        \item Решение может быть неустойчивым, если нет области разреженности в данных.
        \item Требуется настройка двух параметров: \( C \) и \( \gamma \).
    \end{itemize}

    \item \textbf{Применение:} 
    TSVM используется в задачах, где важно учитывать как размеченные, так и неразмеченные данные для повышения точности классификации. Это делает метод особенно полезным для работы с большими объемами неразмеченных данных, которые часто встречаются в реальных приложениях.
\end{itemize}

\subsection{Задачи}
\subsubsection*{Задача 1}
\textbf{Условие:} Покажите на примере двух точек $(1/3, 1/3)$ и $(-2, -2)$ и линейного классификатора  с $w_0 = 0, w_1 = 1, w_2 = 1$ как именно считается модифицированная функция штрафа $L_U(M)$. \\
\textbf{Решение:}
Классифицирующей поверхностью в нашем случае является прямая $y = -x$ с соответствующей полосой, поэтому можем взять точки $(1/3, 1/3)$ и $(-2, -2)$ для того, чтобы продемонстрировать требуемое. Для них соответственно получаем:
\[
    L_1 = (1 - |(1/3*1 + 1/3*1 + 0)|)_+ = 1/3
\]

\[
    L_2 = (1 - |((-2)*1 + (-2)*1 + 0)|)_+ = 0
\]
\[
    L_{all} = 1/3 + 0 = 1/3
\]
Мы умножаем M на 1 вне зависимости от знака класса, т.к. в формуле стоит модуль, это опустили.
\subsubsection*{Задача 2}
\textbf{Условие:}
В рамках предыдущей задачи покажите, что для классификатора с теми же $w_1, w_2$, но $w_0 = -1$ получившийся штраф будет меньше. Соответствует ли это лучшей разделимости точек? Продемонстрируйте картинки с пояснениями.\\
\textbf{Решение:}
Аналогично будем иметь:
\[  
    L_1 = (1 - |(1/3*1 + 1/3*1 + 1)|)_+ = 0
\]

\[
    L_2 = (1 - |((-2)*1 + (-2)*1 + 1)|)_+ = 0
\]

\[
    L_{all} = 0 + 0 = 0
\]
Таким образом штраф уменьшился, и как видно из картинок, точки стали распределены вне линии разделения:
\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{MLbook/chapters/clustering/png/TSVM_3task1.jpg}
        \caption{$w_0 = 0$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{MLbook/chapters/clustering/png/TSVM_3task2.jpg}
        \caption{$w_0 = -1$}
    \end{minipage}
\end{figure}

\subsubsection*{Задача 3}
\textbf{Условие:}
Можно ли использовать метод TSVM для постепенного самообучения, размечая данные по мере их поступления? Какие преимущества и недостатки могут быть у такого подхода?\\
\textbf{Решение:}\\
\textbf{Преимущества:}
\begin{itemize}
    \item \textbf{Эффективное использование неразмеченных данных:} Постепенное добавление размеченных данных на основе текущих предсказаний TSVM позволяет улучшить модель по мере поступления новых данных. Это может быть полезно, если размечать данные вручную дорого или занимает много времени.
    \item \textbf{Гибкость в обучении:} Метод TSVM может адаптироваться к новым данным и улучшать границу классификации, не требуя полного перерасчета модели с нуля каждый раз. Это особенно полезно в задачах, где данные поступают по мере времени (например, в потоках информации).
    \item \textbf{Обучение на малых объемах данных:} TSVM позволяет начать с небольшого набора размеченных данных и постепенно расширять его, используя информацию о неразмеченных примерах. Это важно, когда размеченные данные труднодоступны или ограничены.
\end{itemize}\\
\textbf{Недостатки:}
\begin{itemize}
    \item \textbf{Риски неправильной разметки:} Одной из проблем такого подхода является риск неверной или неточной разметки данных в процессе самообучения. Если начальная модель не обладает высокой точностью, ошибки в разметке могут привести к накоплению ошибок в дальнейшем обучении.
    \item \textbf{Проблемы с неопределенностью:} Неопределенные данные (примеры, которые находятся близко к границе принятия решения) могут быть ошибочно размечены. TSVM решает эту проблему через регуляризацию, но все же может столкнуться с трудностями при обработке сложных случаев, когда граница классификации неясна.
    \item \textbf{Зависимость от начальных данных:} Результат обучения сильно зависит от начального набора размеченных данных. Если эти данные не являются репрезентативными, то модель может в дальнейшем плохо обрабатывать новые примеры.
\end{itemize}

\section*{Алгоритм Ланса-Уильямса}

Пусть $X = \{\{x_1\},..., \{x_l\}\}$ - исходная выборка, $l$ - количество её элементов, необходимо объединить объекты выборки в кластере на основе расстояний между ними. Алгоритм инициализируется таким образом, что каждый объект выборки считается отдельным кластером, затем происходит итеративное слияние кластеров друг с другом. Критерием, по которому кластеры объединяются, является расстояние между ними. %$R(\{x_i\}, \{x_j\}) = \rho (x_i, x_j)$.
Существует множество способов выбора метрики расстояния между кластерами, однако большинство классических метрик можно обобщить, используя формулу Ланса-Уильямса.\\
Пусть в процессе вычисления имеется набор кластеров $C_t,t \in \{1, \l\}$, $U, V$ - кластеры, которые были выбраны для слияния, $W = U \cup V$ , тогда для произвольного кластера $S \in C_t\ \cup \{W\} \textbackslash \{U, V\}$ расстояние $R_{WS}$ до кластера $W$ определяется по следующей формуле.

\begin{equation*}
        R_{WS} = \alpha_U \cdot R_{US} + \alpha_V \cdot R_{VS} + \beta \cdot R_{UV} + \gamma \cdot |R_{US} - R_{VS}|
\end{equation*}

где $R_{US}, R_{VS}, R_{UV}$ - расстояния между кластерами $U$ и $S$, $V$ и $S$, $U$ и $V$ соответственно, $\alpha_U, \alpha_V, \beta, \gamma$ - некоторые действительные коэффициенты. Рассмотрим стандартные способы задания метрики расстояния между множествами и покажем, какие значения коэффициентов соответствуют им:

\begin{itemize}
    \item Расстояние до ближайшего соседа: $R^{\text{б}}_{WS} = \underset{w \in W, s \in S}{min}
  \rho (w, s)$; $\alpha_U = \alpha_V = \frac{1}{2}, \beta = 0, \gamma = - \frac{1}{2}$
    \item Расстояние до наиболее удалённого соседа: $R^{\text{д}}_{WS} = \underset{w \in W, s \in S}{max} \rho (w, s)$; $\alpha_U = \alpha_V = \frac{1}{2}, \beta = 0, \gamma = \frac{1}{2}$
    \item Среднее расстояние между элементами кластера: $R^{\text {с}}_{WS} = \frac{1}{|W||S|} \sum_{w \in W} \sum_{s \in S} \rho (w, s)$; $\alpha_U = \frac{|U|}{|W|}, \alpha_V = \frac{|V|}{|W|}, \beta = \gamma = 0$
    \item Расстояние между центрами: $R^{\text{ц}}_{WS} = \rho^2 \left( \sum_{w \in W} \frac{w}{|W|}, \sum_{s \in S} \frac{s}{|S|} \right)$; $\alpha_U = \frac{|U|}{|W|}, \alpha_V = \frac{|V|}{|W|}, \beta = \alpha_U \cdot \alpha_V, \gamma = 0$
    \item Расстояние Уорда: $R^{\text{у}}_{WS} = \frac{|S||W|}{|S|+|W|} \rho^2 \left( \sum_{w \in W} \frac{w}{|W|}, \sum_{s \in S} \frac{s}{|S|} \right)$; $\alpha_U = \frac{|S| + |U|}{|S|+|W|}, \alpha_V = \frac{|S|+|V|}{|S|+|W|}, \beta = \frac{-|S|}{|S|+|W|}, \gamma = 0$
\end{itemize}

Таким образом, данная метрика действительно является обобщением множества классических способов определения расстояния между кластерами. Ниже представлен псевдокод алгоритма Ланса-Уильямса.

\begin{algorithm}
\caption{Агломеративная кластеризация Ланса-Уильямса}\label{Lance–Williams}
\begin{algorithmic}
\State $t:=1$: $C_t = \{\{x_1\},...,\{x_l\}\}$
\ForEach {$s \in \mathcal S $}
\State найти в $C_{t - 1}$ пару кластеров $U, V$ с минимальным $R_{UV}$
\State $W = U \cup V;$
\State $C_t = C_{t - 1} \cup \{W\} \textbackslash \{U, V\};$
\ForEach {$S \in C_t$}
\State $R_{WS} = \alpha_U \cdot R_{US} + \alpha_V \cdot R_{VS} + \beta \cdot R_{UV} + \gamma \cdot |R_{US} - R_{VS}|$

\end{algorithmic}
\end{algorithm}

На выходе алгоритма получается кластер, состоящий из всех элементов исходной выборки, разделённый иерархически на подкластеры. Дальнейшей задачей является выделение основных подкластеров из данного набора кластеров для дальнейшей работы с ними. Это удобно сделать, используя дендрограмму - график, визуализирующий расстояние между кластерами, образующимися в процессе вычислений.

\begin{center}
\begin{figure}[H]
\includegraphics[width=14cm]{png/Dendrogramma.png}
\caption{Визуализация объектов кластеров и соответствующая им дендрограмма для алгоритма Ланса-Уильямса, в качестве расстояния используется расстояние Уорда}
\label{fig:Dendrogramma}
\end{figure}
\end{center}

На данной дендрограмме можно провести горизонтальную линию по уровню $R_0$, что и определит конечную структуру кластеров. Проще всего сделать это путём отсечения правого участка дендрограммы. На горизонтальной оси находится интервал максимальной длины $|R_{t+1} - R_t|$, в качестве результирующей кластеризации выдаётся множество кластеров $C_t$, их число $K = l - t + 1$. Для того, чтобы улучшить качество кластеризации, необходимо, чтобы метрика расстояния обладала определёнными свойствами.

\subsection*{Монотонность}

Функция расстояния $R$ обладает свойством монотонности, если при каждом слиянии расстояние между объединяемыми кластерами увеличивается: $R_2 \leq R_3 \leq...\leq R_l$. В таких случая дендрограмму можно построить без самопересечений, что в свою очередь позволяет выделить чёткую кластерную структуру. Существует достаточное условие монотонности функции расстояния:

\textbf{Теорема Миллигана.} Если выполняются следующие 3 условия, то кластеризация является монотонной
\begin{itemize}
    \item $\alpha_U \geq 0, \alpha_V \geq 0$;
    \item $\alpha_U + \alpha_V + \beta \geq 1;$
    \item $min \{{\alpha_U, \alpha_V}\} + \gamma \geq 0$
\end{itemize}

\subsection{Свойства растяжения и сжатия}

Фукнция расстояния $R$ обладает свойством растяжения, если при каждом слиянии расстояние от него до других кластеров увеличивается. Функция расстояния $R$ обладает свойством сжатия, если при каждом слиянии расстояние от него до других кластеров уменьшается. Растяжение способствует более чёткому отделению кластеров, однако при избыточно слишком большом растяжении некоторые кластеры находятся там, где их изначально не было. На практике используется гибкое расстояние, являющееся компромиссом между методами ближнего, дальнего соседа и среднего расстояния, регуляруется параметром $\beta$: $\alpha_U = \alpha_V = (1 - \beta)/2, \beta < 1$. При $\beta > 0$ расстояние является сжимающим, при $\beta < 0$ растягивающим. Более точный подбор $\beta$ проводится на основе анализа конкретной задачи. 

В целом, нельзя однозначно сказать, какой выбор коэффициентов даст лучший результат. Среди перечисленных расстояний наиболее стабильным принято считать расстояние Уорда. Однако для улучшения качества кластеризации рекомендуется исследовать алгоритм на точность при различных вариантах метрики. 

\subsection*{Задачи на данную тему}

\subsubsection*{Задача 1} 
Сгенерируйте 2 двухмерых гауссиана с единичными матрицами ковариации, но различными векторами средних. Используйте алгоритм Ланса-Уильямса для кластеризации получившихся наборов точек. Проведите вычисления для разных значений $|\Vec{\mu_1} - \Vec{\mu_2}|$, где $\Vec{\mu_1}$ - вектор среднего для первого гауссиана, $\Vec{\mu_2}$ - вектор среднего для второго гауссиана. Сравните полученные результаты с результатами, получаемыми с помощью алгоритма восстановления плотности вероятности. 

\subsubsection*{Задача 2} 
Возьмите статистику по среднему баллу по базовым кафедральным предметам, количество времени, проводимого на кафедре и на работе/стажировке, а также зарплату для всех студентов вашего потока. Используйте алгоритм Ланса-Уильямса и оцените, насколько часто студенты одной кафедры оказываются в одном кластере. 

\subsubsection*{Задача 3} 
Возьмите данные о поведение акций Мосбиржи за последний месяц, для каждой акции определите такие показатели, как средняя цена, средняя прибыль, размер дивидентов (при выплате) и др., затем проведите кластеризацию. Какие выводы можно сделать на основании полученных данных? Можно ли определить, какие бумаги более предпочтительны для покупки.

\section*{Многомерное шкалирование}

\subsection*{Постановка проблемы многомерного шкалирования}

В большинстве задач, связанных с задачами машинного обучения приходится иметь дело с пространствами признаков больших размерностей (порядка сотен). В этом случае, если рассматривать, к примеру, задачи классификации, бывает важно визуально воспринять качество кластеризации.  Это позволяет исследователям визуально анализировать данные, выявлять кластеры, аномалии и другие закономерности, которые могут быть скрыты в исходном высокоразмерном представлении.

Задача многомерного шкалирования заключается в уменьшении размерности пространства с сохранением структуры расстояний в исходном. Теперь сформулируем задачу формально. Пусть имеется обучающая выборка $X^l=\{x_1, ..., x_l\} \subset X$. Заданы расстояния $R_{ij} = \rho(x_i, x_j)$ для некоторых пар обучающих объектов $(i, j) \in D$. Необходимо для каждого объекта $x_i \in X^l$ построить его признаковое описание в пространстве $\mathbb{R}^n$ размерности $n < l$., то есть вектор $x_i = (x_i^1, ..., x_i^n)$. Основным условием является то, чтобы евклидовы расстояния в $\mathbb{R}^n$:

$$d_{ij}^2= \sum_{k = 1}^n (x_i^k - x_j^k)^2$$

максимально приближали исходные расстояния $R_{ij}$. Один из самых распространенных подходов формализации понятия приближения это минимизация \textit{функционала стресса}:

$$S(X^l) = \sum_{(i, j) \in D} \omega_{ij}(d_{ij}- R_{ij})^2 \rightarrow \mathrm{min}$$

где минимум берется по совокупности переменных $(x_i^k)_{i=1,l}^{k=1,n}$.

На практике чаще всего используется $n=2$ для визуализации кластерной структуры на плоском графике. Чаще всего такое представление искажено $(S > 0)$, но все равно позволяет качественно оценивать структуру данных и ее особенности, выделять закономерности, в том числе и кластерную струкуру. Поэтому двумерное шкалирование часто используют как	инструмент предварительного анализа и понимания данных. 

Веса $\omega_{ij}$ задаются исходя из целей шкалирования. Обычно берут $\omega_{ij} = (R_{ij} )^\gamma$.	При $\gamma < 0$ выделяются и более точно приближаются малые расстояния;	при $\gamma > 0$ наоборот большие. Наиболее адекватным считается значение $\gamma = -2$,
когда функционал стресса приобретает физический смысл потенциальной энергии	системы $l$ материальных точек, связанных упругими связями; требуется найти равновесное состояние системы, в котором потенциальная энергия минимальна.

Функционал стресса $S(X^l)$ сложным образом зависит от $ln$ переменных, имеет огромное количество локальных минимумов, и его вычисление довольно трудоёмко.	Поэтому многие алгоритмы многомерного шкалирования основаны на итерационном	размещении объектов по одному. На каждой итерации оптимизируются евклидовы координаты только одного из объектов при фиксированных остальных объектах.

Отдельно стоить отметить, что вычислительная сложность классических алгоритмов МШ, таких как алгоритм Камерона-Гуверта, квадратична по отношению к числу объектов, что ограничивает их применение для больших наборов данных. Тем более, как сказано выше, в контексте предварительном анализе данных, которые не должен быть долгим. Поэтому были разработаны субквадратичные методы, которые значительно ускоряют процесс вычислений.

\subsection*{Пример субквадратичного алгоритма МШ}

Приведем сразу псевдокод алгоритма и после разберем принцип работы:


\begin{algorithm}[H]
	\caption{Субквадратичный алгоритм}
	\KwIn{\\ $R_{ij}$ — матрица попарных расстояний между объектами, возможно, разреженная; \\ $K$ — размер скелета;}
	\KwOut{евклидовы координаты всех объектов выборки $x_i = (x_i^1, ..., x_i^n),\ i = 1, . . . , l$}
	
	Инициализировать скелет: $U :=$ три достаточно далёкие друг от друга точки
	
	\While{$|U| < K$}{
		Наращивать скелет:
		
		$x := \underset{x_i \in X^l  \setminus U}{\mathrm{argmax}}(\underset{x_j \in U}{\min} R_{ij})$
		
		Ньютон-Рафсон$(x, U)$
		
		$U := U \cup \{x\}$
	}
	
	\While{координаты точек скелета не стабилизируются}{
		Найти наиболее напряженную точку в скелете:
		
		$x := \underset{x_i \in U}{\mathrm{argmin}} S(x)$
		
		Ньютон-Рафсон$(x, U \setminus \{x\})$
	}
	
	\For{$x \in X^l \setminus U$}{
		Ньютон-Рафсон$(x, U)$
	}
\end{algorithm}

Алгоритм многомерного шкалирования начинается с приближенного поиска двух наиболее удаленных точек данных. Для этого достаточно нескольких итераций (обычно 3-4): выбирается произвольная точка $x_i$, затем самая удаленная от нее $x_j$, и так далее. Найденным точкам присваиваются координаты $(0, 0)$ и $(0, R_{ij} )$, где $R_{ij}$ — расстояние между ними. Затем определяется третья точка, наиболее удаленная от первых двух. Ее евклидовы координаты $x_k$ определяются (в двумерном случае) исходя из того, что треугольник $\triangle ijk$
жёстко задан длинами своих сторон.

Далее, точки добавляются последовательно, размещаясь относительно уже определенных. Первые $K$ точек, размещенных с высокой точностью, образуют "скелет". Остальные точки размещаются относительно этого скелета, игнорируя расстояния между самими "не-скелетными" точками. Если $K$ равно общему числу точек $l$, то все точки входят в скелет, обеспечивая не только максимальную точность, но и максимальную вычислительную сложность $O(l^2)$. В общем случае сложность алгоритма составляет $O(K^2) + O(Kl)$, что позволяет регулировать точность и время вычислений путем выбора размера скелета $K$.


Алгоритм начинает с того, что находит две самые удалённые точки выборки $x_i, x_j$. Достаточно	решить эту задачу приближённо. В частности, можно взять произвольную точку,	найти самую удалённую от неё, затем для этой точки найти самую удалённую, и так	далее. Обычно 3–4 итераций хватает, чтобы найти пару достаточно далёких точек.	Найденным точкам приписываются (в двумерном случае) евклидовы координаты
$(0, 0)$ и $(0, R_{ij} )$. Затем находится третья точка $x_k$, наиболее удалённая от первых двух, то есть для которой значение $\min \{R_{ki}, R_{kj}\}$ максимально. 

\subsection*{Карта сходства}

Двумерное представление результатов многомерного шкалирования (МШ) — карта сходства — представляет собой точечный график, где положение точек отражает расстояния между объектами в исходных данных. Поскольку евклидова метрика и стресс (мера искажения) инвариантны к ортогональным преобразованиям (сдвигам, поворотам, отражениям), оси карты не имеют самостоятельной интерпретации. Для понимания структуры данных на карте указываются ориентиры — объекты с известными свойствами. Если искажения малы, то близко расположенные к ориентирам точки должны иметь схожие характеристики. Карта сходства дает лишь качественное представление о взаимном расположении объектов и не подходит для количественного анализа.

\begin{figure}[ht]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{MLbook/chapters/clustering/png/tsne-mnist.png}
		\caption{Пример применения алгоритма МШ TSNE на датасете MNIST}
	\end{minipage}
\end{figure}

\subsection*{Диаграмма Шепарда}

Диаграмма Шепарда позволяет сказать, насколько сильно искажены расстояния на карте сходства. Это точечный график; по горизонтальной оси откладываются исходные расстояния $R_{ij}$ ; по вертикальной оси откладываются евклидовы расстояния $d_{ij}$ ; каждая точка на графике соответствует некоторой паре объектов $(i, j) \in D$. Если число пар превышает несколько тысяч, отображается случайное подмножество	пар. Иногда на диаграмме изображается сглаженная зависимость $d_{ij} (R_{ij} )$, а также	сглаженные границы верхних и нижних доверительных интервалов, в которых $d_{ij} (R_{ij})$	находится с высокой вероятностью при каждом значении R.	Идеальной диаграммой Шепарда является наклонная прямая — биссектриса	первой четверти. Чем «толще» облако точек, представленное на диаграмме, тем сильнее искажения, и тем меньшего доверия заслуживает карта сходства.

\subsection*{Задачи на данную тему}

\paragraph*{Задача 1}

Рассмотрим алгоритм МШ, описанный в тексте. Объясните, почему выбор размера скелета K является компромиссом между точностью и вычислительной сложностью. Какие факторы следует учитывать при выборе оптимального значения K для конкретного набора данных? Как влияет приближенный поиск двух наиболее удаленных точек на точность конечного результата?	


\paragraph*{Задача 2}

Представьте, что вы получили карту сходства и диаграмму Шепарда для некоторого набора данных. На карте сходства наблюдается значительный разброс точек, а на диаграмме Шепарда — большое "облако" точек вокруг биссектрисы. Как вы интерпретируете эти результаты? Что это говорит о качестве полученного низкоразмерного представления данных? Какие действия можно предпринять для улучшения результатов МШ?

\paragraph*{Задача 3}

Предложите модификацию описанного алгоритма МШ, которая могла бы улучшить его производительность или точность. Например, можно рассмотреть альтернативные методы поиска начальных точек, использование других методов оптимизации вместо Ньютона-Рафсона, или включение механизма для учета расстояний между "не-скелетными" точками. Обоснуйте предложенные изменения и оцените их потенциальное влияние на вычислительную сложность и точность.

\maketitle
\begin{theorem}[Шура]
    Для любого целого $k > 0$ существует такое целое $r > 0$, что для любой раскраски первых r натуральных чисел в k цветов найдется одновцветное решение уравнения $x + y = z$.
\end{theorem}

\begin{proof}
Положим $r = R(\underbrace{3, 3, ..., 3}_{\textrm{k}})$.

Положим $\varphi : \mathbb{Z} \cap [1, r] \to [k]$ - произвольная раскраска из условия.

Положим $G = (V, E)$ - полный граф, вершинами которого являются все элементы множества $\mathbb{Z} \cap [1, r]$.

Положим $\psi : E \to [k]$ - раскраска рёбер графа G такая, что для любых $(a, b) \in E \hookrightarrow \psi((a, b)) := \varphi(|a - b|)$.

По определению $R(\underbrace{3, 3, ..., 3}_{\textrm{k}}) \exists v_1, v_2, v_3 \in V: \psi((v_1, v_2)) = \psi((v_2, v_3)) = \psi((v_3, v_1))$.

Без ограничения общности $v_1 > v_2 > v_3$.

Положим $x:= v_1 - v_2$.

Положим $y:= v_2 - v_3$.

Положим $z:= v_1 - v_3$.

$x \in \varphi^{-1}(\psi((v_1, v_2))$, $y \in \varphi^{-1}(\psi((v_2, v_3))$, $z \in \varphi^{-1}(\psi((v_1, v_3))$.

Так как $\psi((v_1, v_2)) = \psi((v_2, v_3)) = \psi((v_3, v_1))$

$x, y, z \in \varphi^{-1}(\psi((v_1, v_2))$.

Такие $x, y, z$ имеют одинаковый цвет при раскраске $\varphi$ и для них выполнено 
$x + y = z$.
\end{proof}

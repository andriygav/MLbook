\section{SVM-классификация}
\subsection{Постановка задачи}
\par Метод опорных векторов (Support Vector Machine, SVM) решает задачу бинарной классификации, где требуется найти оптимальную гиперплоскость, разделяющую два класса в пространстве признаков. Оптимальность понимается как максимизация ширины разделяющей полосы между классами.

\subsection{Математическая формализация}
\par Пусть дана обучающая выборка \( \{(x_i, y_i)\}_{i=1}^{\ell} \), где \( x_i \in \mathbb{R}^n \) - векторы признаков, \( y_i \in \{-1,+1\} \) - метки классов. Разделяющая гиперплоскость описывается уравнением:
\begin{equation*}
    \langle w,x \rangle - w_0 = 0,
\end{equation*}
где \( w \in \mathbb{R}^n \) - вектор весов, \( w_0 \in \mathbb{R} \) - порог.

\subsection{Условия разделимости}
\par Для корректной классификации должны выполняться условия:
\begin{equation*}
    \begin{cases}
        \langle w,x_i \rangle - w_0 \geq +1, & \text{если } y_i = +1 \\
        \langle w,x_i \rangle - w_0 \leq -1, & \text{если } y_i = -1
    \end{cases}
\end{equation*}
\par Эти условия можно объединить:
\begin{equation*}
    y_i(\langle w,x_i \rangle - w_0) \geq 1, \quad i = 1,\ldots,\ell
\end{equation*}

\subsection{Оптимизационная задача}
\par Ширина разделяющей полосы равна \( \frac{2}{\|w\|} \). Задача максимизации ширины эквивалентна задаче минимизации:
\begin{equation*}
    \frac{1}{2}\|w\|^2 \to \min_{w,w_0}
\end{equation*}
при ограничениях \( y_i(\langle w,x_i \rangle - w_0) \geq 1 \).

\subsection{Двойственная задача}
\par Применяя метод множителей Лагранжа, получаем двойственную задачу:
\begin{equation*}
    L(w,w_0,\lambda) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{\ell} \lambda_i(y_i(\langle w,x_i \rangle - w_0) - 1)
\end{equation*}
\par Условия оптимальности:
\begin{equation*}
    \begin{cases}
        w = \sum_{i=1}^{\ell} \lambda_i y_i x_i \\
        \sum_{i=1}^{\ell} \lambda_i y_i = 0
    \end{cases}
\end{equation*}

\subsection{Ядра}
\par Для нелинейной классификации используется переход в пространство признаков большей размерности через отображение \( \phi(x) \). Скалярное произведение заменяется на ядро:
\begin{equation*}
    K(x,z) = \langle \phi(x),\phi(z) \rangle
\end{equation*}
\par Популярные ядра:
\begin{itemize}
    \item Линейное: \( K(x,z) = \langle x,z \rangle \)
    \item Полиномиальное: \( K(x,z) = (\langle x,z \rangle + 1)^d \)
    \item RBF: \( K(x,z) = \exp(-\gamma\|x-z\|^2) \)
\end{itemize}

\subsection{Дискриминантная функция в ядровом пространстве}
\par После применения ядрового преобразования классификация новых точек осуществляется с помощью дискриминантной функции, которая принимает вид:
\begin{equation*}
    f(x) = \sum_{i=1}^{\ell} \lambda_i y_i K(x_i,x) - w_0
\end{equation*}
где:
\begin{itemize}
    \item \(x_i\) - опорные векторы из обучающей выборки
    \item \(\lambda_i\) - множители Лагранжа (двойственные переменные)
    \item \(y_i\) - метки классов опорных векторов
    \item \(K(x_i,x)\) - значение ядровой функции между опорным вектором и классифицируемой точкой
    \item \(w_0\) - порог, определяющий сдвиг разделяющей гиперплоскости
\end{itemize}

\par Важно отметить, что в этой формуле суммирование происходит только по опорным векторам, так как для остальных точек обучающей выборки \(\lambda_i = 0\). Это свойство обеспечивает эффективность вычислений при классификации новых точек.

\par Решающее правило для определения класса новой точки:
\begin{equation*}
    \text{class}(x) = \text{sign}(f(x)) = 
    \begin{cases}
        +1, & \text{если } f(x) > 0 \\
        -1, & \text{если } f(x) < 0
    \end{cases}
\end{equation*}

\subsection{Мягкие границы}
\par Для случая линейно неразделимой выборки вводятся переменные ослабления \( \xi_i \geq 0 \):
\begin{equation*}
    y_i(\langle w,x_i \rangle - w_0) \geq 1 - \xi_i
\end{equation*}
\par Целевая функция модифицируется:
\begin{equation*}
    \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{\ell} \xi_i \to \min_{w,w_0,\xi}
\end{equation*}
где \( C > 0 \) - параметр регуляризации.

\subsection{Задача 1}
\textbf{Условие:} 
\par Дан набор точек в двумерном пространстве с метками классов:
\begin{equation*}
    \begin{array}{ll}
        x_1 = (0,0), & y_1 = -1 \\
        x_2 = (2,0), & y_2 = +1 \\
        x_3 = (0,2), & y_3 = +1 \\
        x_4 = (2,2), & y_4 = +1
    \end{array}
\end{equation*}
\par В результате обучения SVM с линейным ядром получены следующие значения двойственных переменных:
\begin{equation*}
    \lambda_1 = 0.5, \quad \lambda_2 = 0, \quad \lambda_3 = 0.5, \quad \lambda_4 = 0
\end{equation*}
\par Найдите вектор весов \(w\) и определите, является ли точка \(x_{\text{new}} = (1,1)\) опорным вектором, если известно, что она лежит точно на разделяющей гиперплоскости.

\textbf{Решение:}
\par 1. Найдем вектор весов \(w\) через опорные векторы:
\begin{equation*}
    w = \sum_{i=1}^4 \lambda_i y_i x_i
\end{equation*}

\par 2. Подставляем известные значения:
\begin{align*}
    w &= 0.5 \cdot (-1) \cdot (0,0) + 0 \cdot (+1) \cdot (2,0) + \\
    &+ 0.5 \cdot (+1) \cdot (0,2) + 0 \cdot (+1) \cdot (2,2) \\
    &= (0,0) + (0,0) + (0,0.6) + (0,0) \\
    &= (0,1)
\end{align*}

\par 3. Для точек на разделяющей гиперплоскости выполняется:
\begin{equation*}
    \langle w,x \rangle - w_0 = 0
\end{equation*}

\par 4. Подставляя координаты \(x_{\text{new}} = (1,1)\):
\begin{equation*}
    \langle (0,1),(1,1) \rangle - w_0 = 0
\end{equation*}
\begin{equation*}
    w_0 = 1
\end{equation*}

\par 5. Чтобы точка была опорным вектором, она должна лежать на границе разделяющей полосы, то есть:
\begin{equation*}
    y_{\text{new}}(\langle w,x_{\text{new}} \rangle - w_0) = \pm 1
\end{equation*}

\par 6. Проверяем это условие:
\begin{equation*}
    \langle (0,1),(1,1) \rangle - 1 = 0
\end{equation*}
Получаем 0, что не равно $\pm1$.

\textbf{Ответ:} \(w = (0,1)\). Точка \(x_{\text{new}}\) не является опорным вектором.

\subsection{Задача 2}
\textbf{Условие:} 
\par Дано множество точек в двумерном пространстве:
\begin{equation*}
    \begin{array}{ll}
        x_1 = (1,1), & y_1 = +1 \\
        x_2 = (2,2), & y_2 = +1 \\
        x_3 = (0,0), & y_3 = -1 \\
        x_4 = (-1,1), & y_4 = -1
    \end{array}
\end{equation*}
\par После обучения SVM получена разделяющая гиперплоскость \(2/3 \cdot x_{i1} + 1/3 \cdot x_{i2} - 1 = 0\). Определите, какие из точек являются опорными векторами.

\textbf{Решение:}
\par Опорными векторами являются точки, лежащие на границе разделяющей полосы. Для их определения нужно:

\par 1. Запишем вектор весов из уравнения гиперплоскости:
\begin{equation*}
    w = (2/3,1/3) \quad w_0 = 1
\end{equation*}

\par 2. Вычислим отступ для каждой точки по формуле:
\begin{equation*}
    M(x) = y(\langle w,x \rangle - w_0) = y(2x_{i1} + x_{i2} - 3)
\end{equation*}

\par 3. Проверяем каждую точку:
\begin{align*}
    M(x_1) &= (+1)(2/3 \cdot 1 + 1/3 \cdot 1 - 1) = 0 \\
    M(x_2) &= (+1)(2/3 \cdot 2 + 1/3 \cdot 2 - 1) = 1 \\
    M(x_3) &= (-1)(2/3 \cdot 0 + 1/3 \cdot 0 - 1) = 1 \\
    M(x_4) &= (-1)(2/3 \cdot (-1) + 1/3 \cdot 1 - 1) = 4/3
\end{align*}

\par 4. Опорными векторами являются точки с отступом, равным единице, а также точки, лежащие на разделяющей гиперплоскости (отступ равен нулю).

\textbf{Ответ:} Точки \(x_2\) и \(x_3\) являются опорными векторами.

\subsection{Задача 3}
\textbf{Условие:} 
\par При обучении SVM с полиномиальным ядром второй степени \(K(x,z) = (\langle x,z \rangle + 1)^2\) получены следующие опорные векторы:
\begin{equation*}
    \begin{array}{ll}
        x_1 = (2,0), & \lambda_1 = 0.2, \quad y_1 = +1 \\
        x_2 = (0,1), & \lambda_2 = 0.3, \quad y_2 = -1 \\
        x_3 = (1,1), & \lambda_3 = 0.1, \quad y_3 = +1
    \end{array}
\end{equation*}
\par К какому классу будет отнесена точка \(x_{\text{new}} = (1,0)\), если \(w_0 = 0.5\)?

\textbf{Решение:}
\par Для определения класса точки нужно найти знак дискриминантной функции:
\begin{equation*}
    f(x) = \sum_{i=1}^3 \lambda_i y_i K(x_i,x) - w_0
\end{equation*}

\par Вычислим значения ядра для \(x_{\text{new}}\) и каждого опорного вектора:
\begin{align*}
    K(x_1,x_{\text{new}}) &= (2 \cdot 1 + 0 \cdot 0 + 1)^2 = 9 \\
    K(x_2,x_{\text{new}}) &= (0 \cdot 1 + 1 \cdot 0 + 1)^2 = 1 \\
    K(x_3,x_{\text{new}}) &= (1 \cdot 1 + 1 \cdot 0 + 1)^2 = 4
\end{align*}

\par Подставляем в дискриминантную функцию:
\begin{align*}
    f(x_{\text{new}}) &= 0.2 \cdot (+1) \cdot 9 + 0.3 \cdot (-1) \cdot 1 + 0.1 \cdot (+1) \cdot 4 - 0.5 \\
    &= 1.8 - 0.3 + 0.4 - 0.5 = 1.4
\end{align*}

\par Так как \(f(x_{\text{new}}) > 0\), точка относится к классу +1.

\textbf{Ответ:} Точка \(x_{\text{new}}\) будет отнесена к классу +1.

\section{SVM-регрессия}
\subsection{Постановка задачи}
\par В задаче регрессии требуется найти функцию \( f(x) = w^T \phi(x) + b \), которая аппроксимирует целевые значения \( y \) на основе входных данных \( x \), минимизируя ошибки предсказания.
\par В SVM-регрессии вводится допустимая область погрешностей — \(\epsilon\)-окрестность. Это означает, что отклонения \( |f(x_i) - y_i| \) в пределах \(\epsilon\) считаются несущественными, и модель игнорирует их. Цель — минимизировать сложность модели, связанную с \(\|w\|\), штрафуя при этом за отклонения, выходящие за пределы \(\epsilon\).

\subsection{Прямая задача}
\par\textbf{Функция потерь.}  
Для задачи SVM-регрессии используется \(\epsilon\)-чувствительная функция потерь:
\begin{equation*}
    L_\epsilon(f(x), y) = 
    \begin{cases} 
        0, & \text{если } |f(x) - y| \leq \epsilon, \\ 
        |f(x) - y| - \epsilon, & \text{иначе.}
    \end{cases}
\end{equation*}
\noindent\textbf{Прямая постановка задачи:}
\begin{equation*}
    \min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n L_\epsilon(f(x_i), y_i),
\end{equation*}
где:
\begin{itemize}
    \item \(\|w\|^2\) — регуляризационный член, минимизирующий сложность модели,
    \item \(C\) — коэффициент, регулирующий баланс между штрафами за ошибки и сложностью модели,
    \item \(L_\epsilon(f(x_i), y_i)\) — штраф за выход за пределы \(\epsilon\)-окрестности.
\end{itemize}

\subsection{Преобразование задачи}
\par Для учёта отклонений выше \(\epsilon\) вводятся штрафные переменные \(\xi_i\) и \(\xi_i^*\):  
\begin{itemize}
    \item \(\xi_i\) — превышение сверху (\(y_i > f(x_i) + \epsilon\)),
    \item \(\xi_i^*\) — превышение снизу (\(y_i < f(x_i) - \epsilon\)).
\end{itemize}
\par Задача минимизации принимает вид:
\begin{equation*}
    \min_{w, b, \xi, \xi^*} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*),
\end{equation*}
при ограничениях:
\begin{equation*}
\begin{aligned}
    y_i - (w^T \phi(x_i) + b) \leq \epsilon + \xi_i, \\
    (w^T \phi(x_i) + b) - y_i \leq \epsilon + \xi_i^*, \\
    \xi_i, \xi_i^* \geq 0.
\end{aligned}
\end{equation*}

\subsection{Метод Лагранжа}
\par Для решения задачи вводится лагранжиан, который включает:
\begin{itemize}
    \item Целевую функцию,
    \item Ограничения через множители Лагранжа (\(\alpha, \alpha^*, \eta, \eta^*\)).
\end{itemize}
\par Лагранжиан записывается как:
\begin{equation*}
\begin{aligned}
    L(w, b, \xi, \xi^*, \alpha, \alpha^*, \eta, \eta^*) &= \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*) - \\
    &\quad - \sum_{i=1}^n \alpha_i \big[ \epsilon + \xi_i - y_i + w^T \phi(x_i) + b \big] - \\
    &\quad - \sum_{i=1}^n \alpha_i^* \big[ \epsilon + \xi_i^* + y_i - w^T \phi(x_i) - b \big] - \\
    &\quad - \sum_{i=1}^n (\eta_i \xi_i + \eta_i^* \xi_i^*).
\end{aligned}
\end{equation*}
\par Для нахождения двойственной задачи необходимо минимизировать \(L\) по \(w\), \(b\), \(\xi\), \(\xi^*\) и максимизировать по множителям Лагранжа.

\subsection{Условия оптимальности}
\begin{enumerate}
    \item Производная по \(w\):
    \begin{equation*}
        \frac{\partial L}{\partial w} = w - \sum_{i=1}^n (\alpha_i - \alpha_i^*) \phi(x_i) = 0 \implies 
        w = \sum_{i=1}^n (\alpha_i - \alpha_i^*) \phi(x_i).
    \end{equation*}
    \item Производная по \(b\):
    \begin{equation*}
        \frac{\partial L}{\partial b} = \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0.
    \end{equation*}
    \item Производные по \(\xi_i\) и \(\xi_i^*\):
    \begin{equation*}
        \alpha_i + \eta_i = C, \quad \alpha_i^* + \eta_i^* = C, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
    \end{equation*}
\end{enumerate}

\subsection{Двойственная задача}
\par Подставляя условия оптимальности в лагранжиан, исключаем \(w\), \(b\), \(\xi_i\), \(\xi_i^*\). Получаем двойственную задачу:
\begin{equation*}
    \max_{\alpha, \alpha^*} -\frac{1}{2} \sum_{i,j=1}^n (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) K(x_i, x_j) 
    - \epsilon \sum_{i=1}^n (\alpha_i + \alpha_i^*) + \sum_{i=1}^n y_i (\alpha_i - \alpha_i^*),
\end{equation*}
где \(K(x_i, x_j) = \phi(x_i)^T \phi(x_j)\) — ядровая функция.
\par Ограничения:
\begin{equation*}
    \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
\end{equation*}
\par Для решения двойственной задачи используется метод квадратичного программирования.

\subsection{Построение финальной модели}
\par После решения двойственной задачи оптимальные \(\alpha_i\) и \(\alpha_i^*\) определяют параметры модели:
\begin{equation*}
    f(x) = \sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + b.
\end{equation*}
\par Смещение \(b\) вычисляется через опорные векторы — точки, где выполняется одно из условий:
\begin{equation*}
    y_i - (w^T \phi(x_i) + b) = \epsilon, \quad \text{или} \quad y_i - (w^T \phi(x_i) + b) = -\epsilon.
\end{equation*}
\par Опорные векторы (\(\alpha_i > 0\) или \(\alpha_i^* > 0\)) определяют форму модели.

\subsection{Выбор ядра}
\par Выбор ядра играет ключевую роль в качестве работы модели SVM-регрессии. Различные ядра по-разному преобразуют входные данные, что может существенно повлиять на точность предсказаний и обобщающую способность модели.
\par Выбор ядра зависит от особенностей данных, структуры зависимости и доступных вычислительных ресурсов. Экспериментальная проверка нескольких типов ядер и последующая оценка метрик качества модели — это ключевой этап в процессе выбора оптимального ядра.
\par На рисунке ниже представлена SVM-регрессия с тремя типами ядер: RBF, линейным и полиномиальным. 
\begin{figure}[ht!]
    \includegraphics[width = 0.8\textwidth]{chapters/svm/images/svm_regression_cmp_models.png}
    \centering
    \caption{Сравнение SVM-регрессия с разными типами ядер}
    \label{fig:kernel_comparison}
\end{figure}

\subsection{Задача 1}
\textbf{Условие}:
\par Рассмотрим следующий набор точек, лежащих на границе \(\epsilon\) - окрестности для SVM-регрессии с линейным ядром:
\begin{equation*}
    \{(1, 2), (2, 3), (3, 5), (4, 6)\}
\end{equation*}
\par Постройте регрессионную модель и найдите смещение \(b\), если \(w = 2\), \(\epsilon = 1\).
\par \noindent \textbf{Решение:}
\par Для нахождения смещения \(b\) необходимо использовать точки, которые лежат на границах \(\epsilon\)-окрестности. Мы знаем, что для таких точек выполняется равенство:
\begin{equation*}
    y_i - (w x_i + b) = \epsilon \quad \text{или} \quad y_i - (w x_i + b) = -\epsilon
\end{equation*}
\par Найдем для каждой точки \(b\), подставив их координаты в эти уравнения:
\begin{itemize}
\item Для точки \((1, 2)\):
\begin{equation*}
    2 - (2 \cdot 1 + b) = 1 \quad \Rightarrow \quad 2 - (2 + b) = 1 \quad \Rightarrow \quad -b = 1 \quad \Rightarrow \quad b = -1
\end{equation*}
\item Для точки \((2, 3)\):
\begin{equation*}
    3 - (2 \cdot 2 + b) = 1 \quad \Rightarrow \quad 3 - (4 + b) = 1 \quad \Rightarrow \quad -b = 2 \quad \Rightarrow \quad b = -2
\end{equation*}
\item Для точки \((3, 5)\):
\begin{equation*}
    5 - (2 \cdot 3 + b) = 1 \quad \Rightarrow \quad 5 - (6 + b) = 1 \quad \Rightarrow \quad -b = 2 \quad \Rightarrow \quad b = -2
\end{equation*}
\item Для точки \((4, 6)\):
\begin{equation*}
    6 - (2 \cdot 4 + b) = 1 \quad \Rightarrow \quad 6 - (8 + b) = 1 \quad \Rightarrow \quad -b = 3 \quad \Rightarrow \quad b = -3
\end{equation*}
\end{itemize}
\par Для вычисления окончательного значения смещения \(b\), усредняем найденные значения:
\begin{equation*}
    b_{\text{avg}} = \frac{-1 + (-2) + (-2) + (-3)}{4} = \frac{-8}{4} = -2
\end{equation*}
\par Таким образом, смещение \(b = -2\).
\par Регрессионная модель для SVM с линейным ядром имеет следующий вид:
\begin{equation*}
    f(x) = w x + b
\end{equation*}
\par Подставляем данное в условии значения \(w = 2\) и найденное значение \(b = -2\):
\begin{equation*}
    f(x) = 2x - 2
\end{equation*}
\par Это и есть наша линейная регрессионная модель.
\par \noindent \textbf{Ответ:} \(f(x) = 2x - 2\).

\subsection{Задача 2}
\textbf{Условие:}
\par У нас есть два набора данных для задачи регрессии:
\begin{itemize}
    \item Набор 1: \( \{(1, 2), (2, 3), (3, 4), (4, 5)\} \)    
    \item Набор 2: \( \{(1, 1), (2, 4), (3, 9), (4, 16)\} \)  
\end{itemize}
\par Предположим, что мы используем SVM-регрессию с различными типами ядер (линейное, полиномиальное, RBF). Определите, какое ядро будет оптимальным для каждого набора данных.
\par \noindent \textbf{Решение:}
\begin{itemize}
    \item Набор 1: данные имеют линейную зависимость, следовательно, линейное ядро будет лучшим выбором.

    \item Набор 2: данные имеют квадратичную зависимость, следовательно, оптимально будет использовать полиномиальное ядро второй степени.
\end{itemize}
\par \noindent \textbf{Ответ:} для первого набора данных оптимальным ядром будет линейное, для второго набора данных - полиномиальное второй степени.

\subsection{Задача 3}
\textbf{Условие:}
\par Дан набор данных для SVM-регрессии с линейным ядром: 
\begin{equation*}
    \{(1, 2), (2, 2.8), (3, 5.2), (4, 8)\}.
\end{equation*}
\par Параметры модели: \(w = 1.5\), \(b = 0.5\), \(\epsilon = 0.5\). 
\begin{enumerate}
    \item Определите, какие из точек набора данных находятся вне \(\epsilon\)-окрестности (требуют штрафных переменных \(\xi\) или \(\xi^*\)).
    \item Вычислите значения штрафных переменных для этих точек.
\end{enumerate}
\par \noindent \textbf{Решение:}
\begin{enumerate}
\item Определение границ \(\epsilon\)-окрестности:
   Уравнение модели SVM-регрессии с линейным ядром: 
   \begin{equation*}
    f(x) = wx + b.
   \end{equation*}
   Подсталяем данные в условии значения:
   \begin{equation*}
    f(x) = 1.5x + 0.5.
   \end{equation*}
   Границы \(\epsilon\)-окрестности:
   \begin{equation*}
    f(x) - \epsilon \leq y \leq f(x) + \epsilon.
   \end{equation*}
\item Проверка точек:
\begin{itemize}
    \item Для точки \((1, 2)\): 
    \begin{equation*}
        f(1) = 1.5 \cdot 1 + 0.5 = 2.0, \quad 2 - 0.5 \leq 2 \leq 2 + 0.5 \quad (\text{в окрестности}).
    \end{equation*}
    \item Для точки \((2, 2.8)\): 
    \begin{equation*}
        f(2) = 1.5 \cdot 2 + 0.5 = 3.5, \quad 3.5 - 0.5 \not\leq 2.8 \leq 3.5 + 0.5 \quad (\text{вне окрестности}).
    \end{equation*}
    \item Для точки \((3, 5.2)\): 
    \begin{equation*}
        f(3) = 1.5 \cdot 3 + 0.5 = 5.0, \quad 5.0 - 0.5 \leq 5.2 \leq 5.0 + 0.5 \quad (\text{в окрестности}).
    \end{equation*}
    \item Для точки \((4, 8)\): 
    \begin{equation*}
        f(4) = 1.5 \cdot 4 + 0.5 = 6.5, \quad 6.5 - 0.5 \leq 8.0 \not\leq 6.5 + 0.5 \quad (\text{вне окрестности}).
    \end{equation*}
\end{itemize}
\item Штрафные переменные:
\begin{itemize}
    \item Для точки \((2, 2.8)\):
    \begin{equation*}
        \xi_i = f(2) - y - \epsilon = 3.5 - 2.8 - 0.5 = 0.2.
    \end{equation*}
    \item Для точки \((4, 8)\):
    \begin{equation*}
        \xi_i^* = y - f(4) - \epsilon = 8 - 6.5 - 0.5 = 1.0.
    \end{equation*}
\end{itemize}
Таким образом, штрафные переменные:
\begin{equation*}
    \xi_2^* = 0.2, \quad \xi_4 = 1.0.
\end{equation*}
\end{enumerate}
\par \noindent \textbf{Ответ:} \(\xi_2^* = 0.2, \quad \xi_4 = 1.0.\)


\setcounter{secnumdepth}{0}

\section{1-norm SVM (LASSO SVM)}
\subsection*{Аппроксимация эмпирического риска с \(L_1\)-регуляризацией}
\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| & \rightarrow \min_{w, w_0}
\end{align*}

\subsection*{Плюс: отбор признаков с параметром селективности \(\mu\)}
\begin{itemize}
    \item чем больше \(\mu\), тем меньше признаков останется
\end{itemize}

\subsection*{Минус: слишком агрессивный отбор признаков}
\begin{itemize}
    \item по мере увеличения \(\mu\) признак может быть отброшен, хотя $y$ существенно зависит от него (даже когда ещё не все шумовые признаки отброшены)
\end{itemize}

\vspace{10pt}

\section{Сравнение L2 и L1 регуляризации}

Зависимость весов \(w_j\) от коэффициента \(\frac{1}{\mu}\):

\begin{itemize}
    \item \(L_1\) регуляризатор: \(\mu \sum_{j} |w_j|\)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6 \linewidth]{chapters/svm/images/L_1.png}
    \label{fig:image_1}    
\end{figure}

\begin{itemize}
    \item \(L_2\) регуляризатор: \(\mu \sum_{j} w_j^2\)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6 \linewidth]{chapters/svm/images/L_2.png}
    \label{fig:image_2}    
\end{figure}

\section{Doubly Regularized SVM (Elastic Net SVM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| + \frac{1}{2} \sum_{j=1}^{n} w_j^2 & \rightarrow \min_{w, w_0}
\end{align*}

\subsection*{Плюсы:}
\begin{itemize}
    \item Параметр селективности \(\mu\) управляет отбором признаков: чем больше \(\mu\), тем меньше признаков останется
    \item Есть эффект группировки (grouping effect): значимые зависимые признаки отбираются вместе
\end{itemize}

\subsection*{Минусы:}
\begin{itemize}
    \item Шумовые признаки также группируются и могут вместе оставаться в модели
    \item Приходится подбирать два параметра регуляризации \(\mu, \tau\) (есть специальные методы, например, regularization path)
\end{itemize}

\subsection{Elastic Net Analysis}

Elastic Net менее жёстко отбирает признаки.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{chapters/svm/images/Elastic_Net.png}
    \caption{Зависимости весов \(w_j\) от коэффициента \(\log \frac{1}{\mu}\)}
    \label{fig:elastic_net}
\end{figure}

\section{Support Features Machine (SFM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \sum_{j=1}^{n} R_{\mu}(w_{j}) & \rightarrow \min_{w, w_0}
\end{align*}

\begin{align*}
R_{\mu}(w_j)=
    \begin{cases}
        2\mu |w_j|, & |w_j| \leq \mu\\
        \mu^2 + w_j^2, & |w_j| \geq \mu \\
    \end{cases}
\end{align*}

\subsection*{Плюсы}
\begin{itemize}
    \item Только один параметр регуляризации \(\mu\)
    \item Отбор признаков с параметром селективности \(\mu\)
    \item Эффект группировки: значимые зависимые признаки ($|w_j|$ > \(\mu\)) входят в решение совместно (как в $Elastic$ $Net$)
    \item Шумовые признаки ($|w_j|$ < \(\mu\)) не группируются и подавляются независимо друг от друга (как в $LASSO$)
\end{itemize}

\section{Relevance Features Machine (RFM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \sum_{j=1}^{n} \ln(w_j^2 + \frac{1}{\mu}) & \rightarrow \min_{w, w_0}
\end{align*}

\begin{align*}
    R(w) = \ln(w^2 + \frac{1}{\mu}), \quad \mu = 0.1, 1, 100
\end{align*}

\subsection*{Плюсы}
\begin{itemize}
    \item Только один параметр регуляризации \(\mu\)
    \item Отбор признаков с параметром селективности \(\mu\)
    \item Есть эффект группировки
    \item Лучше отбирает набор значимых признаков, когда они только совместно обеспечивают хорошее решение
\end{itemize}

\section{Задачи}

\subsection{Задача 1}

Качественно объяснить, почему $L_1$-регуляризатор приводит к отбору признаков

\subsection{Ответ:}

Аппроксимация эмпирического риска с \(L_1\)-регуляризацией:
\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| & \rightarrow \min_{w, w_0}
\end{align*}

\textcolor{red}{Почему \(L_1\)-регуляризатор приводит к отбору признаков?}

Замена переменных: 
\[
u_j = \frac{1}{2} (|w_j| + w_j), \quad v_j = \frac{1}{2} (|w_j| - w_j).
\]
Тогда 
\[
w_j = u_j - v_j \quad |w_j| = u_j + v_j.
\]

\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(u - v, w_0)\right)_+ + \mu \sum_{j=1}^{n} (u_j + v_j) & \rightarrow \min_{u, v}, \\
    u_j \geq 0, \quad v_j \geq 0, \quad j = 1, \ldots, n.
\end{align*}

чем больше \(\mu\), тем больше индексов \(j\) таких, что \(u_j = v_j = 0\), но тогда \(w_j = 0\), значит, \textcolor{red}{признак не учитывается.}

\subsection{Задача 2}

Привести пример нежелательного эффекта в процессе обучения, с которым поможет справиться регуляризация 
 
\subsection{Ответ:}

Регуляризация помогает в случае линейной зависимости (мультиколлинеарности) признаков:\\
Пусть построен классификатор: $a(x, w) = sign\langle w, x \rangle$ \\
Мультиколлинеарность: $\exists$  $u \in \mathbb{R}^{n}$: $\forall x \in X$ $\langle u, x \rangle = 0$ \\
Неединственность решения и рост нормы вектора весов: $\forall \gamma \in \mathbb{R}$ $a(x, w) = sign\langle w, x \rangle = sign \langle w + \gamma u, x \rangle$ \\
\\
Проявления переобучения:
\begin{itemize}
    \item слишком большие веса $|w_j|$ разных знаков
    \item неустойчивость дискриминантной функции $\langle w, x \rangle$
    \item $Q(X^{\ell}) \ll Q(X^{k})$
\end{itemize}
Способ уменьшить переобучение:\\
регуляризация $||w|| \rightarrow min$ (сокращение весов, $weight$ $decay$)

\subsection{Задача 3}

Дана задача оптимизации:
\begin{align*}
    \frac{1}{2}(wx - b)^2 + \lambda|w| & \rightarrow \min_{w},
\end{align*}
где $x,$ $b$ $\in \mathbb{R};$ $\lambda \geq 0$\\
При каких $\lambda$ данная задачи имеет решение $w_0 \neq 0?$
 
\subsection{Ответ:}

Находим правую и левую односторонние производные в нуле и рассматриваем, когда они больше и меньше 0 соответственно:

\begin{align*}
    \begin{cases}
        -xb + \lambda > 0\\
        -xb - \lambda < 0\\
        \lambda \geq 0\\
    \end{cases} \Leftrightarrow \lambda > |xb|
\end{align*}

Это условие на $\lambda,$ при котором задача имеет решение $w_0 = 0,$ поэтому нам подходит $\lambda \in [0;$ $|xb|)$.


\section{Ядерные функции машины опорных векторов}
При наличии нелинейной связи между признаками и откликом качество линейных классификаторов, как показано на рисунке ниже,
часто может оказаться неудовлетворительным.
\begin{align*}
    \centering
    \includegraphics[width=0.6 \linewidth]{chapters/svm/images/svm_kernels.png}
    \label{fig:image}
\end{align*}
Для учета нелинейности обычно расширяют пространство переменных,
включая различные функциональные преобразования исходных предикторов (полиномы, экспоненты и проч.).
Машину опорных векторов SVM (Support Vector Machine) можно рассматривать как нелинейное обобщение линейного классификатора, полученного при
решении двойственной задачи (прямая задача является задачей бинарной классификации):
$$
\left\{\begin{array}{l}
w=\sum_{i=1}^{\ell} \lambda_i y_i x_i ; \\
w_0=\left\langle w, x_i\right\rangle-y_i, \quad \text { для любого } i: \lambda_i>0, M_i=1 .
\end{array}\right.
$$

Линейный классификатор с признаками $f_i(x)=\left\langle x, x_i\right\rangle$ :
$$
a(x)=\operatorname{sign}\left(\sum_{i=1}^{\ell} \lambda_i y_i\left\langle x, x_i\right\rangle-w_0\right) .
$$

Введя здесь новую нелинейную функцию $K\left(x, x^{\prime}\right)$, вместо $\left\langle x, x^{\prime}\right\rangle$,
можно строить модели с разделяющих поверхностями самой различной формы.

Дадим формальное определение:\newline
\textbf{Oпределение}:\newline
Функция $K: X \times X \rightarrow \mathbb{R}-$ ядро, если $K\left(x, x^{\prime}\right)=\left\langle\psi(x), \psi\left(x^{\prime}\right)\right\rangle$
при некотором $\psi: X \rightarrow H$, где $H-$ гильбертово пространство. \newline
В качестве таких функций чаще всего используют следующие:
\begin{itemize}
\item линейное ядро: $K(x, x^{\prime})=\langle x, x^{\prime} \rangle$, что соответствует классификатору на опорных векторах в исходном пространстве (см. рис. 6.3);
\item полиномиальное ядро со степенью $p: K(x, x^{\prime})=(1+\langle x, x^{\prime} \rangle)^p$;
\item гауссово ядро с радиальной базовой функцией (RBF): $K(x, x^{\prime})=\exp(\gamma\langle x - x^{\prime} \rangle^2)$;
\item сигмодиное ядро: $K(x, x^{\prime})=\tanh(\gamma\langle x, x^{\prime}\rangle+\beta_0)$.
\end{itemize}

Для того, чтобы определять является ли функция ядром, существует\newline
\textbf{Теорема Мерсера}:\newline
Функция двух переменных $K\left(x, x^{\prime}\right)$ является ядром тогда и только тогда, когда она
\begin{itemize}
\item симметрична, то есть $K\left(x, x^{\prime}\right)=K\left(x^{\prime}, x\right)$;
\item неотрицательно определена, то есть $\int_X \int_X K\left(x, x^{\prime}\right) g(x) g\left(x^{\prime}\right) d x d x^{\prime} \geq 0$ для любой функции $g: X \rightarrow \mathbb{R}$;
\end{itemize}

Нужно отметить, что на практике проверка неотрицательной определенности функции $K\left(x, x^{\prime}\right)$ часто является нелегкой задачей.
Вместо этого обычно используют какие-то конструктивные методы синтеза ядер, например достаточно очевидное утверждение: $K\left(x, x^{\prime}\right)=\alpha_1 K_1\left(x, x^{\prime}\right)+\alpha_2 K_2\left(x, x^{\prime}\right)$ при $\alpha_1, \alpha_2>0-$ ядро.

\subsection{Задача 1}
Найти пространство $H$ и преобразование $\psi: X \rightarrow H$, при которых
$K(x, x^{\prime}) = \langle \psi(x),\psi(x^{\prime}) \rangle $, где $X = \mathbb{R}^n,~
K(x, x^{\prime}) = \langle x, x^{\prime} \rangle^2,~ x = (x_1, x_2, \cdots x_n),~ x^{\prime} = (x_1^{\prime}, x_2^{\prime}, \cdots x_n^{\prime})$
\subsection{Решение}
\begin{align*}
K(x, x^{\prime}) = \langle x, x^{\prime} \rangle^2 = \langle (x_1, x_2, \cdots x_n), (x_1^{\prime}, x_2^{\prime}, \cdots x_n^{\prime}) \rangle^2 = (x_1x_1^{\prime} + \cdots + x_nx_n^{\prime})^2  \\
  = x_1^2x_1^{\prime}^2 + \cdots + \cdots x_n^2x_n^{\prime}^2 + 2x_1x_1^{\prime}x_2x_2^{\prime} + \cdots 2x_1x_1^{\prime}x_{n}x_n^{\prime} + \cdots + 2x_{n-1}x_{n-1}^{\prime}x_{n}x_{n}^{\prime}  = \\
  = \langle(x_1^2, x_2^2, \cdots x_n^2, \sqrt{2}x_1x_2, \cdots \sqrt{2}x_{n-1}x_n),(x_1^{\prime}^2, x_2^{\prime}^2, \cdots x_n^{\prime}^2, \sqrt{2}x_1^{\prime}x_2^{\prime},\cdots \sqrt{2}x_{n-1}^{\prime}x_{n}^{\prime})\rangle
\end{align*}
Подсчетом слагаемых можно убедиться, что $ dimH = \frac{1}{2}n(n-1) $, а $\psi:(x_1, x_2, \cdots x_n) \mapsto(x_1^2, x_2^2, \cdots x_n^2, \sqrt{2}x_1x_2, \cdots \sqrt{2}x_{n-1}x_n)$

\subsection{Задача 2}
Доказать, что $\forall \psi: X \rightarrow \mathbb{R}$ произведение $K\left(x, x^{\prime}\right)=\psi(x) \psi\left(x^{\prime}\right)-$ ядро.
\subsection{Решение}

Действительно, произведение $\psi(x) \psi(x^{\prime})$ есть скалярное произведение в одномерном пространстве $\mathbb{R}$, а в силу свойств скалярного произведения
(положительная определенность и симметричность) по теореме Мерсера получаем, что $K(x, x^{\prime})$ ядро.

\subsection{Задача 3}
Доказать, что $K(x, x^{\prime})=\exp(\langle x - x^{\prime} \rangle^2)$ ядро (гауссово ядро с радиальной базовой функцией).
\subsection{Решение}

Для доказательство нужно воспользоваться утверждением: композиция произвольного ядра $K_0$ и произвольной функции
$f: \mathbb{R} \rightarrow \mathbb{R}$, представимой в виде сходящегося степенного ряда с неотрицательными коэффициентами
$K\left(x, x^{\prime}\right)=f\left(K_0\left(x, x^{\prime}\right)\right)$, является ядром.
$K_0(x, x^{\prime}) = \langle x - x^{\prime} \rangle^2$ является ядром по свойствам скалярного произведения. Как известно,
экспонента представима в виде степенного ряда с положительными коэффициентами, поэтому выполнены все условия утверждения, что
доказывает, что $K(x, x^{\prime})$ ядро.


\section{Relevance Vector Machine(RVM)}

\subsection{Идея метода релевантных векторов}

Берем за основу структуру решения как в SVM:\\
\begin{align*}
   \sum_{i=1}^{\ell} \lambda_i y_i x_i
\end{align*}

(опорным векторам $x_i$ соответствуют $\lambda_i \neq 0$) \\
\textbf{Проблема:} Какие из коэффициентов $\lambda_i$ лучше обнулить? \\
Делаем так, что регуляризатор зависит не от w, а от $\lambda_i$. \\
\begin{align*}
       \rho (\lambda) = \frac{1}{(2\pi)^{l/2} \sqrt{\alpha_1...\alpha_l}} exp(-\sum_{i=1}^{\ell} \frac{\lambda_i^2}{2\alpha_i})
\end{align*}
- т.е. $\lambda_i$ - независимые, гауссовские и с дисперсиями $\alpha_i$ \\
Будем решать задачу оптимизации с регуляризатором (логарифмируем плотность - дисперсию не считаем константой): \\
\begin{align*}
       \frac{1}{2} \sum_{i=1}^{\ell} ln\alpha_i + \frac{\lambda_i^2}{\alpha_i}
\end{align*} 
Задача оптимизации:  \\
\begin{align*}
       \sum_{i=1}^{\ell} (1 - M_i(w(\lambda), w_0))_+ + \frac{1}{2} \sum_{i=1}^{\ell} ln\alpha_i + \frac{\lambda_i^2}{\alpha_i} \rightarrow \min_{\lambda, \alpha}
\end{align*}
Если одновременно оптимизируем и $\lambda$ и $\alpha$, то при уменьшении дисперсию, $\lambda$ будет маленькая - то есть "обнуляться" и соответствующие объекты не будут опорными. 

\subsection*{Плюсы}
\begin{itemize}
    \item Опорных векторов, как правило меньше (более "разреженное" решение)
    \item Шумовые вабросы уже не входят в число опорных
    \item Не надо искать параметр регуляризации (вместо этого $\alpha_i$ оптимизируется в процессе обучения)
    \item Аналогично SVM, можно использовать ядра
\end{itemize}

\subsection*{Минусы}
\begin{itemize}
    \item Не всегда есть преимущество по качеству классификации
\end{itemize}

\subsection{Задачи по RVM}
\textbf{Задача 1:}\\
Объяснить, как метод релевантных векторов (Relevance Vector Machine, RVM) достигает разреженности в параметрах модели.  \\
\textbf{Ответ:}\\
Метод релевантных векторов (RVM) достигает разреженности через байесовскую структуру, устанавливая независимые нормальные априорные распределения с нулевым средним для весов модели, каждый с собственной гиперпараметром точности (обратная дисперсия). Конкретно, для каждого веса $w_i$ существует связанный гиперпараметр точности $\alpha_i$. 
Априорное распределение весов задается как:\\
Априорное распределение весов задается как:
\begin{align*}
       p(\mathbf{w} | \boldsymbol{\alpha}) = \prod_{i=1}^N \mathcal{N}(w_i | 0, \alpha_i^{-1})
\end{align*}

В процессе обучения RVM определяет гиперпараметры  $\boldsymbol{\alpha} = [\alpha_1, \alpha_2, \dots, \alpha_N]$, максимизируя маржинальное правдоподобие данных. Многие из этих гиперпараметров стремятся к бесконечности, что приводит к обнулению соответствующих весов $ w_i$. 

\textbf{Задача 2:}\\
Объяснить, в чем заключается основное отличие метода релевантных векторов (RVM) от метода опорных векторов (SVM), если рассматривать их с точки зрения подхода к регуляризации и использования априорных предположений. \\
\textbf{Ответ:}
\begin{itemize}
    \item Основное отличие заключается в том, что RVM использует байесовский подход, тогда как SVM — детерминированный метод. 
    \item В RVM вводятся априорные распределения на веса модели, что позволяет автоматически выполнять регуляризацию, оставляя наиболее значимые параметры.
    \item В результате RVM достигает разреженности (аналогично SVM) без использования параметра регуляризации, а лишь за счет максимизации апостериорной вероятности.

\end{itemize}


\textbf{Задача 3:}\\
Объяснить, в чем метод релевантных векторов может быть быстрее и медленнее метода опорных векторов \\
\textbf{Ответ:} \\
\textbf{Быстрее, потому что:}
 \begin{itemize}
    \item В RVM гораздо быстрее применение модели к новым точкам, потому что опорных векторов гораздо меньше 
    \item В SVM необходимо оптимизировать параметр регуляризации C (или аналогичный параметр для других ядер), что требует дополнительных затрат времени. В RVM регуляризация происходит автоматически через байесовский подход (гиперпараметры $\alpha_i$), устраняя необходимость выбора таких параметров вручную.

\end{itemize}
\textbf{Медленнее, потому что:}
\begin{itemize}
    \item Байесовский подход в RVM делает процесс обучения более ресурсоемким, чем у SVM. Это связано с итеративными расчетами для апостериорных вероятностей и гиперпараметров - обучение происходит дольше
\end{itemize}

\section{Резюме по линейным классификаторам}
\begin{itemize}
    \item SVM - лучший метод линейной классификации
    \item SVM изящно обобщается для нелинейной классификации, для линейной и нелинейной регрессии
    \item Аппрксимация пороговой функции потерь $\L (M)$ увеличивает зазор и повышает качество классификации
    \item Регуляризация устраняет мультиколлинеарность и уменьшает переобучение
    \item Негладкость функции потерь приводит к отбору объектов
    \item Негладкость регуляризатора приводит к отбору признаков
\end{itemize}
\textbf{Комментарий по последним двум пунктам:} \\
Отбор опорных объектов возник из-за того, что возникает негладкая функция потерь, аналогично (но смотрим с другой стороны)  отбор признаков так же связан с негладкостью регуляризатора. \\
Что мы можем отбирать в задаче классификации? \\
У нас есть матрица объекты - признаки, то есть либо строки, либо столбцы являются лишними.\\ Cоответственно и возникает подход: решаем задачу классификации, отфильтровав матрицу по строкам и по столбцам. \\
Фильтрация может быть реализована:
\begin{itemize}
    \item путем выбора негладких функций потерь, которые приводят к фильтрации строк (набора объектов)
    \item путем выбора негладких регуляризаторов, которые приводят к фильтрации столбцов (набора признаков)
\end{itemize}
Роль регуляризации возникает благодаря выписыванию оптимизационной задачи с ограничениями-неравенствами $\rightarrow$ требуется применять условие Каруша-Куна-Таккера (применимы только к гладким функциям) \\ 
Если функция негладкая $\rightarrow$ вводим дополнительные переменные, неотрицательные и могут обращаться в ноль $\rightarrow$ происходит отбор \\


\section{Обобщения линейного SVM. Ядра и спрямляющие пространства. SVM как двухслойная нейронная сеть.}
\subsection*{Нелинейное обобщение SVM}
\textbf{Определение}. Функция $K: X \rightarrow X$ - ядро, если $K(x, \tilde{x}) = \langle \psi, \tilde{\psi} \rangle $ при некотором $\psi: X \rightarrow H$, где $H$ - гильбертово пространство

\noindent\textbf{Теорема}. Функция $K(x, \tilde{x})$ является ядром тогда и только тогда, когда
она симметрична: $K(x, \tilde{x}) = K(\tilde{x}, x)$ и неотрицательно определена:
$ \iint\limits_{XX} K(x, \tilde{x})g(x)g(\tilde{x}) dxdy \ge 0$ для любой $g: X \rightarrow \mathbb{R}$

\noindent\textbf{Конструктивные методы синтеза ядер:}
\begin{itemize}
  \item $K(x, \tilde{x}) = \text{\textlangle} x, \tilde{x} \text{\textrangle} ~$ - ядро
  \item $K(x, \tilde{x}) = 1$ - ядро
  \item $K(x, \tilde{x}) = K_1(x, \tilde{x}) \times K_2(x, \tilde{x})$ - ядро, если $K_1, K_2$ - ядра
  \item $K(x, \tilde{x}) = \psi(x)\psi(\tilde{x})$ - ядро при $\psi: X \rightarrow \mathbb{R}$
  \item $K(x, \tilde{x}) = \alpha_1 K_1(x, \tilde{x}) + \alpha_2 K_2(x, \tilde{x}), ~ \alpha_1 > 0, \alpha_2 > 0$
  \item $K(x ,\tilde{x}) = \iint\limits_{X} s(x, z) s(\tilde{x}, z) dz$ - ядро, если $s: X \times X \rightarrow \mathbb{R}$ - симметричная интегрируемая функция
  \item $K(x, \tilde{x}) = f(K_0(x, \tilde{x}))$ - ядро, если $K_0$ - ядро и $f: \mathbb{R} \rightarrow \mathbb{R}$ представима в виде сходящегося степенного ряда с неотрицательными коэффицентами 
\end{itemize}

\subsection{Задача 1}
Найти пространство $H$ и преобразование $\psi: X \rightarrow H$, при которых 
$K(x, \tilde{x}) = \langle \psi(x),\psi(\tilde{x}) \rangle $, где $X = \mathbb{R}^2,~
K(u, v) = \langle u, v \rangle^2,~ u = (u_1, u_2),~ v = (v_1, v_2)$
\subsection{Решение}
\begin{align*}
  K(u, v) = \langle u, v \rangle^2 = \langle (u_1, u_2), (v_1, v_2) \rangle^2 = (u_1v_1 + u_2v_2)^2 = u_1^2 u_2^2 + 2u_1v_1u_2v_2 = \\
  = \langle(u_1^2, u_2^2, \sqrt{2}u_1u_2),(v_1^2, v_2^2, \sqrt{2}v_1v_2)\rangle 
\end{align*}
То есть $ H = \mathbb{R}^3, ~ \psi: (u_1, u_2) \rightarrow (u_1^2, u_2^2, \sqrt{2}u_1u_2)$

\subsection{Задача 2}
Решите предыдущую задачу при условии, что $X = \mathbb{R}^n, ~ K(u, v) = \langle u, v \rangle^{d}$
\subsection{Решение}
Заметим, что в таком случае компонентами вектора $\psi(u)$ будут различные произведения $(u_1)^{d_1}, (u_2)^{d_2}, ..., (u_n)^{d_n}~$ при
$d_1, d_2, ..., d_n: d_1 \ge 0, d_2 \ge 0, ..., d_n \ge 0$ и $ d_1 + d_2 + ... + d_n = d$. Число мономов и есть размерность пространства $H$: $dim H = C_{n + d - 1} ^ d$ - число сочетаний с повторением.

\subsection{Задача 3}
Представьте нелинейное обобщение SVM для классификатора в виде двухслойной нейронной сети. Считать,
что опорные объекты из множества $ X = \mathbb{R}^n~$
\subsection{Решение}
Обозначим $x_1, x_2, ..., x_h$ как опорные объекты. Тогда
\begin{align}
  a(x) = sign(\sum_{i = 1}^{h} \lambda_i y_i K(x, \tilde{x}) - w_0)
\end{align}

\noindent Двухслойная нейронная сеть представлена на рисунке ниже.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{chapters/svm/images/SVM_neuron.png}
  \caption{SVM в виде двухслойной нейросети}
  \label{fig:mpr}
\end{figure}

\section{Одноклассовый SVM}

\subsection{Формулировка задачи}
Пусть задан набор данных $\{x_i\}_{i=1}^n$, $x_i \in \mathbb{R}^d$. Одноклассовый SVM определяется решением следующей оптимизационной задачи:
\[
\min_{\mathbf{w},\rho,\{\xi_i\}} \frac{1}{2}\|\mathbf{w}\|^2 - \rho + \frac{1}{\nu n}\sum_{i=1}^n \xi_i
\]
при ограничениях
\[
(\mathbf{w}\cdot \phi(x_i)) \ge \rho - \xi_i, \quad \xi_i \ge 0, \quad i=1,\dots,n,
\]
где $\phi(\cdot)$ — отображение в спрямляющее пространство, $\nu \in (0,1]$ — заданный параметр.

\subsection{Двойственная форма}
Двойственная задача может быть записана следующим образом:
\[
\max_{\{\alpha_i\}} \left( \rho = \min_i \left( \mathbf{w} \cdot \phi(x_i) \right) \right), 
\]
где
\[
\mathbf{w} = \sum_{i=1}^n \alpha_i \phi(x_i), \quad \sum_{i=1}^n \alpha_i = 1, \quad 0 \le \alpha_i \le \frac{1}{\nu n}.
\]

Подстановкой ядровой функции $K(x_i,x_j) = \phi(x_i) \cdot \phi(x_j)$ получаем зависимость решения от скалярных произведений в спрямляющем пространстве:
\[
\mathbf{w} \cdot \phi(x) = \sum_{i=1}^n \alpha_i K(x_i,x).
\]

\subsection{Принятие решения}
Для новой точки $x$ проверяется знак:
\[
f(x) = \mathbf{w}\cdot\phi(x) - \rho.
\]
Если $f(x)<0$, точка считается не принадлежащей множеству основных данных.

\subsection{Задача 1}
Дан набор данных $\{x_i\}_{i=1}^n$, $x_i \in \mathbb{R}^d$. Одноклассовый SVM решает задачу:
\[
\min_{\mathbf{w},\rho,\{\xi_i\}} \frac{1}{2}\|\mathbf{w}\|^2 - \rho + \frac{1}{\nu n}\sum_{i=1}^n \xi_i
\]
при ограничениях
\[
(\mathbf{w}\cdot\phi(x_i))\ge\rho-\xi_i, \quad \xi_i\ge0.
\]

\textbf{Требуется:} Найти двойственную постановку задачи.

\subsection{Решение}
Двойственная задача:
\[
\max_{\{\alpha_i\}} \rho = \min_i \sum_{j=1}^n \alpha_j K(x_j,x_i)
\]
при условиях
\[
\sum_{i=1}^n \alpha_i = 1,\quad 0 \le \alpha_i \le \frac{1}{\nu n}.
\]
Здесь $\mathbf{w} = \sum_{i=1}^n \alpha_i \phi(x_i)$ и $K(x_i,x_j)=\phi(x_i)\cdot \phi(x_j)$.

\hrulefill

\subsection{Задача 2}
При заданном решении одноклассового SVM для новой точки $x$ вычисляется:
\[
f(x)=\mathbf{w}\cdot\phi(x)-\rho.
\]
\textbf{Требуется:} Выразить $f(x)$ через $\{\alpha_i\}$ и $K(x_i,x)$.

\subsection{Решение}
\[
f(x) = \left(\sum_{i=1}^n \alpha_i K(x_i,x)\right) - \rho.
\]

\hrulefill

\subsection{Задача 3}
Докажите, что для оптимального решения одноклассового SVM количество ненулевых коэффициентов $\alpha_i$ не меньше $\lceil\nu n\rceil$.

\subsection{Решение}
Из условий задачи:
\[
\sum_{i=1}^n \alpha_i = 1,\quad 0 \le \alpha_i \le \frac{1}{\nu n}.
\]
Предположим, что ненулевых $\alpha_i$ меньше, чем $\lceil\nu n\rceil$. Тогда даже при максимальном значении каждого ненулевого $\alpha_i = \frac{1}{\nu n}$ сумма $\sum_{i=1}^n \alpha_i$ не могла бы достичь 1 (поскольку $\lceil \nu n \rceil \cdot \frac{1}{\nu n}\ge1$). Следовательно, для выполнения условия суммы равной единице необходимо не менее $\lceil\nu n\rceil$ ненулевых коэффициентов.


\section{Теорема Мерсера}

Как уже было сказано ранее, для применения SVM к выборке, не являющейся линейно разделимой, можно перейти в гильбертово пространство $\mathcal{H}$ большей размерности, то есть признаковое описание $x \in \mathcal{X}$ каждого объекта можно заменить вектором $\varphi(x) \in \mathcal{H}$. В результате выборка, которую нельзя линейно разделить в $\mathcal{X}$, может стать линейно разделимой в $\mathcal{H}$. Поскольку для метода опорных векторов важно лишь скалярное произведение объектов, можно явно не находить вложение $\varphi \colon \mathcal{X} \to \mathcal{H}$, а вместо скалярного произведения $\langle x, x' \rangle_\mathcal{X}$ использовать ядро $K(x, x') = \langle \varphi(x), \varphi(x') \rangle_\mathcal{H}$. Естественным образом возникает следующий вопрос: каким требованиям должна удовлетворять функция $K(x, x')$, чтобы она являлась ядром? Необходимые и достаточные условия определяет следующая теорема.

\begin{theorem}[Мерсер]
Пусть $\mathcal{X}$ --- компактное хаусдорфово пространство с борелевской мерой $\mu$ (например, компактное подмножество $\mathbb{R}^n$ с мерой Лебега). Непрерывная функция $K \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ является ядром тогда и только тогда, когда она удовлетворяет следующим двум условиям:
\begin{enumerate}
    \item симметричность: \[ \forall x, x' \in \mathcal{X}: \ K(x, x') = K(x', x), \]
    \item неотрицательная определённость: \[ \forall g \in L^2(\mathcal{X}): \ \int_\mathcal{X} \int_\mathcal{X} K(x, x') g(x) g(x') d\mu(x) d\mu(x') \ge 0. \]
\end{enumerate}
\end{theorem}
\begin{proof}
($\Rightarrow$) Пусть $K(x, x') = \langle \varphi(x), \varphi(x') \rangle_\mathcal{H}$ для некоторой функции $\varphi \colon \mathcal{X} \to \mathcal{H}$. Симметричность следует из симметричности скалярного произведения. Покажем неотрицательную определённость:
\begin{align*}
    \int_\mathcal{X} \int_\mathcal{X} K(x, x') g(x) g(x') d\mu(x) d\mu(x') &= \int_\mathcal{X} \int_\mathcal{X} \langle \varphi(x), \varphi(x') \rangle_\mathcal{H} g(x) g(x') d\mu(x) d\mu(x') \\
    &= \int_\mathcal{X} \int_\mathcal{X} \langle \varphi(x) g(x), \varphi(x') g(x') \rangle_\mathcal{H} d\mu(x) d\mu(x') \\
    &= \left\langle \int_\mathcal{X} \varphi(x) g(x) d\mu(x), \int_\mathcal{X} \varphi(x') g(x') d\mu(x') \right\rangle \ge 0.
\end{align*}

($\Leftarrow$) Рассмотрим интегральный оператор Гильберта-Шмидта $T_K \colon L^2(\mathcal{X}) \to L^2(\mathcal{X})$, соответствующий функции $K(x, x')$:
\[ [T_K(f)](x) = \int_\mathcal{X} f(x') K(x, x') d\mu(x'). \]
Из симметричности функции $K(x, x')$ и теоремы Фубини следует, что оператор $T_K$ является самосопряжённым. Пользуясь теоремой Арцела-Асколи можно показать, что образ единичного шара в $L^2(\mathcal{X})$ под действием $T_K$ является предкомпактом, откуда следует, что $T_K$ --- компактный оператор. Значит, к $T_K$ применима спектральная теорема, то есть существует ортонормированная система векторов $(\psi_i)_{i=1} ^\infty$ в $L^2(\mathcal{X})$ и последовательность вещественных чисел $(\lambda_i)_{i=1}^\infty$ такие, что оператор $T_K$ представим в виде
\[ T_K(f) = \sum_{i=1}^\infty \lambda_i \langle f, \psi_i \rangle_{L^2(\mathcal{X})} \psi_i. \]
Из неотрицательной определённости функции $K(x, x')$ получаем неотрицательность чисел $\lambda_i$:
\begin{align*}
    \lambda_i \langle \psi_i, \psi_i \rangle_{L^2(\mathcal{X})} &= \langle \psi_i, T_K \psi_i \rangle_{L^2(\mathcal{X})} \\
    &= \int_\mathcal{X} \psi(x) \left( \int_\mathcal{X} \psi(x') K(x, x') d\mu(x') \right) d\mu(x) \ge 0.
\end{align*}
Определим функцию $\varphi \colon \mathcal{X} \to \ell^2(\mathbb{N})$ следующим образом: $\varphi(x) = (\sqrt{\lambda_i} \psi_i(x))_{i=1}^\infty$. Для любой функции $f \in L^2(\mathcal{X})$ и любого $x \in \mathcal{X}$ имеем
\begin{align*}
    [T_K(f)](x) &= \sum_{i=1}^\infty \lambda_i \langle f, \psi_i \rangle_{L^2(\mathcal{X})} \psi_i(x) \\
    &= \sum_{i=1}^\infty \lambda_i \psi_i(x) \int_\mathcal{X} f(x') \psi(x') d\mu(x') \\
    &= \int_\mathcal{X} \sum_{i=1}^\infty \lambda_i \psi_i(x) f(x') \psi(x') d\mu(x') \\
    &= \int_\mathcal{X} \langle \varphi(x), \varphi(x') \rangle_{\ell^2(\mathbb{N})} f(x') d\mu(x').
\end{align*}
Таким образом, $K(x, x') = \langle \varphi(x), \varphi(x') \rangle_{\ell^2(\mathbb{N})}$.
\end{proof}

В качестве примера применения теоремы Мерсера покажем, каким образом можно получать новые ядра из уже известных.

\begin{lemma}
\label{lemma:new_kernels_from_old}
Следующие функции являются ядрами:
\begin{enumerate}
    \item $K(x, x') = \operatorname{const}$;
    \item $K(x, x') = c_1 K_1(x, x') + c_2 K_2(x, x')$, где $K_1, K_2 \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ --- ядра, $c_1, c_2 \ge 0$ --- константы;
    \item $K(x, x') = K_1(x, x') K_2(x, x')$, где $K_1, K_2 \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ --- ядра;
    \item $K(x, x') = p(K'(x, x'))$, где $K' \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ --- ядро, $p$ --- многочлен с неотрицательными коэффициентами;
    \item $K(x, x') = \exp(K'(x, x'))$, где $K' \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ --- ядро.
\end{enumerate}
\end{lemma}
\begin{proof}
Докажем первый пункт. Симметричность очевидна. Неотрицательная определённость следует из теоремы Фубини:
\[ \int_\mathcal{X} \int_\mathcal{X} c g(x) g(x') d\mu(x) d\mu(x') = c \left( \int_\mathcal{X} g(x) d\mu(x) \right)^2 \ge 0. \]
Доказательство остальных пунктов оставлено в качестве упражнения.
\end{proof}

\subsection{Задачи}

\begin{problem}
Являются ли следующие функции ядрами на множестве $[0,1] \times [0,1]$?
\begin{enumerate}
    \item $K_1(x, x') = x^4 + (x')^2$;
    \item $K_2(x, x') = |x - x'|$;
    \item $K_3(x, x') = x x'$.
\end{enumerate}
\end{problem}
\begin{solution}
\begin{enumerate}
    \item Нет, поскольку $K_1$ не симметрична: $K_1\left(1, \frac{1}{2}\right) \ne K_1\left(\frac{1}{2}, 1\right)$.
    \item Нет, поскольку $K_2$ не является неотрицательно определённой:
    \[ \int_0^1 \int_0^1 |x - x'| \cdot (-x) d\mu(x) d\mu(x') = -\frac{1}{6} < 0. \]
    \item Да, это следует из леммы \ref{lemma:new_kernels_from_old}.
\end{enumerate}
\end{solution}

\begin{problem}
Пусть $K \colon \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ --- ядро. Выразите $\int_\mathcal{X} K(x, x) d\mu(x)$ через собственные значения соответствующего интегрального оператора Гильберта-Шмидта $T_K(f) = \int_\mathcal{X} f(x') K(x, x') d\mu(x')$.
\end{problem}
\begin{solution}
\begin{align*}
    \int_\mathcal{X} K(x, x) d\mu(x) &= \int_\mathcal{X} \langle \varphi(x), \varphi(x) \rangle_{\ell^2(\mathbb{N})} d\mu(x) \\
    &= \int_\mathcal{X} \sum_{i=1}^\infty \lambda_i \psi_i^2(x) d\mu(x) \\
    &= \sum_{i=1}^\infty \lambda_i \langle \psi_i, \psi_i \rangle \\
    &= \sum_{i=1}^\infty \lambda_i.
\end{align*}
Таким образом, интеграл $\int_\mathcal{X} K(x, x) d\mu(x)$ равен следу оператора $T_K$.
\end{solution}

\begin{problem}
Докажите пункты 2-5 в лемме \ref{lemma:new_kernels_from_old}.
\end{problem}
\begin{solution}
Симметричность во всех случаях очевидна. По теореме Мерсера достаточно доказать неотрицательную определённость.
\begin{enumerate}
    \setcounter{enumi}{1}
    \item Коническая комбинация неотрицательно определённых функций также неотрицательно определена.
    \item Имеем
    \begin{align*}
        K_1(x, x') K_2(x, x') &= \langle \varphi_1(x), \varphi_1(x') \rangle \langle \varphi_2(x), \varphi_2(x') \rangle \\
        &= \left( \sum_{i=1}^\infty \lambda_i^{(1)} \psi_i^{(1)}(x)\psi_i^{(1)}(x') \right) \left( \sum_{j=1}^\infty \lambda_j^{(2)} \psi_j^{(2)}(x)\psi_j^{(2)}(x') \right) \\
        &= \sum_{i=1}^\infty \sum_{j=1}^\infty \lambda_i^{(1)} \lambda_j^{(2)} \psi_i^{(1)}(x) \psi_j^{(2)}(x) \psi_i^{(1)}(x') \psi_j^{(2)}(x').
    \end{align*}
    Возьмём произвольную биекцию $\tau \colon \mathbb{N} \to \mathbb{N} \times \mathbb{N}$ и определим следующее отображение:
    \[ \varPhi(x) = \left(\sqrt{\lambda^{(1)}_{\tau(k)_1} \lambda^{(2)}_{\tau(k)_2}} \cdot \psi_{\tau(k)_1}^{(1)}(x) \cdot \psi_{\tau(k)_2}^{(2)}(x)\right)_{k=1}^\infty. \]
    Тогда $K_1(x, x') K_2(x, x') = \langle \varPhi(x), \varPhi(x') \rangle$.
    \item Следует из пунктов 1, 2 и 3.
    \item Из пункта 4 следует, что $K_n(x, x') = \sum_{i=0}^n \frac{[K'(x, x')]^n}{n!}$ является ядром для любого $n \in \mathbb{N}$. Последовательность функций $K_n$ равномерно сходится к $K = \exp \circ K'$ на компактном пространстве $\mathcal{X}$, поэтому $K$ неотрицательно определена.
\end{enumerate}
\end{solution}

\section{Задача квадратичного программирования и двойственная задача. Понятие опорных векторов.}

Квадратичное программирование (QP) — это важная область оптимизации, которая находит применение в различных областях, включая машинное обучение, экономику и инженерные задачи. В этой главе мы рассмотрим основные понятия, связанные с квадратичным программированием, включая его определение, формулировку двойственной задачи и понятие опорных векторов.

\section{Квадратичное программирование}

\subsection{Определение}

Квадратичное программирование — это задача оптимизации, в которой целевая функция является квадратичной, а ограничения линейными. Формально задача может быть записана следующим образом:

[
\text{minimize} quad \frac{1}{2} x^T Q x + c^T x
]

при условиях:

[
Ax leq b
]

где:
\begin{itemize}
    item ( x ) — вектор переменных,
    item ( Q ) — симметричная положительно определенная матрица (матрица коэффициентов квадратичного члена),
    item ( c ) — вектор коэффициентов линейного члена,
    item ( A ) — матрица коэффициентов ограничений,
    item ( b ) — вектор ограничений.
\end{itemize}

\subsection{Применения}

Квадратичное программирование используется в различных задачах, таких как:
\begin{itemize}
    item Оптимизация портфелей в финансовом анализе.
    item Обучение моделей машинного обучения, например, в методах опорных векторов (SVM).
    item Ресурсное распределение и планирование.
\end{itemize}

\section{Двойственная задача}

\subsection{Определение двойственной задачи}

Каждая задача оптимизации имеет связанную с ней двойственную задачу. Двойственная задача для квадратичного программирования формулируется на основе лагранжевой функции. Лагранжева функция для нашей задачи выглядит следующим образом:

[
L(x, lambda) = \frac{1}{2} x^T Q x + c^T x + lambda^T (b - Ax)
]

где ( lambda ) — вектор множителей Лагранжа.

subsection{Формулировка двойственной задачи}

Двойственная задача может быть сформулирована как:

[
\text{maximize} quad g(lambda) = b^T lambda - \frac{1}{2} lambda^T A Q^{-1} A^T lambda
]

при условиях:

[
lambda geq 0
]

где ( g(lambda) ) — двойственная функция.

subsection{Связь между прямой и двойственной задачами}

Решение прямой задачи и двойственной задачи связано через теорему о двойственности, которая утверждает, что при выполнении условий сильной двойственности значение целевой функции прямой задачи равно значению целевой функции двойственной задачи.

\section{Понятие опорных векторов}

\subsection{Определение опорных векторов}

Опорные векторы — это данные, которые лежат на границе между классами в задачах классификации, особенно в контексте метода опорных векторов (SVM). Эти точки являются критическими для определения разделяющей гиперплоскости.

\subsection{Роль опорных векторов в SVM}

В методе опорных векторов задача заключается в нахождении гиперплоскости, которая максимизирует зазор (margin) между классами. Опорные векторы — это те точки, которые находятся на границах этого зазора. Они играют ключевую роль в обучении модели, так как именно они влияют на положение гиперплоскости.

\subsection{Связь с квадратичным программированием}

Задача SVM может быть сформулирована как задача квадратичного программирования, где цель состоит в минимизации квадратичной функции, связанной с расстоянием до опорных векторов и ошибками классификации.

\section{Задача 1: Квадратичное программирование}

\textbf{Формулировка задачи:} Рассмотрим следующую задачу квадратичного программирования:

Максимизировать:
[
f(x) = -x_1^2 - x_2^2
]

при условиях:
[
x_1 + x_2 leq 1
]
[
x_1 geq 0, quad x_2 geq 0
]

\textbf{Решение:} Для решения этой задачи мы можем использовать метод множителей Лагранжа. Сначала формируем функцию Лагранжа:

[
L(x_1, x_2, lambda) = -x_1^2 - x_2^2 + lambda(1 - x_1 - x_2)
]

Теперь найдем частные производные и приравняем их к нулю:

1. (\frac{partial L}{partial x_1} = -2x_1 - lambda = 0)
2. (\frac{partial L}{partial x_2} = -2x_2 - lambda = 0)
3. (\frac{partial L}{partial lambda} = 1 - x_1 - x_2 = 0)

Из первых двух уравнений получаем:
[
lambda = -2x_1, quad lambda = -2x_2 implies x_1 = x_2
]

Подставляя это в третье уравнение:
[
1 - 2x_1 = 0 implies x_1 = \frac{1}{2}, quad x_2 = \frac{1}{2}
]

Таким образом, оптимальное решение:
[
(x_1, x_2) = left(\frac{1}{2}, \frac{1}{2}\right)
]

\section{Задача 2: Двойственная задача}

\textbf{Формулировка задачи:} Для задачи квадратичного программирования из предыдущего примера сформулируем двойственную задачу. Двойственная задача имеет вид:

Минимизировать:
[
g(lambda) = \frac{1}{4} + lambda
]

при условиях:
[
lambda geq 0
]

\textbf{Решение:} Для минимизации функции (g(lambda)) необходимо найти значение (lambda), которое минимизирует эту функцию. Поскольку (g(lambda)) является линейной функцией, её минимум достигается при границе допустимого множества.

Минимум функции (g(lambda)) будет достигаться при (lambda = 0):

Таким образом, оптимальное значение:
[
g(0) = \frac{1}{4}
]

section*{Задача 3: Понятие опорных векторов}

\textbf{Формулировка задачи:} Рассмотрим задачу классификации с использованием метода опорных векторов (SVM). Пусть у нас есть набор точек данных с двумя классами:

- Класс 1: ( (1, 2), (2, 3), (3, 3) )
- Класс -1: ( (5, 4), (6, 5), (7, 7) )

Найдите опорные векторы для этой задачи.

\textbf{Решение:} Сначала мы должны построить гиперплоскость, разделяющую два класса. В методе опорных векторов опорные векторы — это точки данных, которые лежат на границе между классами и определяют положение этой границы.

Для простоты предположим, что мы можем провести линейную границу. Мы можем использовать алгоритм SVM для нахождения этой границы. Опорные векторы будут теми точками данных, которые находятся ближе всего к этой границе.

В данном примере опорными векторами могут быть точки из обоих классов, которые находятся ближе всего к разделяющей гиперплоскости. Например, если гиперплоскость проходит между точками ( (3, 3) ) и ( (5, 4) ), то опорными векторами будут:

- Из класса 1: ( (3, 3) )
- Из класса -1: ( (5, 4) )

Эти точки определяют положение разделяющей гиперплоскости и влияют на результат классификации.

\section{Тематическое моделирование}
Тематическое моделирование представляет подход к моделированию текстов. Определим некоторые основные понятия и обозначения для них:
\begin{itemize}
    \item Терм ($w$) = единица языка, в качестве которой может выступать слово, словосочетание или токен. Далее мы для простоты мы будем называть его просто словом.
    \item Документ ($d$) = неупорядоченный набор термов. 
    \item Частота терма $w$ в документе $d$ (обозначается $n_{wd}$) = количество повторений терма
\end{itemize}
Теперь мы готовы формально определить главную задачу.\\
\textbf{Задача тематического моделирования}\\
\textbf{Дано:} Множество документов $D$, а значит все встречающиеся в них термы $W$ и распределение частот термов $n_{wd}$.\\
\textbf{Найти:} Вероятностную тематическую модель:
\begin{equation}\label{tem_def}
    p(w|d) = \sum\limits_{t\in T} p(w|t) p(t|d) = \sum\limits_{t\in T} \phi_{wt} \theta_{td}, 
\end{equation}
где матрицы $\phi_{wt} = p(w|t)$,  $\theta_{td} = p(t|d)$ --- параметры модели.

Теперь подробно рассмотрим то, что здесь написано. На интуитивном уровне у нас есть понимание того, что такое тема текста. Такое деление не вполне точно, поэтому мы условно будем говорить о том, что существует вероятность $p(t|d)$ принадлежности темы $t$ к тексту $d$. С другой стороны у нас есть понимание, какой набор слов обычно используется в какой-либо теме. Одно и то же слово $w$ присутствует в разных темах $t$ с вероятностями $p(w|t)$. Теперь становится видно, что определение тематической модели вида \eqref{tem_def} является почти точной записью формулы Байеса. Отличие лишь в предположении: 
\begin{equation}
    p(w|d,t) = p(w|t), 
\end{equation}
которое называется \textbf{гипотеза о частотной независимости}, то есть что частота употребления слова в теме не зависит от того, в каком тексте оно пишется.

На данную задачу можно смотреть с разных сторон:
\begin{enumerate}
    \item \textbf{Мягкая би-кластеризация.} Мы решаем задачу кластеризации на пространстве слов и текстов, где в качестве кластеров выступают темы, а всякое слово или текст могут принадлежать к разным темам.
    \item \textbf{Разделение смеси распределений.} Каждый текст в этой модели является смесью распределений $p(w|t)$ и задачей тематического моделирования становится нахождение этих распределений и того, с какими весами они входят $p(t|d)$.
    \item \textbf{Автокодировщик текстов.} В данной модели кодировщиком текста является отображение между вектором распределений слов $p(w|d)$ и вектором распределений по темам $p(t|d)$. Для декодирования данного отображения используется простое умножение на матрицу $p(w|t)$.
    \item \textbf{Матричное разложение.} Дана большая матрица $p(w|d)$ размера $|W|\times|D|$ и мы решаем задачу о представлении её в виде произведении матриц меньшего размера $|W|\times|T|$ и $|T|\times|D|$.
\end{enumerate}

Теперь мы поняли, как интуитивно обосновывается тематическая модель \eqref{tem_def} и с какими задачами она связана. Перейдём к тому, как она решается.

Для начала, число выбранных тем выбирается неоднозначно, но оно должно быть сильно меньше количества слов и текстов:
\begin{equation}
    |T| \ll min(|W|, |D|)
\end{equation}

Конечно же, разложение \eqref{tem_def} на практике не бывает предельно точным, поэтому мы должны ввести метрику того, насколько точно наше решение. Возьмём принцип максимального правдоподобия, а точнее его логарифма:
\begin{equation}
    \ln\left(\prod\limits_{d\in D}\prod\limits_{w\in W} p(w, d)^{n_{w,d}}\right) = \max
\end{equation} 
Используя формулу \eqref{tem_def}, получаем:
\begin{equation}
    \prod\limits_{d\in D}\prod\limits_{w\in W}n_{w,d} \ln\left(\sum\limits_{t\in T} \phi_{wt} \theta_{td}\right) = \max\limits_{\Phi, \Theta}
\end{equation} 
Ограничениями при формальном определении такой задачи является то, что матрицы $\Phi$ и $\Theta$ являются стохастическими, то есть:
\begin{equation}
    \left\{
    \begin{aligned}
        &\phi_{wt} \geq 0, ~~~\theta_{td} \geq 0\\
        &\sum_{w\in W} \phi_{wt} = \sum_{t\in T} \theta_{td} = 1
    \end{aligned}
    \right.
\end{equation}
Данная задача относится к классу максимизации функции на единичных симплексах и рассказывается в ином параграфе. Заранее также отметим, что данная задача не может быть решена однозначно и для однозначного решения вводятся дополнительные требования регуляризации, что также рассказывается в ином параграфе.

\subsection*{Задача 1}
Оцените применимость тематического моделирования на следующих выборках документов:
\begin{enumerate}
    \item Все сообщения, которые писал пользователь за последний год в телеграмме.
    \item Собрание сочинений Льва Толстого (без дневников и разделения на главы). 
    \item Тексты новостей Lenta.ru за последние двадцать лет.
\end{enumerate}
\textbf{Решение:}
\begin{enumerate}
    \item Почти никакое сообщение из мессенджера не подходит в качестве документа, потому что, как правило, оно слишком короткое, чтоб затронуть множество тем. Таким образом ''аномальными'' будут почти все документы. 
    \item Лев Толстой написал 174 произведения, что сравнительно мало по сравнению с тем, сколько тем он при этом затронул. Значит тематическое моделирование применять не стоит.
    \item Тексты новостей одного издания хорошо подходят для тематического моделирования!
\end{enumerate}


\subsection*{Задача 2}
Можно ли с помощью тематического моделирования получить генератор текста по заданной тематике? Можно ли его использовать для рекомендательной системы новостей?\\
\textbf{Решение:}\\
1. Использование набора контекстных слов по текущей теме вполне может пригодиться для упрощения алгоритма генератора текста, однако конечно для него нужно что-то ещё! В основе тематического моделирования лежит представление текста как \textbf{неупорядоченного набора термов}. То есть получить связный текст по заданной теме из того, как часто встречаются слова в этой теме просто так не получится. \\
2. А вот для рекомендаций новостей модель хорошо подойдёт без существенных усложнений! Можно для каждого пользователя хранить веса тем новостей, на которые он кликает чаще всего и предлагать свежие новости в соответствие с его предпочтениями. Конечно, для оценки нового текста потребуется оценивать его по уже существующей тематической модели.

\subsection*{Задача 3}
Представим, что вы анализируете тексты статей журнала J. У вас есть гипотеза, что публикующиеся в журнале статьи хорошо разделяются на несколько меньших, несвязных между собой, областей. Как можно проверить данную гипотезу на основе существующей тематической модели?\\
\textbf{Решение:}\\
Для данной задачи не потребуется анализировать матрицу распределения слов по темам. Нужно только распределение документов по темам. Согласно гипотезе, существует хорошо определённая задача кластеризации, где в качестве ''точек'' берутся статьи, а в качестве соответствующих ''векторов'' --- веса распределения статьи по набору тем. Степень того, насколько хорошо получается решить данную задачу кластеризации оценивает верность гипотезы. Для большей точности следует исключить веса тем, связанных с формальной структурой статьи (абстракт, введение, заключение и т. д.)

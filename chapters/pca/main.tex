\section{Введение в метод главных компонент(PCA)}
\textbf{Метод главных компонент (Principal Component Analysis, PCA)} — это статистический метод, используемый для снижения размерности данных с сохранением наиболее значимой информации. PCA находит новые признаки (главные компоненты), 
которые представляют собой линейные комбинации исходных признаков, причем эти компоненты ортогональны и ранжированы по величине объясняемой дисперсии.
\textbf{Основные этапы метода:} \\
1. \textbf{Центрирование данных}:\\ Данные центрируются так, чтобы среднее значение каждой переменной было равно нулю:
$$
X_c=X-\bar{X},
$$
где $X$ - исходная матрица данных (размер $n \times p$ ), $\bar{X}$ - вектор средних значений по столбцам.\\
2. \textbf{Построение ковариационной матрицы}: \\ Вычисляется ковариационная матрица:
$$
\Sigma=\frac{1}{n-1} X_c^T X_c
$$
где $\Sigma$ - симметричная матрица размером $p \times p$.\\
3. \textbf{Собственные значения и собственные векторы}: \\ Решается задача нахождения собственных значений и собственных векторов ковариационной матрицы:
$$
\Sigma \mathbf{v}_i=\lambda_i \mathbf{v}_i
$$
где $\lambda_i$ - собственные значения, $\mathbf{v}_i$ - соответствующие им собственные векторы.\\
4. \textbf{Ранжирование главных компонент}: \\ Собственные значения упорядочиваются по убыванию:
$$
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p
$$
Первые несколько компонент, соответствующие самым большим собственным значениям, объясняют большую часть дисперсии данных.\\
5. \textbf{Проекция данных}: \\ Данные проецируются на главные компоненты:
$$
Z=X_c V_k,
$$
где $V_k$ — матрица $k$ собственных векторов, соответствующих $k$ наибольшим собственным значениям, $Z$ — матрица данных в пространстве главных компонент.

Свойства метода: \\
- Главные компоненты ортогональны:

$$
\mathbf{v}_i^T \mathbf{v}_j=0, \quad i \neq j
$$

- Дисперсия объясняется последовательностью собственных значений:

$$
\text { Объясненная дисперсия }=\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i} \text {. }
$$

\section{Задачи на использование метода главных компонент}

\subsection{Задача 1: Вклад признаков в главные компоненты}
Пусть $X \in R_{n\times p}$ набор данных с n образцами (строками) и p признаками (столбцами). PCA стремится найти набор собственных векторов (главных компонент), которые максимизируют дисперсию данных при проецировании на эти векторы.

Задача состоит в том, чтобы математически оценить, какой вклад вносит каждый признак в главные компоненты, и проранжировать признаки в зависимости от их вклада.\\ \\
\textbf{Решение:}
Метод РСА ищет собственные векторы $\mathbf{v}_i$ и собственные значения $\lambda_i$ удовлетворяющие:

$$
\Sigma \mathbf{v}_i=\lambda_i \mathbf{v}_i,
$$


где $\lambda_i$ - величина дисперсии данных вдоль $\mathbf{v}_i$.
Собственные векторы $\mathbf{v}_i$ формируют матрицу $V=\left[\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p\right]$, где каждый столбец $\mathbf{v}_i$ указывает направления главных компонент.\\
\textbf{Вклад признака в главные компоненты:}\\

Каждый признак в $X$ вносит вклад в главные компоненты через веса собственных векторов $\mathbf{v}_i$ . Элементы $v_{i j}$ (где $v_{i j}-j$-й элемент $i$-го собственного вектора) определяют значимость $j$-го признака для $i$-й главной компоненты.

Вклад $j$-го признака в $i$-ю главную компоненту оценивается как квадрат соответствующего элемента $v_{i j}^2$ .\\

Общий вклад $j$-го признака во все главные компоненты можно найти, суммируя его взвешенные вклады с учётом дисперсий ( $\lambda_i$ ):\\
$j=\sum_{i=1}^p \lambda_i v_{i j}^2$.\\
Этот показатель учитывает как значимость признака для каждой компоненты $\left(v_{i j}^2\right)$, так и долю дисперсии, объясняемую компонентой ( $\lambda_i$ ).\\
На конкретном примере:
Пусть собственные вектора образуют матрицу V:

$$
V=\left[\begin{array}{cccc}
0.5 & 0.6 & 0.3 & 0.1 \\
0.4 & -0.7 & 0.2 & 0.5 \\
-0.6 & 0.2 & 0.7 & -0.4 \\
0.5 & 0.3 & -0.6 & -0.6
\end{array}\right]
$$
Собственные значения:

$$
\Lambda=\operatorname{diag}(4.0,2.5,1.2,0.3)
$$

Посчитаем вклад признака $1(j=1)$ :
Для этого берём первую строчку $V$ :

$$
v_{1, \cdot}=[0.5,0.6,0.3,0.1]
$$
Считаем:

$$
\begin{aligned}
\text { Contribution }_1 & =\left(0.5^2 \cdot 4.0\right)+\left(0.6^2 \cdot 2.5\right)+\left(0.3^2 \cdot 1.2\right)+\left(0.1^2 \cdot 0.3\right) \\
& =1.0+0.9+0.108+0.003=2.011
\end{aligned}
$$

То же самое повторяем для остальных строк и находим максимальное значение.
\subsection{Задача 2: Ошибка "реконструкции" PCA}
Пусть $X \in R_{n\times p}$ набор данных с $n$ образцами (строками) и $p$ признаками (столбцами), с помощью метода главных компонент нужно:
\begin{itemize}
\item{Спроецируйте данные в более низкоразмерное пространство, определяемое $k$ главными компонентами.}
\item {Реконструируйте исходные данные из пространства пониженной размерности.}
\item {Вычислите ошибку реконструкции и оцените, как она меняется в зависимости от количества сохраняемых компонент k.}
\end{itemize}
\textbf{Решение:}
Для реконструкции данных из $k$-мерного подпространства используется обратная проекция:

$$
\hat{X}=Z V_k^T+\bar{X}
$$


Здесь:
- $Z V_k^T$ возвращает проекцию данных в исходное $p$-мерное пространство.\\
- Добавление $\bar{X}$ восстанавливает исходное смещение данных. \\
Ошибка реконструкции должна показывать какую часть информации мы потеряли при использовании только $k$ компонент при репрезентации данных.\\
1. Определим ошибку реконструкции для одного объекта $x_i$ :

$$
E_i=\left\|x_i-\hat{x}_i\right\|^2=\left\|\left(x_i-\bar{X}\right)-\left(z_i V_k^{\top}\right)\right\|^2
$$

2. Обобщим на весь набор данных:

$$
E=\frac{1}{n \times p} \sum_{i=1}^n\left\|x_i-\hat{x}_i\right\|^2
$$

3. Заменим на выражение для $\hat{x}_i$:

$$
E=\frac{1}{n \times p} \sum_{i=1}^n\left\|x_i-\bar{X}-Z V_k^{\top}\right\|^2
$$
Мы получили выражение для ошибки реконструкции. Теперь докажем, что 
ошибка реконструкции $E$ уменьшается монотонно с $k$, и когда $k=p, E=0$.

1. Общая дисперсия данных - это след ковариационной матрицы, которая представляет собой сумму всех собственных значений:

$$
\text { Total Variance }=\sum_{j=1}^p \lambda_j
$$

2. Дисперсия, которую уловили $k$ компонент это:

$$
\text { Captured Variance }=\sum_{j=1}^k \lambda_j
$$

3. Ошибка реконструкции по сути является дисперсией, которую не удалось уловить, то есть просто:

$$
E=\text { Total Variance }- \text { Captured Variance }=\sum_{j=k+1}^p \lambda_j
$$

4. Так как $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$, добавление большего числа компонент ( $k \rightarrow k+1$ ) уменьшает $E$ :

$$
\sum_{j=k+1}^p \lambda_j>\sum_{j=k+2}^p \lambda_j
$$

5. В тот момент, когда $k=p, \sum_{j=k+1}^p \lambda_j=0$ $\Rightarrow$ $E=0$.

\subsection{Задача 3: Построить критерий D-оптимальности для выбора лучшиз k-компонент }
Набор данных представляет собой матрицу $n \times p$, где n - число образцов (строк), а p - число признаков (столбцов). Введём критерий D-оптимальности, используемый для выбора подмножества точек из набора данных, которое максимизирует детерминант информационной матрицы.
$$
D_{opt} : max  det(X^{T}X)
$$

Ключевым свойством критерия D-оптимальности является то, что он максимизирует объём многомерной фигуры, которая получается из рассматриваемых признаков. \\
Нужно построить критерий D-оптимальности для выбора лучших 
k главных компонент, которые максимизируют детерминант объясненной дисперсии (или объём фигуры) в k-мерном подпространстве PCA. 

\textbf{Решение:}

Критерий D-оптимальности для подпространства $k$ задаётся максимизацией детерминанта информационной матрицы $\Lambda_k$ :

$$
D_{o p t}(k)=\max \operatorname{det}\left(\Lambda_k\right)
$$

Так как $\Lambda_k$ является диагональной матрицей, её детерминант равен произведению собственных значений:

$$
\operatorname{det}\left(\Lambda_k\right)=\prod_{i=1}^k \lambda_i
$$

Итак, наша задача сводится к выбору $k$-мерного подпространства (т.е. первых $k$ главных компонент), которые максимизируют произведение $\lambda_1 \cdot \lambda_2 \cdots \cdot \lambda_k$, что эквивалентно решению следующей задачи:

$$
\max _{V_k} \prod_{i=1}^k \lambda_i
$$


где $\lambda_i$ - собственные значения матрицы ковариации $\Sigma$.
Для вычисления $D_{\text {opt }}(k)$ :\\
\\
1. Центрируем данные:

$$
X_c=X-\bar{X},
$$


где $\bar{X}$ - матрица средних значений.\\
2. Вычисляем ковариационную матрицу:

$$
\Sigma=\frac{1}{n-1} X_c^T X_c
$$

3. Находим собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_p$ и соответствующие собственные векторы $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$.\\
4. Выбираем первые $k$ собственных значений $\lambda_1, \lambda_2, \ldots, \lambda_k$, которые максимизируют:

$$
\prod_{i=1}^k \lambda_i
$$


Максимизация $\prod_{i=1}^k \lambda_i$ эквивалентна максимизации объёма \textbf{$k$-мерного эллипсоида}, описывающего данные в пространстве первых $k$ главных компонент. Это позволяет отобрать $k$ измерений, которые сохраняют максимальную дисперсию данных.
\end{document}

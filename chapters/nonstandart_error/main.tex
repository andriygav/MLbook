\section*{Нестандартные функции потерь. Метод наименьших модулей. Квант\'{и}льная регрессия.}

Квадратичная функция потерь обычно дает хорошие результаты, является удобной в использовании, а потому и применяется чаще всего. Но в некоторых ситуациях она все же неприменима, и приходится использовать нестандартные функции потерь.

\subsection*{Метод наименьших модулей.}

Здесь и далее будем использовать следующие обозначения: $\ell$ - количество объектов в тренировочной выборке, $x_i$ ($i \in \left\{1, \dotsc, \ell \right\}$) - вектор признаков, $y_i$ ($i \in \left\{1, \dotsc, \ell \right\}$) - таргет, $a$ - модель, $\varepsilon_i := a(x_i) - y_i$ ($i \in \left\{1, \dotsc, \ell \right\}$).

Для стандартной квадратичной функции потерь $\mathscr{L}(\varepsilon) = \varepsilon^2$ задача будет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left(a(x_i) - y_i\right)^2 \longrightarrow \min\limits_{a}.$$

Заменим функцию потерь на $\mathscr{L}(\varepsilon) = |\varepsilon|$.
Задача станет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left|a(x_i) - y_i\right| \longrightarrow \min\limits_{a}.$$
Данный метод называется \textit{методом наименьших модулей}.

В какой ситуации такой подход может оказаться полезным? Рассмотрим ситуацию,  когда наша модель - это константа, то есть она вообще не зависит от признаков. Для квадратичной функции потерь получим:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left(a - y_i\right)^2 \longrightarrow \min\limits_{a}.$$
Здесь можно посчитать ответ аналитически, оптимальной константой $a$ будет $a = \frac{1}{\ell}\sum\limits_{i=1}^\ell y_i$, то есть среднее арифметическое таргетов. Это плохая оценка, если, например, в нашей выборке присутствуют выбросы, или распределение ошибок имеет тяжёлые <<хвосты>>.

А в случае использования метода наименьших модулей задача будет выглядеть так:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\left|a - y_i\right| \longrightarrow \min\limits_{a}.$$
Здесь опять же можно посчитать ответ аналитически, оптимальным $a$ будет $a = \median\left\{ y_1, \dotsc, y_\ell \right\} = y_{(\ell / 2)}$ (серединный член вариационного ряда). Эта оценка хороша тем, что устойчива к выбросам, хорошо работает для распределений ошибок с тяжелыми <<хвостами>>.

Таким образом, использование метода наименьших модулей в некоторых ситуациях помогает бороться с выбросами.

\subsection*{Квант\'{и}льная регрессия.}

Метод наименьших модулей можно обобщить. Давайте по-разному штрафовать отрицательные и положительные ошибки. Т.е. будем рассматривать функцию потерь вида
$$\mathscr{L}(\varepsilon) = \begin{cases}
    C_+|\varepsilon|, \varepsilon \geq 0,\\
    C_-|\varepsilon|, \varepsilon < 0.
\end{cases}$$

\begin{figure}[h]
    \centering
    \includegraphics{chapters/nonstandart_error/images/ФПКР.png}
\end{figure}

Опять же рассмотрим случай, когда модель не зависит от признаков, то есть является константой:
$$\frac{1}{\ell}\sum\limits_{i=1}^\ell\mathscr{L}\left(a - y_i\right) \longrightarrow \min\limits_{a}.$$
Решение данной задачи опять же можно получить аналитически, оптимальным $a$ будет $a = y_{(q)}$ ($q$-тый член вариационного ряда), где $q = \frac{\ell C_-}{C_- + C_+}$.
Именно поэтому используемый метод называется \textit{методом квант\'{и}льной регрессии}.

Рассмотрим также еще один случай, когда квантильная регрессия оказывается хорошим вариантом с точки зрения решения задачи оптимизации, а именно случай линейной модели: $a(x) = \left\langle x, w \right\rangle$, где $w$ - вектор весов.

Сделаем замену переменных $\varepsilon_i^+ = (a(x_i) - y_i)_+$, $\varepsilon_i^- = (a(x_i) - y_i)_-$. Тогда наша задача будет иметь вид
$$\begin{cases}
    \frac{1}{\ell}\sum_{i=1}^\ell C_+\varepsilon_i^+ + C_-\varepsilon_i^- \longrightarrow \min\limits_{w},\\
    \left\langle w, x_i \right\rangle - y_i = \varepsilon_i^+ - \varepsilon_i^-, i \in \left\{ 1 , \dotsc, \ell \right\}.
\end{cases}$$

Это задача линейного программирования, для решения которой существует масса способов.

\newpage

\subsection*{Задачи.}

\subsubsection*{Задача 1.}

Предположим, у Вас есть датасет с данными о жителях некоторой страны Южной или Центральной Африки, а также данные об их среднем ежедневном доходе, и Вы хотите научиться предсказывать этот доход по значениям рассматриваемых признаков. Полученная модель в последующем будет использоваться для составления плана по оказанию гуманитарной помощи населению. Что использовать предпочтительнее: квадратичную функцию потерь или функцию потерь квант\'{и}льной регрессии? Почему? Если использовать предпочтительнее функцию потерь квант\'{и}льной регрессии, то каким должно быть соотношение параметров $C_+$ и $C_-$ и почему?

\begin{solution}
    В условии задачи не зря указано, из какого региона у нас страна. Б\'{о}льшая часть населения в ней, скорее всего, крайне бедна, при этом есть очень незначительное количество сверхбогатых людей, т.е., в нашей терминологии, выбросов. Поэтому функция потерь квант\'{и}льной регрессии более предпочтительна, чем квадратичная функция потерь.

    Теперь обратим внимание на то, как будет использована модель, чтобы понять, какая ошибка для нас наиболее страшна: положительная или отрицательная. Заметим, что если мы предсказали доход человека больше реального, т.е. ошибка положительна, то, скорее всего, на него будет выделено меньше помощи. Это кажется более страшным, чем ситуация, в которой мы предсказали доход человека меньше реального, и дали ему чуть больше помощи. Поэтому штрафовать положительные ошибки стоит сильнее, чем отрицательные, то есть стоит выбрать $C_+$ и $C_-$ так, чтобы $C_+/C_- > 1$.
\end{solution}

\subsubsection*{Задача 2.}

Предположим, что у в природе некоторый таргет действительно линейно зависит от вектора признаков, но вектор таргетов, который у нас есть, зашумлен ошбиками, причем ошибки эти приходят из симметричного распределения Коши. Почему использование квадратичной функции потерь в данном случае будет крайне нежелательным? А если ошибки приходит из распределения $\mathcal{N}(a, \sigma^2)$, где $a \neq 0$?

\begin{solution}
    Из теории вероятностей и математической статистики известно, что у распределения Коши тяжелые <<хвосты>>, из-за чего у него даже нет матожидания. Именно поэтому квадратичная функция потерь, очень сильно штрафующая за большие ошибки, здесь не подходит. А вот метод наименьших модулей подхолит лучше.

    Также можно заметить, что квадратичная функция потерь одинаково штрафует положительные и отрицательные ошибки, то есть неявно подразумевается симметричность распределения. В общем случае это не так. Квант\'{и}льная регрессия позволяет более гибко работать с несимметричными распределениями.
\end{solution}

\subsubsection*{Задача 3.}

Предположим, что Вы тестируете новое лекарство на мышах. Вы хотите понять, какая доза лекарства оптимальна: уже вылечивает мышь, но все еще не дает негативных побочных эффектов. Известно, что каждый новый побочный эффект проявляется при превышении некоторого примерно одинакового для всех мышей порога передозировки и действует все сильнее и сильнее при дальнейшем превышении этого порога. Также известно, что оптимум не единственен, а достигается на некотором отрезке, длина которого вам тоже заранее известна. Наконец, ниже некоторого порога лекарство действует все хуже и хуже. У Вас есть некоторые данные о мышах, а также таргеты - максимальные оптимальные дозы лекарств. Предложите модификацию функции потерь квант\'{и}льной регрессии, оптимальную для данной задачи.

\begin{solution}
    Основная идея квант\'{и}льной регрессии по сравнению с методом наименьших модулей состоит в том, что мы по-разному штрафуем положительные и отрицательные ошибки. Давайте разовьем эту идею, и будем по-разному штрафовать ошибки в зависимости от того, в какую часть числовой прямой мы попали.

    Так, для нашей задачи, пусть новые побочные эффекты проявляются при превышении дозировки на $a_1, \dotsc, a_n$ у.е., где $0= a_1 < \ldots < a_n$. А длина оптимального отрезка дозировки равна $b$, Тогда можно взять такую функцию потерь:
    $$\mathscr{L}(\varepsilon) = \begin{cases}
        -v_b\varepsilon, \varepsilon < -b,\\
        0, \varepsilon \in [-b; a_1],\\
        v_{a_1}\varepsilon, \varepsilon \in (a_1; a_2],\\
        (v_{a_1} + v_{a_2})\varepsilon, \varepsilon \in (a_2; a_3],\\
        \vdots\\
        (v_{a_1} + \dotsb + v_{a_{n - 1}})\varepsilon, \varepsilon \in (a_{n-1}; a_n],\\
        (v_{a_1} + \dotsb + v_{a_n})\varepsilon, \varepsilon > a_n,
    \end{cases}$$
    где $v_b, v_{a_1}, \dotsc, v_{a_n}$ - скорости роста соответствующих проблем.
\end{solution}

\newpage
\section*{SVM-Regression}

Регрессионные модели на основе метода опорных векторов (SVM-regression) зачастую используют особую функцию потерь, отличающуюся от стандартной квадратичной ошибки (MSE). В частности, классический SVM для регрессии использует \(\varepsilon\)-insensitive функцию потерь (\(\varepsilon\)-insensitive loss), которая игнорирует ошибки, величина которых меньше заранее заданного порога \(\varepsilon\). Однако существуют и нестандартные функции потерь, которые могут быть полезны в специфических задачах, например, когда необходимо учитывать асимметрию ошибок, различные веса отдельных наблюдений или же более сложные, ориентированные на распределение отклонений, формулы.

Классическая \(\varepsilon\)-insensitive функция потерь:
Для модели вида \(f(x) = \langle w, x \rangle + b\) функция потерь определяется следующим образом:
\begin{align*}
L(\varepsilon) = \max(0, | y - f(x) | - \varepsilon).
\end{align*}

Если \(| y - f(x) | \leq \varepsilon\), то штраф равен нулю, что делает модель нечувствительной к небольшим отклонениям и позволяет контролировать сложность аппроксимации.

Нестандартные функции потерь могут иметь следующие особенности:

\begin{enumerate}
    \item Асимметричные функции потерь: например, более сильное штрафование положительных отклонений, чем отрицательных, или наоборот. Это может быть полезно, если цена переоценки или недооценки целевой переменной различна.
    \item Частично-кусочные функции: использование различных режимов штрафования в зависимости от величины отклонения, например, линейная область штрафа для малых ошибок и квадратичная — для больших.
    \item Функции с весами для разных точек: если некоторые точки обучающей выборки важнее, можно ввести веса и штрафовать ошибки по более значимым точкам сильнее.
    \item Нелинейные преобразования ошибки: например, логарифмическая или экспоненциальная функция потерь, которая меняет чувствительность модели к большим отклонениям.
\end{enumerate}

Пример асимметричной \(\varepsilon\)-insensitive функции потерь:
\begin{align*}
    L(\varepsilon, \alpha) = 
        \begin{cases}
            \case 0, & | y - f(x) | \leq \varepsilon \\
            \case \alpha(y - f(x)) - \varepsilon, & y - f(x) > \varepsilon \\
            \case \frac{(y - f(x))}{\alpha} - \varepsilon, & f(x) - y > \varepsilon
        \end{cases}
\end{align*}
где \(\alpha > 1\) задаёт степень асимметрии штрафования. Такая функция потерь позволит модели сильнее реагировать на недостаточную оценку и слабее —-- на переоценку.

\newpage
\subsection*{Задачи}

\subsubsection*{Задача 1.}
Пусть у нас есть набор точек для регрессии \(\{(x_i, y_i)\}_{i=1}^m\), и используется \(\varepsilon\)-insensitive функция потерь. Как будет влиять на итоговую модель выбор \(\varepsilon\)? Что произойдёт с моделью при увеличении и при уменьшении \(\varepsilon\)?

\subsubsection*{Решение}
При увеличении \(\varepsilon\) расширяется область, внутри которой ошибка не штрафуется. Что означает, что модель меньше старается подогнать точно каждую точку. Итоговая функция при этом может стать более гладкой и с меньшей чувствительностью к выбросам, но при этом с большой вероятностью систематической ошибки внутри расширенной области.

При уменьшении \(\varepsilon\) модель стремится к более точному описанию данных. При этом модель станет более чувствительной к шуму и выбросам.

\subsubsection{Задача 2.}
Предположим, что мы хотим использовать асимметричную \(\varepsilon\)-нечувствительную функцию потерь. Запишите целевую функцию и ограничения для такой SVM-регрессии (линейный случай), если функция потерь равна:

\begin{align*}
    L(\varepsilon) = 
    \begin{cases}
            \case 0, & | y_i - f(x_i) | \leq \varepsilon \\
            \case 2(y_i - f(x_i) - \varepsilon), & y_i - f(x_i) > \varepsilon \\
            \case f(x_i) - y_i- \varepsilon, & f(x_i) - y_i > \varepsilon
        \end{cases}
\end{align*}

\subsubsection{Решение}
Введём неотрицательные переменные \(\xi_i^+\) и \(\xi_i^-\) для верхних и нижних ошибок. Тогда ограничения на ошибки с учётом \(\varepsilon\):

\begin{align*}
    y_i - (w^Tx_i + b) \leq \varepsilon + \xi_i^+\\
    (w^Tx_i + b) - y_i \leq \varepsilon + \xi_i^-.
\end{align*}

Целевая функция с учётом асимметрии штрафов:
\begin{align*}
    \min_{w, b, \xi_i^+, \xi_i^-} \frac{1}{2} \| w \|^2 + C \sum\limits_{i = 1}^n (2\xi_i^+ + \xi_i^-)
\end{align*}

Таким образом, мы увеличили штраф в два раза для положительных отклонений (когда модель переоценивает значение, \(f(x_i) > y_i +\varepsilon\)) по сравнению с отрицательными отклонениями.

\subsubsection{Задача 3.}
Пусть у нас есть три варианта функций потерь для SVM-регрессии над одним и тем же набором данных \(\{(x_i, y_i)\}_{i=1}^m\):
\begin{enumerate}
    \item Стандартная \(\varepsilon\)-insensitive функция потерь.
    \item Модифицированная \(\varepsilon\)-insensitive функция потерь, где штраф начинается не с \(0\), а с небольшой линейной части: 
    \begin{align*}
        L(\varepsilon) = \max(0, | y - f(x) | - \varepsilon) + \delta(y - f(x)),
    \end{align*}
    где \(\delta > 0\) малый коэффициент.

    \item Huber-функция потерь с параметром \(\delta\).
\end{enumerate}

Предположим, что мы обучили три SVM-регрессии, каждый с одной из этих функций потерь, при одинаковых параметрах \(C\). Укажите, в каких случаях может быть предпочтительна Huber-функция по сравнению с 
\(\varepsilon\)-insensitive, и для чего может пригодиться модификация с добавлением линейной части \(\delta(y - f(x))\).

\subsubsection{Решение}
\begin{enumerate}
    \item Huber-функция потерь предпочтительна в ситуациях, когда в данных присутствуют выбросы или редкие, но крупные отклонения. Она сочетает в себе квадратичную форму для небольших ошибок (что способствует более мягкой подгонке и «дружелюбно» относится к шуму) и линейную для больших отклонений (не позволяя слишком сильно штрафовать крупные ошибки и тем самым уменьшая чувствительность к выбросам).
    \item Модифицированная \(\varepsilon\)-insensitive функция потерь с дополнительной линейной частью может быть полезна, когда нам важно учесть даже малые ошибки, но мы хотим сохранить идею \(\varepsilon\)-зоны. Добавление \(\delta(y - f(x))\) означает, что даже если ошибка меньше \(\varepsilon\), мы всё же учитываем некий штраф (хотя и небольшой). Это может помочь, когда не хочется полностью игнорировать малые отклонения, но при этом сохранять определённую «зону толерантности» к маленьким ошибкам, чтобы не переобучаться на шумах.
\end{enumerate}

\section*{Энтропийные функции потерь. Перекрёстная энтропия.}

В задачах классификации (бинарной или многоклассовой) очень распространено использование энтропийных функций потерь, в первую очередь перекрёстной энтропии. Эти функции потерь тесно связаны с понятиями теории вероятностей и информационной теории, в частности с понятием дивергенции Кульбака–Лейблера и энтропией Шеннона.

Основная идея: мы хотим не просто предсказать «класс», но и оценить распределение вероятностей классов. Пусть у нас есть:

\begin{itemize}
    \item Набор объектов: $(x_1, y_1), \dots, (x_\ell, y_\ell)$, где $x_i$ – вектор признаков, а $y_i$ – соответствующий истинный класс объекта (для простоты – номер класса или унитарный вектор-«one-hot»).
    \item Модель $a$, выдающая оценку распределения вероятностей по классам для входа $x_i$. Обозначим предсказанную моделью вероятность класса $k$ как $p_{ik} = a_k(x_i)$, где $\sum_k p_{ik} = 1$.
\end{itemize}

\subsection*{Бинарная классификация}

В случае двух классов, обозначим целевой класс как $y_i \in \{0,1\}$ и предсказанную моделью вероятность класса 1 для объекта $i$ как $p_i = a_1(x_i)$. Тогда функция потерь на одном объекте может быть записана как (перекрёстная энтропия для бинарного случая):

$$
\mathcal{L}(y_i, p_i) = -\left[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right].
$$

Эта функция потерь равна отрицательному логарифму правдоподобия при условии, что $y_i$ берётся из Бернуллиевского распределения с параметром $p_i$. Минимизация этой функции потерь равносильна максимизации правдоподобия.

\subsection*{Многоклассовая классификация}

Пусть класс $y_i$ закодирован в one-hot формате как вектор $Y_i = (y_{i1}, \dots, y_{iK})$, где $y_{ik} = 1$, если объект относится к классу $k$, и 0 в противном случае. Предсказание модели есть вероятностный вектор $P_i = (p_{i1}, \dots, p_{iK})$. Тогда перекрёстная энтропия:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^{K} y_{ik}\log(p_{ik}).
$$

Оптимизируя по параметрам модели, мы стремимся сделать предсказанное распределение вероятностей $P_i$ как можно более «острым» и совпадающим с истинным распределением мишени $Y_i$ (которое в обучающих данных детерминировано и задаётся one-hot вектором).

\subsection*{Связь с KL-дивергенцией}

Перекрёстная энтропия между истинным распределением $Y_i$ и предсказанным $P_i$ связана с дивергенцией Кульбака–Лейблера:

$$
H(Y_i, P_i) = H(Y_i) + D_{\text{KL}}(Y_i \| P_i),
$$

где $H(Y_i)$ – энтропия истинного распределения (константа в рамках оптимизации), а $D_{\text{KL}}(Y_i \| P_i)$ – дивергенция Кульбака–Лейблера, которая всегда неотрицательна. Минимизируя перекрёстную энтропию, мы минимизируем $D_{\text{KL}}$, что приводит к более точному приближению истинного распределения предсказанным.

\subsection*{Модификации}

\begin{itemize}
    \item \textbf{Взвешенная перекрёстная энтропия}: для решения проблем несбалансированных классов можно вводить веса $w_k$ для каждого класса:
    $$
    \mathcal{L}(Y_i, P_i) = -\sum_{k=1}^{K} w_k y_{ik}\log(p_{ik}).
    $$
    
    \item \textbf{Label smoothing}: позволяет сгладить «жесткие» one-hot лейблы, заменяя значение 1 на $1-\alpha$ и распределяя оставшуюся массу $\alpha$ равномерно по остальным классам. Это снижает перенакрой и делает модель более устойчивой.
\end{itemize}

\newpage

\section*{Задачи}

\subsubsection*{Задача 1 (Бинарная классификация и правдоподобие)}

Пусть у нас есть задача бинарной классификации: $y_i \in \{0,1\}$, и модель $a(x_i)$, дающая оценку вероятности класса 1 для $x_i$. Предположим, что истинное распределение метки $y_i$ для данного $x_i$ – это бернуллиевское распределение с параметром $p_i = a(x_i)$. Покажите, что минимизация средней перекрёстной энтропии

$$
\frac{1}{\ell}\sum_{i=1}^{\ell} \left[-y_i\log(p_i) - (1 - y_i)\log(1 - p_i)\right]
$$

эквивалентна максимизации правдоподобия выборки. Объясните, почему это даёт статистически обоснованную функцию потерь.

\begin{solution}
Правдоподобие выборки при условии независимости объектов есть:

$$
L = \prod_{i=1}^{\ell} p_i^{y_i}(1 - p_i)^{1 - y_i}.
$$

Взяв логарифм, получим:

$$
\log L = \sum_{i=1}^{\ell} [y_i\log(p_i) + (1 - y_i)\log(1 - p_i)].
$$

Максимизация $\log L$ по параметрам модели эквивалентна минимизации

$$
-\log L = -\sum_{i=1}^{\ell} [y_i\log(p_i) + (1 - y_i)\log(1 - p_i)],
$$

что есть сумма (или среднее) перекрёстной энтропии. Таким образом, минимизация перекрёстной энтропии совпадает с максимизацией правдоподобия. Это придаёт функции потерь статистическое обоснование: мы получаем состоятельную оценку параметров модели при верном специфицировании вероятностной модели.
\end{solution}

\subsubsection*{Задача 2 (Небаланс классов)}

Предположим, что у вас есть задача многоклассовой классификации с сильно несбалансированными классами. Один из классов встречается существенно реже других. Объясните, как можно модифицировать функцию перекрёстной энтропии, чтобы придать более высокий штраф за неправильную классификацию редкого класса. Предложите конкретную формулу и аргументируйте её использование.

\begin{solution}
Стандартная перекрёстная энтропия для многоклассовой задачи:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^K y_{ik}\log(p_{ik}).
$$

Если класс $r$ встречается редко, мы можем ввести для него повышающий вес $w_r > 1$. Тогда функция потерь модифицируется так:

$$
\mathcal{L}(Y_i, P_i) = -\sum_{k=1}^K w_k y_{ik}\log(p_{ik}),
$$

где $w_k = 1$ для всех «частых» классов, а $w_r > 1$ для редкого класса. Это увеличивает штраф за ошибку на редком классе и стимулирует модель уделять ему больше внимания. В итоге модель будет «стараться» точнее предсказывать редкий класс, жертвуя немного точностью на других, более частых классах, что зачастую улучшает общую полезность модели при решении практических задач с несбалансированными данными.
\end{solution}

\subsubsection*{Задача 3 (Label Smoothing)}

Предположим, что у вас есть задача многоклассовой классификации с $K$ классами. Истинные метки заданы в виде one-hot векторов. Вы подозреваете, что модель может слишком «уверенно» переобучаться, пытаясь подогнать вероятности к очень жёсткому распределению (где истинный класс имеет вероятность 1, а остальные 0). Предложите модификацию перекрёстной энтропии (label smoothing), объясните, в чём она заключается и как именно изменится функция потерь.

\begin{solution}
Идея label smoothing состоит в том, чтобы заменить истинный one-hot вектор $Y_i$ на более «сглаженный» вектор $Y_i'$. Пусть $\alpha$ – небольшой положительный параметр сглаживания (например, 0.1). Тогда:

\begin{itemize}
    \item Для истинного класса $c_i$ объекта $i$ мы присваиваем $y_{ic_i}' = 1 - \alpha$.
    \item Для всех остальных классов $k \neq c_i$ мы присваиваем $y_{ik}' = \frac{\alpha}{K-1}$.
\end{itemize}

Таким образом, истинный вектор $(0,\dots,0,1,0,\dots,0)$ превращается в вектор, где истинный класс имеет вероятность чуть меньше 1, а остальные классы получают небольшую ненулевую вероятность.

Новая функция потерь будет выглядеть так:

$$
\mathcal{L}(Y_i', P_i) = -\sum_{k=1}^K y_{ik}' \log(p_{ik}).
$$

Поскольку $Y_i'$ теперь не является «жёстким» one-hot вектором, модель не будет излишне стремиться предсказывать для истинного класса вероятность ровно 1, что снижает риск переобучения и делает распределение прогнозов более гладким и устойчивым.
\end{solution}


\newpage

\section*{Нестандартные функции потерь. Функции потерь для обработки временных рядов.}

Временные ряды часто имеют уникальные особенности, такие как сезонность, тренд и автокорреляция, которые необходимо учитывать при построение модели. Поэтому использование стандартных функций потерь не всегда рационально.

Сезонность относится к периодическим колебаниям в данных временных рядов, которые происходят в определенные временные интервалы. Это могут быть ежедневные, еженедельные, ежемесячные или годовые паттерны. Например, увеличение продаж в праздничные периоды.
Поэтому требуются функции потерь, способные учитывать периодические паттерны, важно правильно взвешивать ошибки для разных периодов. Стандартные функции потерь не справляются с этой задачей.

Тренд - долгосрочные изменения в данных, которые могут быть линейными или нелинейными,
Например, рост цен на жилье связанное с ростом населения города или экономики в целом.
Функции потерь должны адекватно работать при наличии долгосрочного тренда, важно учитывать разные масштабы значений в начале и конце прогнозируемого ряда.

Автокорреляция это мера корреляции между значениями ряда с разницей во времени, зависимость текущих значений от предыдущих, что требует учета временной структуры данных.
Стандартные функции потерь не учитывают эту зависимость, тогда как, специальные функции могут учитывать структуру автокорреляции.

Далее используются следующие обозначения:\\
$y_{i,t}$ - значение $i$ временного ряда в момент времени $t$.\\
$f_{i,t}$ - предсказанное значение $i$ временного ряда в момент времени $t$.\\
$f_{i,t}^q$ - предсказанный квантиль $q$ временного ряда $i$ в момент времени $t$\\
$N$ - количество временных рядов.\\
$T$ -  длина временного ряда.\\
$H$ - длина предсказанных значений.\\

Нестандартные функции потерь используемые для работы с временными рядами:\\
\\
$MASE$ (Mean Absolute Scaled Error)

$$MASE = \frac{1}{N} \frac{1}{H} \sum_{i = 1}^{N} \frac{1}{a_{i}}\sum_{t = T + 1} ^ {T + H}|y_{i,t} - f_{i,t}| $$ , где $a_{i}$
$$a_{i} = \frac{1}{T - m} \sum_{t = m + 1}^{T}|y_{i,t} - y_{i, t - m}|$$, для $m$ - период сезонности.

Фактически $MASE$ это $MAE$ предсказанных значений разделенное на $MAE$ наивного прогноза(предсказанное значение это последнее наблюдаемое значение), с поправкой на сезонность ряда.
$MASE$ устойчивее к выбросам, учитывает сезонность поэтому подходят для оценки временных рядов.\\
\\
$SQL$ Scaled quantile loss.
$$SQL = \frac{1}{N} \frac{1}{H} \sum_{i = 1}^{N} \frac{1}{a_{i}}\sum_{t = T + 1} ^ {T + H}p_{q}(y_{i,t}, f_{i,t}^{q})$$, где
$$p_{q}(y_{i,t}, f_{i,t}^{q}) = \begin{cases}
    2(1 - q)(f_{i,t}^q - y_{i,t}), если y_{i,t} < f_{i,t}^q\\
    2q(y_{i,t} - f_{i,t}^q ),если y_{i,t} \geq f_{i,t}^q
\end{cases}$$
\\
Совпадает с $MASE$, если $q = \frac{1}{2}$, устойчивость к выбросам, позволяет оценивать качество предсказаний на разных уровнях (квинтилях), что дает более полное представление о распределении ошибок. Это полезно, когда важно учитывать как нижние, так и верхние границы предсказаний. Это делает $SQL$ полезным инструментом для анализа и предсказания временных рядов, особенно в ситуациях, когда важно учитывать распределение ошибок и устойчивость к выбросам.\\
\\
Функция потерь учитывающая автокорреляцию. ACF loss может быть определена как разница между автокорреляцией фактических значений и автокорреляцией предсказанных значений

Для временного ряда - автокорреляция на лаге k определяется как:
    \[
   \text{ACF}(k) = \frac{\sum_{t=k+1}^{N} (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t=1}^{N} (y_t - \bar{y})^2}
   \]

   где \( \bar{y} \) — среднее значение временного ряда\\

$ACF_$ loss
   \[
   \text{ACF loss} = \sum_{k=1}^{K} \left( \text{ACF}_{\text{actual}}(k) - \text{ACF}_{\text{predicted}}(k) \right)^2
   \]
   где \( K \) — максимальный лаг, на котором вы хотите оценивать автокорреляцию.
Таким образом, ACF loss измеряет, насколько хорошо предсказанные значения сохраняют автокорреляционные свойства временного ряда. \\
\\
Задача 1\\
\\
При прогнозировании спроса на товар важнее не допустить нехватки товара, чем его переизбытка. Какую функцию потерь следует использовать?
\\
\\
Ответ:
Scaled Quantile Loss с $q$ > 0.5 (например, 0.8), что приведет к более "осторожным" прогнозам с меньшей вероятностью недооценки, также такая фукнция потерь учитывает сезонность, в отличие от стандартных функций потерь.
\\
\\
Задача 2\\
\\
Для задачи финансового анализа необходимо предсказывать цены акций, которые имеют сильные временные зависимости. Какую функцию потерь следует использовать для этой задачи и почему?
\\
\\
Ответ: Используйте ACF loss, чтобы сохранить автокорреляционные свойства временного ряда. Это важно, так как финансовые данные часто имеют зависимость от времени, и использование ACF loss поможет избежать искажений в оценках риска и улучшить качество предсказаний, сохраняя структуру временных зависимостей.
\\
\\
Задача 3\\
Компании необходимо прогнозировать ежемесячные продажи. Товар компании имеет явную сезонность, какую фукнцию потерь лучше использовать и почему?
\\
Ответ: MASE будет оптимальным выбором, так как она, в отличие от стандартных функций, учитывает сезонность и менее чувствительна к выбросам.
\\

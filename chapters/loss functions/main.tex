\documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{color}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{url}
\usepackage{multirow}
\usepackage{lastpage}
\usepackage{systeme}
\usepackage{booktabs}
\usepackage{float}

\theoremstyle{definition}
\newtheorem*{definition}{Определение}
\theoremstyle{definition}
\newtheorem*{proposition}{Предложение}
\theoremstyle{definition}
\newtheorem{problem}{Задача}
\theoremstyle{remark}
\newtheorem*{remark}{Замечание}
\theoremstyle{remark}
\newtheorem*{solution}{Решение:}

\usepackage[toc,page]{appendix}
\usepackage{hyperref}

\usepackage{geometry}
\geometry{left=1.5cm}
\geometry{right=1.5cm}
\geometry{top=2.0cm}
\geometry{bottom=3.0cm}
\renewcommand{\baselinestretch}{1.0}

%https://tex.stackexchange.com/questions/163451/total-number-of-citations
\usepackage{totcount}
\newtotcounter{citnum} %From the package documentation
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}

\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

\usepackage[
backend=biber,
style=numeric,
sorting=ynt
]{biblatex}
% Не работает в TeXstudio
%\addbibresource{chapters/*/main.bib}
\addbibresource{chapters/general/main.bib}
\addbibresource{chapters/linear/main.bib}
\addbibresource{chapters/neural/main.bib}
\addbibresource{chapters/metric/main.bib}
\addbibresource{chapters/svm/main.bib}
\addbibresource{chapters/pca/main.bib}
\addbibresource{chapters/nonlinear/main.bib}
\addbibresource{chapters/general_linear/main.bib}
\addbibresource{chapters/nonstandart_error/main.bib}
\addbibresource{chapters/feature_selection/main.bib}
\addbibresource{chapters/logical/main.bib}
\addbibresource{chapters/rules/main.bib}
\addbibresource{chapters/mixture/main.bib}
\addbibresource{chapters/boosting/main.bib}
\addbibresource{chapters/bayesian/main.bib}
\addbibresource{chapters/clustering/main.bib}
\addbibresource{chapters/unlabeled/main.bib}

\setlength{\parindent}{0pt} % Удаляет отступы для всего документа
\usepackage{tabularx} % Пакет для адаптивных таблиц
\usepackage[none]{hyphenat} % Добавляет переносы слов
\sloppy % Позволяет LaTeX гибче переносить слова и уменьшает ошибки Overfull
\usepackage{makecell} % Для удобного переноса строк в ячейках
\usepackage{multirow} % Для многоклеточных ячеек
\usepackage{booktabs} % Для красивых таблиц

\begin{document}


\section{Введение}

Функция потерь является ключевым компонентом в обучении моделей машинного обучения, определяющим не только качество обучения, но и способность модели к обобщению на новых данных. В задачах регрессии выбор функции потерь особенно критичен, так как он влияет на следующие аспекты:

\begin{enumerate}
    \item \textbf{Обработка ошибок разного масштаба}
    \begin{itemize}
        \item Способность функции различать и по-разному обрабатывать малые и большие отклонения
        \item Возможность настройки чувствительности к ошибкам разного масштаба
    \end{itemize}

    \item \textbf{Устойчивость к выбросам}
    \begin{itemize}
        \item Способность функции минимизировать влияние аномальных значений
        \item Баланс между точностью и робастностью
    \end{itemize}

    \item \textbf{Эффективность оптимизации}
    \begin{itemize}
        \item Влияние на скорость сходимости алгоритмов оптимизации
        \item Наличие и характер локальных минимумов
    \end{itemize}

    \item \textbf{Интерпретируемость}
    \begin{itemize}
        \item Простота понимания получаемых результатов
        \item Возможность сравнения качества разных моделей
    \end{itemize}
\end{enumerate}


\paragraph{Обозначения}
\begin{itemize}
    \item $n$: количество примеров в выборке
    \item $y_i$: истинное значение для $i$-го примера
    \item $\hat{y}_i$: предсказанное значение для $i$-го примера
\end{itemize}

\section{Основные функции потерь}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Среднеквадратичная ошибка (MSE)}

MSE имеет глубокое статистическое обоснование, связанное с методом максимального правдоподобия при предположении о нормальном распределении ошибок. Если рассматривать задачу регрессии как оценку условного математического ожидания $\mathbb{E}(Y|X)$, то MSE является оптимальной функцией потерь. Квадратичная форма функции следует из свойств нормального распределения и принципа максимизации энтропии. При минимизации MSE мы фактически решаем задачу поиска несмещенной оценки с минимальной дисперсией, что напрямую связано с теоремой Гаусса-Маркова в линейной регрессии.

\paragraph{Формула:}
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

\paragraph{Характеристики:}
\begin{itemize}
    \item Сильно штрафует большие отклонения из-за квадратичной зависимости
    \item Обеспечивает стабильную оптимизацию для линейных моделей
    \item Чувствительна к выбросам
    \item Наиболее эффективна в задачах, где большие ошибки недопустимы
\end{itemize}

\paragraph{Применение:} Финансовое прогнозирование, предсказание цен.

\paragraph{Ситуация:} Агентство недвижимости разрабатывает модель для оценки стоимости квартир в центре города. Ошибка в 1 миллион рублей для квартиры стоимостью 5 миллионов гораздо критичнее, чем такая же ошибка для квартиры за 20 миллионов.

\subsubsection*{Дано:}
\begin{itemize}
    \item Реальные цены: [5.2, 12.8, 18.5, 22.1, 7.4] млн руб
    \item Предсказанные: [6.1, 12.5, 17.8, 23.0, 8.5] млн руб
\end{itemize}


\section*{Решение:}

\begin{enumerate}
    \item {Найдем разницы (ошибки):}
    \[
    \begin{aligned}
    5.2 - 6.1 &= -0.9 \\
    12.8 - 12.5 &= 0.3 \\
    18.5 - 17.8 &= 0.7 \\
    22.1 - 23.0 &= -0.9 \\
    7.4 - 8.5 &= -1.1
    \end{aligned}
    \]

    \item {Возведем в квадрат:}
    \[
    \begin{aligned}
    (-0.9)^2 &= 0.81 \\
    (0.3)^2 &= 0.09 \\
    (0.7)^2 &= 0.49 \\
    (-0.9)^2 &= 0.81 \\
    (-1.1)^2 &= 1.21
    \end{aligned}
    \]

    \item {Найдем среднее:}
    \[
    \text{MSE} = \frac{0.81 + 0.09 + 0.49 + 0.81 + 1.21}{5} = \frac{3.41}{5} = 0.68
    \]
\end{enumerate}

\paragraph*{Python-код для решения задачи}

\begin{verbatim}
import numpy as np
import matplotlib.pyplot as plt

def solve_mse():
    """Решение задачи с MSE для прогнозирования цен на недвижимость"""
    print("\n=== Задача 1: MSE для прогнозирования цен на недвижимость ===")
    
    # Данные
    actual_prices = np.array([5.2, 12.8, 18.5, 22.1, 7.4])
    predicted_prices = np.array([6.1, 12.5, 17.8, 23.0, 8.5])
    
    # Решение
    errors = actual_prices - predicted_prices
    mse = np.mean(errors**2)
    rmse = np.sqrt(mse)
    relative_errors = np.abs(errors) / actual_prices * 100
    
    print("Решение:")
    print(f"MSE = {mse:.2f} млн руб²")
    print(f"RMSE = {rmse:.2f} млн руб")
    print("\nОтносительные ошибки по объектам:")
    for i, error in enumerate(relative_errors):
        print(f"Объект {i+1}: {error:.1f}%")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: сравнение цен
    plt.subplot(1, 2, 1)
    x = range(len(actual_prices))
    width = 0.35
    plt.bar(x, actual_prices, width, label='Реальные цены', color='skyblue')
    plt.bar([i + width for i in x], predicted_prices, width, label='Предсказанные цены', color='lightcoral')
    plt.xlabel('Объект')
    plt.ylabel('Цена (млн руб.)')
    plt.title('Сравнение цен на недвижимость')
    plt.legend()
    
    # График справа: квадраты ошибок
    plt.subplot(1, 2, 2)
    squared_errors = errors**2
    plt.bar(x, squared_errors, color='red', alpha=0.6)
    plt.axhline(y=mse, color='black', linestyle='--', label=f'MSE = {mse:.2f}')
    plt.xlabel('Объект')
    plt.ylabel('Квадрат ошибки')
    plt.title('Квадраты ошибок')
    plt.legend()
    
    plt.tight_layout()
    
    plt.savefig('imgs/mse.png')
    plt.show()
\end{verbatim}

\paragraph{Вывод программы}
\begin{verbatim}
Задача 1: MSE для прогнозирования цен на недвижимость
Решение:
MSE = 0.68 млн руб²
RMSE = 0.83 млн руб

Относительные ошибки по объектам:
Объект 1: 17.3%
Объект 2: 2.3%
Объект 3: 3.8%
Объект 4: 4.1%
Объект 5: 14.9%
\end{verbatim}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/mse.png}
    \caption{График квадратов ошибок.}
    \label{fig:mse_graph}
\end{figure}

\noindent\textbf{Выводы:}
\begin{itemize}
    \item Большие ошибки штрафуются значительно сильнее маленьких из-за возведения в квадрат.
    \item На графике квадратов ошибок это видно по тому, что даже небольшое увеличение отклонения приводит к значительному росту столбца.
    \item Если в данных есть выброс, его столбец будет непропорционально высоким.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Средняя абсолютная ошибка (MAE)}

MAE можно вывести как оптимальную функцию потерь при предположении о распределении ошибок по закону Лапласа. Геометрически MAE представляет собой L1-норму вектора ошибок, что делает её более устойчивой к выбросам по сравнению с L2-нормой в MSE. С точки зрения теории оптимизации, MAE приводит к задаче выпуклой оптимизации, но не является дифференцируемой в нуле, что может создавать определенные сложности при градиентном спуске. В статистическом смысле, минимизация MAE эквивалентна поиску условной медианы распределения целевой переменной.

\paragraph{Формула:}
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

\paragraph{Характеристики:}
\begin{itemize}
    \item Линейно штрафует отклонения любого размера
    \item Более устойчива к выбросам по сравнению с MSE
    \item Результат измеряется в тех же единицах, что и целевая переменная
    \item Оптимальна для задач с умеренными выбросами
\end{itemize}

\paragraph{Применение:} Прогнозирование спроса, метеорология

\paragraph{Ситуация:} Энергетическая компания прогнозирует ежедневное потребление электроэнергии для планирования нагрузки. Небольшие отклонения допустимы и легко компенсируются резервными мощностями.

\subsubsection*{Дано:}
\begin{itemize}
    \item Реальное потребление: [245, 256, 278, 235, 290]
    \item Предсказанное: [250, 262, 275, 230, 285]
\end{itemize}

\subsection*{Решение:}

\begin{enumerate}
    \item {Найдем модули разностей:}
    \[
    \begin{aligned}
    |245 - 250| &= 5 \\
    |256 - 262| &= 6 \\
    |278 - 275| &= 3 \\
    |235 - 230| &= 5 \\
    |290 - 285| &= 5
    \end{aligned}
    \]

    \item {Найдем среднее:}
    \[
    \text{MAE} = \frac{5 + 6 + 3 + 5 + 5}{5} = \frac{24}{5} = 4.8
    \]
\end{enumerate}

\paragraph*{Python-код для решения задачи}

\begin{verbatim}
def solve_mae():
    """Решение задачи с MAE для прогнозирования потребления электроэнергии"""
    print("\n=== Задача 2: MAE для прогнозирования потребления электроэнергии ===")
    
    # Данные
    actual_consumption = np.array([245, 256, 278, 235, 290])
    predicted_consumption = np.array([250, 262, 275, 230, 285])
    
    # Решение
    errors = actual_consumption - predicted_consumption
    mae = np.mean(np.abs(errors))
    
    print("Решение:")
    print(f"MAE = {mae:.2f} МВт·ч")
    print("\nАбсолютные ошибки по дням:")
    for i, error in enumerate(np.abs(errors)):
        print(f"День {i+1}: {error:.1f} МВт·ч")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: линии потребления
    plt.subplot(1, 2, 1)
    days = range(1, len(actual_consumption) + 1)
    plt.plot(days, actual_consumption, 'o-', label='Реальное потребление', color='blue')
    plt.plot(days, predicted_consumption, 'o--', label='Предсказанное потребление', color='red')
    plt.fill_between(days, actual_consumption, predicted_consumption, alpha=0.2, color='gray')
    plt.xlabel('День')
    plt.ylabel('Потребление (МВт·ч)')
    plt.title('Сравнение потребления электроэнергии')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График справа: абсолютные ошибки
    plt.subplot(1, 2, 2)
    abs_errors = np.abs(errors)
    plt.bar(days, abs_errors, color='red', alpha=0.6)
    plt.axhline(y=mae, color='black', linestyle='--', label=f'MAE = {mae:.2f}')
    plt.xlabel('День')
    plt.ylabel('Абсолютная ошибка (МВт·ч)')
    plt.title('Абсолютные ошибки')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    plt.savefig('imgs/mae.png')
    plt.show()
\end{verbatim}
\paragraph{Вывод программы}
\begin{verbatim}
=== Задача 2: MAE для прогнозирования потребления электроэнергии ===
Решение:
MAE = 4.80 МВт·ч

Абсолютные ошибки по дням:
День 1: 5.0 МВт·ч
День 2: 6.0 МВт·ч
День 3: 3.0 МВт·ч
День 4: 5.0 МВт·ч
День 5: 5.0 МВт·ч
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/mae.png}
    \caption{График абсолютных ошибок.}
    \label{fig:mae_graph}
\end{figure}

\noindent\textbf{Выводы:}
\begin{itemize}
    \item Ключевой момент: все ошибки штрафуются пропорционально их величине.
    \item На графике абсолютных ошибок столбцы растут линейно с увеличением отклонения.
    \item В отличие от MSE, нет "драматического" роста штрафа при больших ошибках.
\end{itemize}


    

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%3
\subsection{Средняя абсолютная процентная ошибка (MAPE)}

MAPE возникает из потребности в масштабно-инвариантной метрике ошибки. Её можно вывести из предположения о мультипликативном характере ошибок, когда отклонения пропорциональны абсолютным значениям целевой переменной. Математически это эквивалентно предположению, что логарифмы относительных ошибок распределены по закону Лапласа. MAPE особенно полезна в экономических приложениях, где важна относительная точность прогноза. Однако функция имеет особенность при $y_i = 0$, что следует учитывать при её применении.

\paragraph{Формула:}
\[
\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right| \times 100\%
\]

\paragraph{Характеристики:}
\begin{itemize}
    \item Представляет ошибку в процентном выражении
    \item Позволяет сравнивать модели на разных масштабах данных
    \item Неприменима при наличии нулевых значений в данных
    \item Удобна для бизнес-метрик
\end{itemize}

\paragraph{Применение:} Бизнес-прогнозирование, анализ продаж

\paragraph{Ситуация:} Сеть супермаркетов прогнозирует продажи разных категорий товаров - от жевательной резинки до бытовой техники. Важно иметь единую метрику для сравнения качества прогнозов разных категорий.

\subsubsection*{Дано:}

Жвачка (тыс шт):
\begin{itemize}
    \item Реальные: [1200, 1150, 1300, 1180, 1250]
    \item Прогноз: [1180, 1200, 1250, 1150, 1300]
\end{itemize}

Телевизоры (шт):
\begin{itemize}
    \item Реальные: [5, 8, 6, 7, 4]
    \item Прогноз: [6, 7, 5, 8, 5]
\end{itemize}

\subsection*{Решение для жвачки:}
1) Относительные ошибки:
\begin{align*}
\frac{|1200-1180|}{1200} \times 100\% &= 1.67\% \\
\frac{|1150-1200|}{1150} \times 100\% &= 4.35\% \\
\frac{|1300-1250|}{1300} \times 100\% &= 3.85\% \\
\frac{|1180-1150|}{1180} \times 100\% &= 2.54\% \\
\frac{|1250-1300|}{1250} \times 100\% &= 4.00\%
\end{align*}


2) Среднее:
\[
\text{MAPE}_{\text{жвачка}} = \frac{1.67 + 4.35 + 3.85 + 2.54 + 4.00}{5} = 3.28\%
\]

Аналогично для телевизоров:
\[
\text{MAPE}_{\text{телевизоры}} = 17.7\%
\]

\paragraph{Python-код для решения задачи:}

\begin{verbatim}
def solve_mape():
    """Решение задачи с MAPE для прогнозирования продаж"""
    print("\n=== Задача 3: MAPE для прогнозирования продаж ===")
    
    # Данные
    actual_gum = np.array([1200, 1150, 1300, 1180, 1250])
    actual_tv = np.array([5, 8, 6, 7, 4])
    predicted_gum = np.array([1180, 1200, 1250, 1150, 1300])
    predicted_tv = np.array([6, 7, 5, 8, 5])
    
    # Решение
    mape_gum = np.mean(np.abs((actual_gum - predicted_gum) / actual_gum)) * 100
    mape_tv = np.mean(np.abs((actual_tv - predicted_tv) / actual_tv)) * 100
    
    print("Решение:")
    print(f"MAPE для жевательной резинки: {mape_gum:.1f}%")
    print(f"MAPE для телевизоров: {mape_tv:.1f}%")
    
    # Визуализация
    plt.figure(figsize=(15, 5))
    
    # График для жевательной резинки
    plt.subplot(1, 3, 1)
    days = range(1, len(actual_gum) + 1)
    plt.plot(days, actual_gum, 'o-', label='Реальные продажи', color='blue')
    plt.plot(days, predicted_gum, 'o--', label='Прогноз', color='red')
    plt.title('Продажи жевательной резинки')
    plt.xlabel('День')
    plt.ylabel('Количество (шт.)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График для телевизоров
    plt.subplot(1, 3, 2)
    plt.plot(days, actual_tv, 'o-', label='Реальные продажи', color='blue')
    plt.plot(days, predicted_tv, 'o--', label='Прогноз', color='red')
    plt.title('Продажи телевизоров')
    plt.xlabel('День')
    plt.ylabel('Количество (шт.)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График сравнения MAPE
    plt.subplot(1, 3, 3)
    categories = ['Жев. резинка', 'Телевизоры']
    mape_values = [mape_gum, mape_tv]
    plt.bar(categories, mape_values, color=['skyblue', 'lightcoral'])
    plt.title('Сравнение MAPE')
    plt.ylabel('MAPE (%)')
    
    plt.tight_layout()
    
    plt.savefig('imgs/mape.png')
    plt.show()
\end{verbatim}
\paragraph{Вывод программы}
\begin{verbatim}
=== Задача 3: MAPE для прогнозирования продаж ===
Решение:
MAPE для жевательной резинки: 3.3%
MAPE для телевизоров: 17.7%
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/mape.png}
    \caption{Сравнение ошибок MAPE для двух категорий товаров.}
    \label{fig:mape_graph}
\end{figure}

\noindent\textbf{Выводы:}
\begin{itemize}
    \item Главное преимущество: можно сравнивать ошибки для величин разного масштаба
    \item На графиках видно, как MAPE позволяет сравнивать точность прогноза для жвачек (тысячи штук) и телевизоров (единицы штук)
    \item Процентная ошибка может быть одинаковой даже при очень разных абсолютных значениях
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Функция потерь Хубера (Huber Loss)}

Функция Хубера представляет собой элегантное решение проблемы компромисса между робастностью MAE и статистической эффективностью MSE. Она возникает в рамках теории робастной статистики как оптимальная М-оценка. Параметр $\delta$ определяет точку перехода между квадратичным и линейным режимами, что можно интерпретировать как адаптивное определение выбросов. Теоретически можно показать, что функция Хубера минимизирует максимальную асимптотическую дисперсию оценки при ограниченном влиянии выбросов (ограниченной функции влияния).

\paragraph{Формула:}
\[
L_\delta(y_i, \hat{y}_i) = 
\begin{cases} 
\frac{1}{2} (y_i - \hat{y}_i)^2, & \text{если } |y_i - \hat{y}_i| \leq \delta \\
\delta |y_i - \hat{y}_i| - \frac{1}{2} \delta^2, & \text{если } |y_i - \hat{y}_i| > \delta
\end{cases}
\]
где $\delta$ — параметр, определяющий границу между квадратичной и линейной частями.

\paragraph{Характеристики:}
\begin{itemize}
    \item Сочетает преимущества MSE и MAE
    \item Адаптивно переключается между квадратичной и линейной функцией
    \item Обеспечивает робастность к выбросам при сохранении чувствительности к малым ошибкам
    \item Требует настройки параметра $\delta$
\end{itemize}

\paragraph{Применение:} Задачи с зашумленными данными, финансовые прогнозы

\paragraph{Ситуация:} Служба доставки еды прогнозирует время доставки заказов. В данных есть выбросы (пробки, погодные условия), но они не должны сильно влиять на общую модель.

\subsubsection*{Дано:}
\begin{itemize}
    \item Реальное время: [25, 30, 28, 85, 27]
    \item Прогноз: [28, 32, 30, 35, 29]
    \item $\delta = 15$
\end{itemize}

\subsection*{Решение:}

\begin{enumerate}
    \item {Разности:}
    \[
    [-3, -2, -2, 50, -2]
    \]
    
    \item{Для каждого значения применяем формулу:}
    \[
    L_\delta(x) = \begin{cases}
    \frac{1}{2}x^2 & \text{если } |x| \leq \delta \\
    \delta|x| - \frac{1}{2}\delta^2 & \text{если } |x| > \delta
    \end{cases}
    \]
    Для $x = -3$: $\frac{1}{2}(9) = 4.5$
    Для $x = -2$: $\frac{1}{2}(4) = 2$
    Для $x = -2$: $\frac{1}{2}(4) = 2$
    Для $x = 50$: $15(50) - \frac{1}{2}(225) = 637.5$
    Для $x = -2$: $\frac{1}{2}(4) = 2$
    
    Среднее = 129.6
\end{enumerate}

\paragraph*{Python-код для решения задачи}

\begin{verbatim}
def solve_huber():
    """Решение задачи с Huber Loss для времени доставки"""
    print("\n=== Задача 4: Huber Loss для времени доставки ===")
    
    # Данные
    actual_times = np.array([25, 30, 28, 85, 27])
    predicted_times = np.array([28, 32, 30, 35, 29])
    delta = 15  # параметр Huber Loss
    
    # Решение
    errors = actual_times - predicted_times
    huber_losses = np.array([
        0.5 * err**2 if abs(err) <= delta else
        delta * abs(err) - 0.5 * delta**2
        for err in errors
    ])
    mean_huber_loss = np.mean(huber_losses)
    
    print("Решение:")
    print(f"Средняя Huber Loss (delta): {mean_huber_loss:.2f}")
    print("\nПотери по доставкам:")
    for i, loss in enumerate(huber_losses):
        print(f"Доставка {i+1}: {loss:.2f}")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: времена доставки
    plt.subplot(1, 2, 1)
    x = range(1, len(actual_times) + 1)
    plt.plot(x, actual_times, 'o-', label='Реальное время', color='blue')
    plt.plot(x, predicted_times, 'o--', label='Предсказанное время', color='red')
    plt.xlabel('Доставка')
    plt.ylabel('Время (мин)')
    plt.title('Времена доставки')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График справа: сравнение потерь
    plt.subplot(1, 2, 2)
    mse_losses = errors**2
    mae_losses = np.abs(errors)
    
    x_pos = np.arange(len(errors))
    width = 0.25
    
    plt.bar(x_pos - width, mse_losses, width, label='MSE', color='skyblue')
    plt.bar(x_pos, mae_losses, width, label='MAE', color='lightgreen')
    plt.bar(x_pos + width, huber_losses, width, label='Huber', color='lightcoral')
    
    plt.xlabel('Доставка')
    plt.ylabel('Значение функции потерь')
    plt.title('Сравнение функций потерь')
    plt.grid()
    plt.legend()
    
    plt.tight_layout()
    
    plt.savefig('imgs/huber.png')
    plt.show()
\end{verbatim}
\paragraph{Вывод программы}
\begin{verbatim}
=== Задача 4: Huber Loss для времени доставки ===
Решение:
Средняя Huber Loss (delta=15): 129.60

Потери по доставкам:
Доставка 1: 4.50
Доставка 2: 2.00
Доставка 3: 2.00
Доставка 4: 637.50
Доставка 5: 2.00
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/huber.png}
    \caption{График сравнения потерь для MSE, MAE и Huber.}
    \label{fig:huber_graph}
\end{figure}

\noindent\textbf{Выводы:}
\begin{itemize}
    \item Ключевая особенность: комбинирует преимущества MSE и MAE.
    \item На графике видно, что для маленьких ошибок ведет себя как MSE (квадратично).
    \item Для больших ошибок переключается на линейный рост, как MAE.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/huber_plus.png}
    \caption{Сравнение роста потерь для MSE, MAE и Huber на разных уровнях ошибок.}
    \label{fig:huber_plus}
\end{figure}

\noindent На левом графике:
\begin{itemize}
    \item MSE растет квадратично (очень быстро) для больших ошибок.
    \item MAE растет линейно.
    \item Huber сначала растет как MSE, а потом переходит на линейный рост как MAE.
\end{itemize}

На правом графике (детальный вид малых ошибок):
\begin{itemize}
    \item Для ошибок меньше $\delta=4$, Huber ведет себя как MSE.
    \item Для ошибок больше $\delta$, Huber начинает вести себя как MAE.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{5. Функция потерь Fair}

Fair Loss возникла из потребности в непрерывно дифференцируемой робастной функции потерь. Её можно рассматривать как гладкую аппроксимацию MAE, полученную через интегрирование сигмоидальной функции влияния. Параметр $\lambda$ определяет скорость насыщения функции потерь, что позволяет настраивать компромисс между точностью и робастностью. С точки зрения статистической теории, Fair Loss можно вывести как обобщённое правдоподобие с предположением о распределении ошибок с "тяжёлыми хвостами".

\subsection*{Формула:}

\[
\text{Fair Loss} = \sum_{i=1}^{n} \log\left(1 + \left|\frac{y_i - \hat{y}_i}{\lambda}\right|\right)
\]
где $\lambda$ - параметр, настраивающий чувствительность к ошибкам.

\subsection*{Характеристики:}
\begin{itemize}
    \item Обеспечивает плавное уменьшение влияния больших ошибок
    \item Настраиваемая чувствительность через параметр $\lambda$
    \item Хорошо работает с зашумленными данными
    \item Сохраняет дифференцируемость
\end{itemize}

\subsection*{Применение:} Задачи с неравномерным распределением ошибок

\subsection*{Ситуация:} Стриминговый сервис прогнозирует количество просмотров новых эпизодов сериалов. Некоторые эпизоды могут стать вирусными, создавая естественные выбросы.

\subsection*{Дано:}
\begin{itemize}
    \item Реальные просмотры: $[150, 165, 950, 180, 175]$
    \item Прогноз: $[160, 170, 200, 175, 180]$
    \item $c = 100$
\end{itemize}

\subsection*{Решение:}
1. Вычисляем ошибки:
\[
\begin{aligned}
e_1 &= 150 - 160 = -10 \\
e_2 &= 165 - 170 = -5 \\
e_3 &= 950 - 200 = 750 \\
e_4 &= 180 - 175 = 5 \\
e_5 &= 175 - 180 = -5
\end{aligned}
\]

2. Для каждой ошибки считаем Fair Loss:

Для $e_1 = -10$:
\[
\begin{aligned}
\frac{|e_1|}{c} &= \frac{|-10|}{100} = 0.1 \\
\ln\left(1 + \frac{|e_1|}{c}\right) &= \ln(1 + 0.1) = \ln(1.1) \approx 0.0953 \\
L_{\text{Fair}}(e_1) &= 100^2(0.1 - 0.0953) = 10000 \times 0.0047 = 47
\end{aligned}
\]

Для $e_2 = -5$:
\[
\begin{aligned}
\frac{|e_2|}{c} &= \frac{|-5|}{100} = 0.05 \\
\ln\left(1 + \frac{|e_2|}{c}\right) &= \ln(1 + 0.05) = \ln(1.05) \approx 0.0488 \\
L_{\text{Fair}}(e_2) &= 100^2(0.05 - 0.0488) = 10000 \times 0.0012 = 12
\end{aligned}
\]

Для $e_3 = 750$:
\[
\begin{aligned}
\frac{|e_3|}{c} &= \frac{|750|}{100} = 7.5 \\
\ln\left(1 + \frac{|e_3|}{c}\right) &= \ln(1 + 7.5) = \ln(8.5) \approx 2.14 \\
L_{\text{Fair}}(e_3) &= 100^2(7.5 - 2.14) = 10000 \times 5.36 = 53600
\end{aligned}
\]

Для $e_4 = 5$:
\[
\begin{aligned}
\frac{|e_4|}{c} &= \frac{|5|}{100} = 0.05 \\
\ln\left(1 + \frac{|e_4|}{c}\right) &= \ln(1 + 0.05) = \ln(1.05) \approx 0.0488 \\
L_{\text{Fair}}(e_4) &= 100^2(0.05 - 0.0488) = 10000 \times 0.0012 = 12
\end{aligned}
\]

Для $e_5 = -5$:
\[
\begin{aligned}
\frac{|e_5|}{c} &= \frac{|-5|}{100} = 0.05 \\
\ln\left(1 + \frac{|e_5|}{c}\right) &= \ln(1 + 0.05) = \ln(1.05) \approx 0.0488 \\
L_{\text{Fair}}(e_5) &= 100^2(0.05 - 0.0488) = 10000 \times 0.0012 = 12
\end{aligned}
\]

3. Вычисляем среднее значение:
\[
\begin{aligned}
\text{Среднее} &= \frac{47 + 12 + 53600 + 12 + 12}{5} &= \frac{53683}{5} &\approx 10736.51
\end{aligned}
\]
\paragraph*{Python-код для решения задачи}
\begin{verbatim}
def solve_fair():
    """Решение задачи с Fair Loss для просмотров контента"""
    print("\n=== Задача 5: Fair Loss для просмотров контента ===")
    
    # Данные
    actual_views = np.array([150, 165, 950, 180, 175])
    predicted_views = np.array([160, 170, 200, 175, 180])
    c = 100  # параметр Fair Loss
    
    # Решение
    errors = np.abs(actual_views - predicted_views)
    fair_losses = c**2 * (errors/c - np.log(1 + errors/c))
    mean_fair_loss = np.mean(fair_losses)
    
    print("Решение:")
    print(f"Средняя Fair Loss (c={c}): {mean_fair_loss:.2f}")
    print("\nОтклонения по эпизодам:")
    for i, (act, pred) in enumerate(zip(actual_views, predicted_views)):
        print(f"Эпизод {i+1}: отклонение {abs(act - pred)} тыс. просмотров")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: просмотры
    plt.subplot(1, 2, 1)
    x = range(1, len(actual_views) + 1)
    plt.plot(x, actual_views, 'o-', label='Реальные просмотры', color='blue')
    plt.plot(x, predicted_views, 'o--', label='Предсказанные просмотры', color='red')
    plt.xlabel('Эпизод')
    plt.ylabel('Просмотры (тыс.)')
    plt.title('Просмотры эпизодов')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График справа: Fair Loss
    plt.subplot(1, 2, 2)
    plt.bar(x, fair_losses, color='lightcoral', alpha=0.6)
    plt.axhline(y=mean_fair_loss, color='black', linestyle='--', 
                label=f'Средняя Fair Loss = {mean_fair_loss:.2f}')
    plt.xlabel('Эпизод')
    plt.ylabel('Fair Loss')
    plt.title('Значения Fair Loss')
    plt.yscale("log")
    plt.legend()
    
    plt.tight_layout()
    
    plt.savefig('imgs/fair.png')
    plt.show()
\end{verbatim}
\paragraph{Вывод программы}
\begin{verbatim}
Решение задачи 5: Fair Loss для просмотров контента

Средняя Fair Loss (с параметром $c=100$) составляет 10736.51.

Отклонения по эпизодам
    Эпизод 1: отклонение 10 тыс. просмотров
    Эпизод 2: отклонение 5 тыс. просмотров
    Эпизод 3: отклонение 750 тыс. просмотров
    Эпизод 4: отклонение 5 тыс. просмотров
 Эпизод 5: отклонение 5 тыс. просмотров
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/fair.png}
    \caption{График значений Fair Loss для разных эпизодов.}
\end{figure}

\textbf{Главное:} "Справедливое" отношение к ошибкам разного размера.

График показывает, как функция плавно уменьшает штраф для больших ошибок. Нет резких переходов, как в Huber Loss.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/fair_plus.png}
    \caption{Сравнение функции потерь Fair Loss с другими функциями потерь.}
\end{figure}

\subsection*{Ключевые особенности Fair Loss:}
\begin{itemize}
    \item Более мягкий рост для больших ошибок, чем у MAE
    \item Не такой агрессивный рост, как у MSE
    \item Плавное "насыщение" функции при увеличении ошибки
\end{itemize}

Это делает Fair Loss особенно полезной, когда нам нужно:
\begin{itemize}
    \item Уменьшить влияние выбросов
    \item Сохранить непрерывность и гладкость функции
    \item Избежать резких переходов между режимами работы
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Функция потерь Тьюки (Tukey Loss)}

Функция Тьюки является результатом поиска максимально робастной функции потерь с ограниченным влиянием выбросов. Её биквадратная форма обеспечивает плавное "отключение" влияния точек данных за пределами установленного порога. Математически функция возникает как решение задачи минимизации асимптотической дисперсии оценки при нулевом влиянии выбросов за пределами заданного расстояния. Теоретически можно показать, что функция Тьюки обладает оптимальными свойствами с точки зрения минимаксной робастности.

\subsection*{Формула:}
\[
\text{Tukey Loss} =
\begin{cases}
\frac{c^2}{6} \left(1 - \left(1 - \left(\frac{|y - \hat{y}|}{c}\right)^2\right)^3\right), & \text{если } |y - \hat{y}| \leq c \\
\frac{c^2}{6}, & \text{если } |y - \hat{y}| > c
\end{cases}
\]
где $c$ - параметр, контролирующий точку отсечения.

\subsection*{Характеристики:}
\begin{itemize}
    \item Полностью игнорирует выбросы за порогом $c$
    \item Обеспечивает максимальную робастность
    \item Требует тщательной настройки параметра $c$
    \item Дифференцируема во всей области определения
\end{itemize}

\subsection*{Применение:} Задачи с сильно зашумленными данными

\subsection*{Ситуация:} Промышленное предприятие анализирует данные с датчиков оборудования. Датчики иногда дают явно ошибочные показания из-за сбоев.

\subsection*{Дано:}
Реальные значения: [85, 82, 999, 88, 86] \\
Прогноз: [84, 83, 85, 87, 85] \\
$c = 10$

\subsection*{Решение:}

1. Вычисляем разности
\begin{align*}
x_1 &= 85 - 84 = 1 \\
x_2 &= 82 - 83 = -1 \\
x_3 &= 999 - 85 = 914 \\
x_4 &= 88 - 87 = 1 \\
x_5 &= 86 - 85 = 1
\end{align*}

2. Применяем функцию Tukey Loss

Для $x_1 = 1$ ($|x| \leq c$):
\[
\left(\frac{1}{10}\right)^2 = 0.01, \quad (1 - 0.01)^3 = 0.97, \quad L_{\text{Tukey}}(1) = \frac{100}{6}(1 - 0.97) \approx 0.5
\]

Для $x_2 = -1$ аналогично, $L_{\text{Tukey}}(x_2) \approx 0.5$

Для $x_3 = 914$ ($|x| > c$):
\[
L_{\text{Tukey}}(914) = \frac{100}{6} \approx 16.67
\]

Для $x_4 = 1$, $L_{\text{Tukey}}(x_4) \approx 0.5$

Для $x_5 = 1$, $L_{\text{Tukey}}(x_5) \approx 0.5$

Для обычных значений $L_{\text{Tukey}} \approx 0.5$, для выброса $L_{\text{Tukey}} = 16.67$.
\paragraph*{Python-код для решения задачи}
\begin{verbatim}
def solve_tukey():
    """Решение задачи с Tukey Loss для данных с датчиков"""
    print('=== Задача 6: Tukey Loss для данных с датчиков ===')
    # Данные
    actual_temp = np.array([85, 82, 999, 88, 86])  
    predicted_temp = np.array([84, 83, 85, 87, 85])
    c = 10  # параметр Tukey Loss
    
    # Вычисляем потери без нормализации
    errors = actual_temp - predicted_temp
    
    # MSE
    mse_losses = errors**2
    
    # MAE
    mae_losses = np.abs(errors)
    
    # Tukey Loss
    tukey_losses = np.array([
        (c**2/6) * (1 - (1 - min(1, abs(err/c))**2)**3) 
        for err in errors
    ])
    
    # Визуализация
    plt.figure(figsize=(15, 5))
    
    # График 1: Исходные данные
    plt.subplot(1, 3, 1)
    x = range(1, len(actual_temp) + 1)
    plt.plot(x, actual_temp, 'o-', label='Показания датчика', color='blue')
    plt.plot(x, predicted_temp, 'o--', label='Предсказанные значения', color='red')
    plt.xlabel('Измерение')
    plt.ylabel('Температура (°C)')
    plt.title('Исходные данные')
    plt.legend()
    
    # График 2: Абсолютные значения функций потерь
    plt.subplot(1, 3, 2)
    x_pos = np.arange(len(errors))
    width = 0.25
    
    plt.bar(x_pos - width, mse_losses, width, label='MSE')
    plt.bar(x_pos, mae_losses, width, label='MAE')
    plt.bar(x_pos + width, tukey_losses, width, label='Tukey')
    plt.xlabel('Измерение')
    plt.ylabel('Значение функции потерь')
    plt.title('Абсолютные значения потерь\n(логарифмическая шкала)')
    plt.yscale('log')
    plt.legend()
    
    # График 3: Значения функций потерь без выброса
    plt.subplot(1, 3, 3)
    mask = errors != errors[2]  # исключаем выброс
    x_pos = np.arange(sum(mask))
    
    plt.bar(x_pos - width, mse_losses[mask], width, label='MSE')
    plt.bar(x_pos, mae_losses[mask], width, label='MAE')
    plt.bar(x_pos + width, tukey_losses[mask], width, label='Tukey')
    plt.xlabel('Измерение (без выброса)')
    plt.ylabel('Значение функции потерь')
    plt.title('Значения потерь без выброса')
    plt.legend()
    
    plt.tight_layout()
    
    # Вывод значений
    print("\nЗначения функций потерь:")
    for i in range(len(errors)):
        print(f"\nИзмерение {i+1}:")
        print(f"MSE: {mse_losses[i]:.2f}")
        print(f"MAE: {mae_losses[i]:.2f}")
        print(f"Tukey: {tukey_losses[i]:.2f}")
        
    plt.savefig('imgs/tukey.png')
    plt.show()
\end{verbatim}

\subsection*{Вывод значений функций потерь:}
\paragraph{Вывод программы}
\begin{verbatim}
=== Задача 6: Tukey Loss для данных с датчиков ===
Значения функций потерь:

Измерение 1:
MSE: 1.00
MAE: 1.00
Tukey: 0.50

Измерение 2:
MSE: 1.00
MAE: 1.00
Tukey: 0.50

Измерение 3:
MSE: 835396.00
MAE: 914.00
Tukey: 16.67

Измерение 4:
MSE: 1.00
MAE: 1.00
Tukey: 0.50

Измерение 5:
MSE: 1.00
MAE: 1.00
Tukey: 0.50
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/tukey.png}
    \caption{График значений Tukey Loss для разных измерений.}
\end{figure}

\subsection*{Выводы:}
\begin{itemize}
    \item Критический момент: полностью игнорирует ошибки после определенного порога.
    \item На графике видно, что после превышения порога значение функции остается постоянным.
    \item Для нормальных данных (без выброса) ведет себя похоже на MSE.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Log-Cosh Loss}

Log-Cosh Loss можно рассматривать как гладкую аппроксимацию абсолютной ошибки, полученную через композицию логарифмической и гиперболической функций. Её можно вывести как решение дифференциального уравнения, требующего плавного перехода между квадратичным поведением для малых ошибок и линейным для больших. С точки зрения информационной теории, Log-Cosh связана с принципом максимальной энтропии при ограничениях на моменты распределения ошибок. Важным свойством является существование всех производных, что делает её привлекательной для методов оптимизации высокого порядка.

\paragraph{Формула}

\[
L_{\text{Log-Cosh}}(y_i, \hat{y}_i) = \frac{1}{n} \sum_{i=1}^{n} \log(\cosh(y_i - \hat{y}_i))
\]

\paragraph{Характеристики:}
\begin{itemize}
    \item Сочетает преимущества MSE и MAE
    \item Всегда дифференцируема
    \item Устойчива к выбросам
    \item Не требует настройки гиперпараметров
\end{itemize}

\paragraph{Применение:} Финансовое прогнозирование, общие задачи регрессии

\paragraph{Ситуация:} Финансовая компания прогнозирует курс акций. Важно сбалансированно учитывать как небольшие колебания, так и редкие сильные движения рынка.

\paragraph{Дано:}
\begin{itemize}
    \item Реальные цены: [145.5, 142.8, 158.2, 144.1, 146.3]
    \item Прогноз: [146.0, 143.5, 148.0, 145.0, 145.8]
\end{itemize}

1. Вычисляем разности
\[
\begin{aligned}
x_1 &= 145.5 - 146.0 = -0.5 \\
x_2 &= 142.8 - 143.5 = -0.7 \\
x_3 &= 158.2 - 148.0 = 10.2 \\
x_4 &= 144.1 - 145.0 = -0.9 \\
x_5 &= 146.3 - 145.8 = 0.5
\end{aligned}
\]

2. Применяем Log-Cosh Loss
Для $x_1$ = -0.5:
\[
\begin{aligned}
\cosh(-0.5) &= \frac{e^{-0.5} + e^{0.5}}{2} \approx 1.128 \\
L_{LogCosh}(-0.5) &= \ln(1.128) \approx 0.1201
\end{aligned}
\]

Для $x_2$ = -0.7:
\[
\begin{aligned}
\cosh(-0.7) &= \frac{e^{-0.7} + e^{0.7}}{2} \approx 1.255 \\
L_{LogCosh}(-0.7) &= \ln(1.255) \approx 0.2273
\end{aligned}
\]

Для $x_3$ = 10.2:
\[
\begin{aligned}
\cosh(10.2) &= \frac{e^{10.2} + e^{-10.2}}{2} \approx 13477.9 \\
L_{LogCosh}(10.2) &= \ln(13477.9) \approx 9.5069
\end{aligned}
\]

Для $x_4$ = -0.9:
\[
\begin{aligned}
\cosh(-0.9) &= \frac{e^{-0.9} + e^{0.9}}{2} \approx 1.433 \\
L_{LogCosh}(-0.9) &= \ln(1.433) \approx 0.3598
\end{aligned}
\]

Для $x_5$ = 0.5:
\[
\begin{aligned}
\cosh(0.5) &= \frac{e^{0.5} + e^{-0.5}}{2} \approx 1.128 \\
L_{LogCosh}(0.5) &= \ln(1.128) \approx 0.1201
\end{aligned}
\]

3. Вычисляем среднее значение
\[
\begin{aligned}
\text{Среднее} &= \frac{0.1201 + 0.2273 + 9.5069 + 0.3598 + 0.1201}{5} \\
&= \frac{10.3342}{5} \\
&\approx 2.0668
\end{aligned}
\]

\paragraph*{Python-код для решения задачи}
\begin{verbatim}
def solve_log_cosh():
    """Решение задачи с Log-Cosh Loss для прогнозирования курса акций"""
    print("\n=== Задача 7: Log-Cosh Loss для прогнозирования курса акций ===")
    
    # Данные
    actual_prices = np.array([145.5, 142.8, 158.2, 144.1, 146.3])
    predicted_prices = np.array([146.0, 143.5, 148.0, 145.0, 145.8])
    
    # Решение
    errors = actual_prices - predicted_prices
    log_cosh_losses = np.log(np.cosh(errors))
    mean_log_cosh = np.mean(log_cosh_losses)
    
    print("Решение:")
    print(f"Средняя Log-Cosh Loss: {mean_log_cosh:.4f}")
    print("\nЗначения потерь по дням:")
    for i, loss in enumerate(log_cosh_losses):
        print(f"День {i+1}: {loss:.4f}")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: цены акций
    plt.subplot(1, 2, 1)
    days = range(1, len(actual_prices) + 1)
    plt.plot(days, actual_prices, 'o-', label='Реальные цены', color='blue')
    plt.plot(days, predicted_prices, 'o--', label='Предсказанные цены', color='red')
    plt.fill_between(days, actual_prices, predicted_prices, alpha=0.2, color='gray')
    plt.xlabel('День')
    plt.ylabel('Цена (USD)')
    plt.title('Динамика цен акций')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График справа: сравнение функций потерь
    plt.subplot(1, 2, 2)
    mse_losses = errors**2
    mae_losses = np.abs(errors)
    
    x_pos = np.arange(len(errors))
    width = 0.25
    
    plt.bar(x_pos - width, mse_losses, width, label='MSE', color='skyblue')
    plt.bar(x_pos, mae_losses, width, label='MAE', color='lightgreen')
    plt.bar(x_pos + width, log_cosh_losses, width, label='Log-Cosh', color='lightcoral')
    
    plt.xlabel('День')
    plt.ylabel('Значение функции потерь')
    plt.yscale('log')
    plt.title('Сравнение функций потерь')
    plt.legend()
    
    plt.tight_layout()
    
    plt.savefig('imgs/log_cosh.png')
    plt.show()
\end{verbatim}
\paragraph{Вывод программы}
\begin{verbatim}
Решение:
Средняя Log-Cosh Loss: 2.0668

Значения потерь по дням:
День 1: 0.1201
День 2: 0.2273
День 3: 9.5069
День 4: 0.3598
День 5: 0.1201
\end{verbatim}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/log_cosh.png}
    \caption{Графики для Log-Cosh Loss}
\end{figure}

\paragraph{Основное свойство:} плавный переход от поведения MSE к поведению MAE

\paragraph{На графике:} для маленьких ошибок столбцы похожи на MSE. Для больших ошибок рост становится более линейным, как у MAE.

\subsection{Квантильная функция потерь (Quantile Loss)}

Квантильная функция потерь возникает естественным образом при решении задачи оценки условных квантилей распределения. Её можно вывести как решение задачи минимизации ожидаемых потерь при асимметричном штрафе за переоценку и недооценку. Математически это связано с задачей минимизации эмпирического риска с кусочно-линейной функцией потерь. В терминах теории вероятностей, минимизация квантильной функции потерь эквивалентна поиску точки, в которой функция распределения принимает заданное значение $\tau$. Это свойство делает её незаменимой в задачах оценки неопределённости и риск-менеджмента.

\paragraph{Формула:}

\[
L_{\text{Quantile}}(y_i, \hat{y}_i) = 
\begin{cases} 
\tau (y_i - \hat{y}_i), & \text{если } y_i \geq \hat{y}_i \\
(1 - \tau) (\hat{y}_i - y_i), & \text{если } y_i < \hat{y}_i
\end{cases}
\]
где $\tau$ - параметр квантиля $(0 < \tau < 1)$.

\paragraph{Характеристики:}
\begin{itemize}
    \item Позволяет оценивать разные квантили распределения
    \item Асимметрично штрафует ошибки разного знака
    \item Полезна для оценки неопределенности
    \item Требует выбора параметра $\tau$
\end{itemize}

\paragraph{Применение:} Оценка рисков, управление запасами

\paragraph{Ситуация:} Интернет-магазин определяет оптимальный размер запасов товаров. Недостаток товара приводит к потере продаж, а избыток - к затратам на хранение.

\paragraph{Дано:}
\begin{itemize}
    \item Реальный спрос: [120, 115, 125, 118, 122]
    \item Прогноз: [118, 117, 123, 119, 120]
    \item $\tau = 0.7$
\end{itemize}

\subsection*{Решение:}
1. Вычисляем разности
\[
\begin{aligned}
x_1 &= 120 - 118 = 2 \text{ (недооценка)} \\
x_2 &= 115 - 117 = -2 \text{ (переоценка)} \\
x_3 &= 125 - 123 = 2 \text{ (недооценка)} \\
x_4 &= 118 - 119 = -1 \text{ (переоценка)} \\
x_5 &= 122 - 120 = 2 \text{ (недооценка)}
\end{aligned}
\]

2. Применяем Quantile Loss:
Для $x_1 = 2$ (недооценка):
\[
L_{0.7}(2) = 0.7 \times 2 = 1.4
\]
Для $x_2 = -2$ (переоценка):
\[
L_{0.7}(-2) = (0.7 - 1) \times (-2) = 0.6
\]
Для $x_3 = 2$ (недооценка):
\[
L_{0.7}(2) = 0.7 \times 2 = 1.4
\]
Для $x_4 = -1$ (переоценка):
\[
L_{0.7}(-1) = (0.7 - 1) \times (-1) = 0.3
\]
Для $x_5 = 2$ (недооценка):
\[
L_{0.7}(2) = 0.7 \times 2 = 1.4
\]

3. Вычисляем среднее значение
\[
\begin{aligned}
\text{Среднее} &= \frac{1.4 + 0.6 + 1.4 + 0.3 + 1.4}{5} \\
&= \frac{5.1}{5} \\
&= 1.02
\end{aligned}
\]

\paragraph*{Python-код для решения задачи}
\begin{verbatim}
def solve_quantile():
    """Решение задачи с Quantile Loss для управления складскими запасами"""
    print("\n=== Задача 8: Quantile Loss для управления складскими запасами ===")
    
    # Данные
    actual_demand = np.array([120, 115, 125, 118, 122])
    predicted_demand = np.array([118, 117, 123, 119, 120])
    tau = 0.7  # квантиль (>0.5 означает, что переоценка предпочтительнее недооценки)
    
    # Решение
    errors = actual_demand - predicted_demand
    quantile_losses = np.array([
        tau * err if err >= 0 else (tau - 1) * err
        for err in errors
    ])
    mean_quantile_loss = np.mean(quantile_losses)
    
    print("Решение:")
    print(f"Средняя Quantile Loss (tau): {mean_quantile_loss:.2f}")
    print("\nАнализ ошибок прогноза:")
    for i, (act, pred) in enumerate(zip(actual_demand, predicted_demand)):
        error_type = "недооценка" if act > pred else "переоценка"
        print(f"День {i+1}: {error_type} на {abs(act - pred)} единиц")
    
    # Визуализация
    plt.figure(figsize=(12, 5))
    
    # График слева: спрос
    plt.subplot(1, 2, 1)
    days = range(1, len(actual_demand) + 1)
    plt.plot(days, actual_demand, 'o-', label='Реальный спрос', color='blue')
    plt.plot(days, predicted_demand, 'o--', label='Предсказанный спрос', color='red')
    plt.fill_between(days, actual_demand, predicted_demand, 
                    where=(actual_demand >= predicted_demand),
                    color='red', alpha=0.2, label='Недооценка')
    plt.fill_between(days, actual_demand, predicted_demand,
                    where=(actual_demand < predicted_demand),
                    color='blue', alpha=0.2, label='Переоценка')
    plt.xlabel('День')
    plt.ylabel('Спрос (шт.)')
    plt.title(f'Прогноз спроса (tau)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # График справа: потери
    plt.subplot(1, 2, 2)
    plt.bar(days, quantile_losses, color=['red' if ql > 0 else 'blue' 
                                        for ql in quantile_losses], alpha=0.6)
    plt.axhline(y=mean_quantile_loss, color='black', linestyle='--',
                label=f'Средняя потеря = {mean_quantile_loss:.2f}')
    plt.xlabel('День')
    plt.ylabel('Quantile Loss')
    plt.title('Значения квантильной функции потерь')
    plt.legend()
    
    plt.tight_layout()
    
    plt.savefig('imgs/quantile.png')
    plt.show()

\end{verbatim}

\paragraph{Вывод программы:}
\begin{verbatim}
=== Задача 8: Quantile Loss для управления складскими запасами ===
Решение:
Средняя Quantile Loss (tau=0.7): 1.02

Анализ ошибок прогноза:
День 1: недооценка на 2 единиц
День 2: переоценка на 2 единиц
День 3: недооценка на 2 единиц
День 4: переоценка на 1 единиц
День 5: недооценка на 2 единиц
\end{verbatim}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/quantile.png}
    \caption{Графики для Quantile Loss}
\end{figure}

\paragraph{Основное свойство:} асимметричное отношение к переоценке и недооценке

\paragraph{На графике:} разным цветом показаны ошибки разного типа. Если $\tau > 0.5$, недооценка штрафуется сильнее, чем переоценка (и наоборот).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Сравнительная таблица}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Функция} & \textbf{Устойчивость} & \textbf{Гладкость} & \textbf{Интерпретируемость} & \textbf{Основное} \\
\textbf{потерь} & \textbf{к выбросам} & & & \textbf{применение} \\
\hline
MSE    & Низкая & Высокая & Средняя & Точные предсказания \\
MAE    & Высокая & Средняя & Высокая & Робастные модели \\
MAPE   & Низкая & Средняя & Высокая & Бизнес-метрики \\
Huber  & Средняя & Высокая & Средняя & Универсальные задачи \\
Fair   & Высокая & Средняя & Средняя & Зашумленные данные \\
Tukey  & Очень высокая & Средняя & Низкая & Сильные выбросы \\
Log-Cosh & Высокая & Высокая & Средняя & Общие задачи \\
Quantile & Средняя & Низкая & Высокая & Оценка рисков \\
\hline
\end{tabular}
\caption{Сравнение функций потерь}
\end{table}

\paragraph{Общий принцип выбора функции потерь:}

\begin{itemize}
    \item \textbf{MSE} — если важно сильно штрафовать большие ошибки
    \item \textbf{MAE} — если все ошибки одинаково важны
    \item \textbf{MAPE} — для сравнения разных масштабов
    \item \textbf{Huber/Fair/Tukey} — если есть выбросы
    \item \textbf{Log-Cosh} — для плавного поведения
    \item \textbf{Quantile} — если разные типы ошибок имеют разную важность
\end{itemize}



\section{Заключение}

Функции потерь являются фундаментальным инструментом в машинном обучении, определяющим, как модель будет учиться на ошибках и адаптироваться к данным. Каждая функция потерь имеет свои уникальные характеристики и области применения: \textbf{MSE} эффективна для задач, где критичны большие отклонения, \textbf{MAE} обеспечивает устойчивость к выбросам, \textbf{MAPE} позволяет сравнивать ошибки на разных масштабах данных. Развитие этой области привело к появлению более сложных функций, таких как \textbf{Huber Loss}, \textbf{Fair Loss} и \textbf{Tukey Loss}, которые предлагают различные компромиссы между чувствительностью к выбросам и эффективностью обучения. \textbf{Log-Cosh Loss} обеспечивает плавное поведение и хорошую дифференцируемость, а \textbf{Quantile Loss} позволяет работать с асимметричными рисками. 

Выбор конкретной функции потерь должен основываться на специфике задачи, характере данных и требованиях к модели. При этом важно понимать не только математические свойства функций потерь, но и их практические последствия для конечного результата. Современные подходы часто комбинируют различные функции потерь или адаптируют их параметры в процессе обучения, что позволяет достигать оптимального баланса между различными требованиями к модели.



\end{document}
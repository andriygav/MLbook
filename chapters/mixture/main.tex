\section*{Random Forest как пример реализации композиции классификаторов}

Случайный лес (Random Forest) — это ансамблевый метод машинного обучения для задач классификации и регрессии, основанный на объединении множества решающих деревьев. Пусть у нас есть обучающая выборка 
\[
D = \{ (x_i, y_i) \mid i = 1, 2, \ldots, N \},
\]
где \(x_i \in \mathbb{R}^M\) — вектор признаков, а \(y_i\) — целевая переменная (класс или число).

Основная идея случайного леса:
\begin{enumerate}
    \item \textbf{Бэггинг (Bootstrap Aggregating):} Для построения каждого дерева \(T_b\), где \(b = 1, \ldots, B\), выбирается бутстрэп-выборка \(D_b\) размера \(N\) из \(D\) с возвращением. Таким образом, каждое дерево обучается на несколько отличающейся подвыборке, что уменьшает корреляцию между деревьями.
    \item \textbf{Случайный подбор признаков:} Для выбора лучшего признака на каждом разбиении узла дерева рассматривается случайное подмножество из \(m \leq M\) признаков. Это дополнительно уменьшает корреляцию между деревьями.
    \item \textbf{Агрегирование результатов:} Для классификации итоговый прогноз получается голосованием по большинству:
    \[
    \hat{y} = \text{argmax}_c \sum_{b=1}^B \mathbb{I}[T_b(x) = c],
    \]
    где \(c\) — номер класса.
    
    Для регрессии берётся среднее предсказаний всех деревьев:
    \[
    \hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(x).
    \]
\end{enumerate}

\subsection*{Меры качества разбиений}

При построении деревьев для классификации часто минимизируют критерий impurity, например энтропию или индекс Джини. Пусть узел \(t\) содержит объекты из подмножества обучающих данных. Для классификации с \(K\) классами доли объектов каждого класса в узле \(t\) есть:
\[
p_{tk} = \frac{\sum_{x_i \in t} \mathbb{I}[y_i = k]}{|t|}.
\]

\noindent Тогда \textbf{энтропия} узла:
\[
H(t) = -\sum_{k=1}^K p_{tk} \log_2 (p_{tk}),
\]
а \textbf{индекс Джини}:
\[
G(t) = 1 - \sum_{k=1}^K p_{tk}^2.
\]

Разбиение выбирают так, чтобы максимально снизить неопределённость:
\[
\Delta G = G(t) - \left( \frac{|t_L|}{|t|} G(t_L) + \frac{|t_R|}{|t|} G(t_R)\right),
\]
где \(t_L\) и \(t_R\) — левый и правый потомки узла \(t\).

\subsection*{Важность признаков}

Важность признака \(X_j\) в случайном лесу можно оценить как среднее по деревьям снижение критерия (например, индекса Джини), вызванное разбиениями по этому признаку:
\[
\text{Importance}(X_j) = \frac{1}{B} \sum_{b=1}^B \sum_{t \in T_b, \, \text{split\_feat}(t)=X_j} \Delta G(t).
\]

\subsection*{Смещённость и дисперсия}

Случайный лес уменьшает дисперсию по сравнению с одним деревом за счёт усреднения. Предполагая независимость ошибок:
\[
{\sigma^2}_{\text{RF}} \approx \frac{{\sigma^2}_{\text{tree}}}{B},
\]
где \({\sigma^2}_{\text{tree}}\) — дисперсия предсказания одного дерева. При этом смещённость ($b$) ансамбля в целом остаётся на уровне смещённости базовых моделей.

\subsection*{Задача 1:}

\textbf{Условие:} Пусть случайный лес состоит из \(B = 101\) деревьев. Каждое дерево независимо допускает ошибку классификации с вероятностью \(p = 0.3\). Найдите вероятность того, что случайный лес (использующий правило большинства) даст неправильный ответ при классификации объекта.

\textbf{Решение:} 
Для ошибки леса необходимо, чтобы более половины деревьев дали неправильный ответ. Поскольку \(B = 101\), более половины — это минимум \(51\) дерево. Обозначим случайную величину \(X\) — число деревьев, ошибающихся в классификации. Тогда:
\[
X \sim \text{Binomial}(B=101, p=0.3).
\]

Нужно найти:
\[
P(X \geq 51) = \sum_{k=51}^{101} \binom{101}{k} (0.3)^k (0.7)^{101-k}.
\]

Для наглядной оценки можно используем нормальное приближение биномиального распределения:
\[
\mu = Bp = 101 \times 0.3 = 30.3, \quad \sigma^2 = Bp(1-p) = 101 \times 0.3 \times 0.7 \approx 21.21, \quad \sigma \approx 4.61.
\]

Граница \(51\) сильно превышает \(\mu=30.3\), то есть мы ищем маловероятное событие. При точном вычислении получаем приближенную оценку:
\[
P(X \geq 51) \approx 4.46 \times 10^{-10} \quad (\text{очень малая вероятность}).
\]

Таким образом, вероятность того, что случайный лес с такими параметрами ошибётся, чрезвычайно мала.

\textbf{Ответ:} Вероятность ошибки случайного леса примерно равна \(4.46 \times 10^{-10}\), что фактически близко к нулю.


\subsection*{Задача 2}

\textbf{Условие:} Пусть в случайном лесе из \(B = 100\) деревьев признак \(X_1\) используется для разбиений в сумме 20 раз в различных узлах по всем деревьям. Каждое разбиение по \(X_1\) даёт снижение индекса Джини на \(\Delta G(t) = 0.01\). Остальные признаки либо не используются, либо их вкладом пренебрегаем. Найдите оценку важности признака \(X_1\).

\textbf{Решение:} Используя формулу важности:
\[
\text{Importance}(X_1) = \frac{1}{B} \sum_{b=1}^B \sum_{t \in T_b, \text{split\_feat}(t)=X_1} \Delta G(t).
\]

При \(B = 100\) сумма уменьшений по всем деревьям:
\[
\sum_{b=1}^B \sum_{t: X_1} \Delta G(t) = 20 \times 0.01 = 0.2.
\]

Тогда:
\[
\text{Importance}(X_1) = \frac{0.2}{100} = 0.002.
\]

\textbf{Ответ:} Важность признака \(X_1 = 0.002.\)


\subsection*{Задача 3:}

\textbf{Условие:} Пусть для одного решающего дерева смещённость \(b = 0.2\) и дисперсия \(\sigma^2 = 0.1\). Рассмотрим случайный лес из \(B = 100\) деревьев, построенный методом бэггинга, и предположим независимость ошибок между деревьями. Найдите смещённость и дисперсию этого случайного леса.

\textbf{Решение:}
\begin{enumerate}
    \item Смещённость не изменяется при усреднении, так как математическое ожидание предсказаний остаётся тем же:
    \[
    b_{\text{RF}} = b = 0.2.
    \]
    \item Дисперсия при усреднении \(B = 100\) независимых моделей уменьшается в \(100\) раз:
    \[
    {\sigma^2}_{\text{RF}} = \frac{\sigma^2}{B} = \frac{0.1}{100} = 0.001.
    \]
\end{enumerate}

\textbf{Ответ:} Для случайного леса из 100 деревьев смещённость остаётся \(b_{\text{RF}}=0.2\), а дисперсия становится \({\sigma^2}_{\text{RF}}=0.001\).



\section{Mixture of Experts (Смесь экспертов)}

Модель \textit{Смеси экспертов} (Квазилинейный ансамбль, Mixture of Experts, MoE) является архитектурой машинного обучения, которая сочетает в себе несколько моделей (экспертов) для решения сложных задач. Основная идея заключается в том, чтобы разделить пространство входных данных на части, в каждом из которых определенный эксперт специализируется. Общая модель обучается так, чтобы комбинировать выходы экспертов с учетом их специализации.

Математически, выход модели MoE для признакового описания объекта $\mathbf{x}$ может быть представлен как:

$$
y = \sum_{k=1}^{K} g_k(\mathbf{x}) f_k(\mathbf{x}),
$$

где:
\begin{itemize}
    \item $K$ — количество экспертов,
    \item $f_k(\mathbf{x})$ — локальная модель, функция $k$-го эксперта, производящая прогноз,
    \item $g_k(\mathbf{x})$ — шлюзовая функция или функция компетентности, определяющая вес вклада $k$-го эксперта, причём $\sum_{k=1}^{K} g_k(\mathbf{x}) = 1$ и $g_k(\mathbf{x}) \geq 0$ для всех $k$.
\end{itemize}

Шлюзовая (Gate) функция обычно моделируется с помощью функции softmax

$$
g_k(\mathbf{x}) = \frac{\exp(h_k(\mathbf{x}))}{\sum_{j=1}^{K} \exp(h_j(\mathbf{x}))},
$$

где $h_k(\mathbf{x})$ — функция компетентности для $k$-го эксперта. Выбираются из каких-либо содержательных соображений.

Преимущество MoE заключается в способности моделировать сложные зависимости путем разделения задачи между специализированными экспертами, что может улучшить обобщающую способность и эффективность обучения.

\subsection{Задачи и решения}

\subsubsection{Задача 1}

Пусть имеется модель MoE с двумя экспертами, функции которых заданы как $f_1(x) = 2x$ и $f_2(x) = x^2$. Функции $h_k (x):$ $h_1 (x) = -x$, $h_2 (x) = x$.

Требуется найти выражение для общего выхода модели $y$ в зависимости от $x$.

\textbf{Решение:}

Шлюзовая функция моделируется как:

$$
g_1(x) = \frac{\exp(-x)}{\exp(-x) + \exp(x)}, \quad g_2(x) = \frac{\exp(x)}{\exp(-x) + \exp(x)}.
$$

Суммарный выход модели:

$$
y = g_1(x) f_1(x) + g_2(x) f_2(x) = g_1(x) \cdot 2x + g_2(x) \cdot x^2.
$$

Подставим выражения для $g_1(x)$ и $g_2(x)$:

$$
y = \frac{\exp(-x)}{\exp(-x) + \exp(x)} \cdot 2x + \frac{\exp(x)}{\exp(-x) + \exp(x)} \cdot x^2.
$$

Поэтому итоговое выражение для $y$:

$$
y = \frac{\exp(-x) \cdot 2x + \exp(x) \cdot x^2}{\exp(-x) + \exp(x)}.
$$

\subsubsection{Задача 2}

Рассмотрим модель MoE, где шлюзовая функция выбирает только одного эксперта в зависимости от знака $x$:
$$
g_1(x) = 
\begin{cases} 
1, & \text{если } x \geq 0, \\ 
0, & \text{если } x < 0,
\end{cases}
$$
$$
g_2(x) = 
\begin{cases} 
0, & \text{если } x \geq 0, \\ 
1, & \text{если } x < 0.
\end{cases}
$$
Функции экспертов заданы как $f_1(x) = x^2$ и $f_2(x) = -x$. Найдите общий выход модели $y$ для любого $x$.

\textbf{Решение:}

Поскольку в каждый момент времени активен только один эксперт, общий выход модели определяется функцией активного эксперта.

Для $x \geq 0$: 
$$
g_1(x) = 1, \quad g_2(x) = 0, \quad y = g_1(x) f_1(x) = x^2.
$$

Для $x < 0$: 
$$
g_1(x) = 0, \quad g_2(x) = 1, \quad y = g_2(x) f_2(x) = -x.
$$

Таким образом, общий выход модели равен:
$$
y = 
\begin{cases} 
x^2, & \text{если } x \geq 0, \\ 
-x, & \text{если } x < 0.
\end{cases}
$$
Это означает, что модель ведет себя по-разному на положительных и отрицательных значениях $x$, отражая специализацию экспертов на разных областях входных данных.

\subsubsection{Задача 3}

Рассмотрим модель Mixture of Experts, состоящую из двух экспертных моделей $f_1$ и $f_2$, а также шлюзовой функции $g$. Пусть на вход подаётся одно признаковое значение $x$. Выражения для выходов моделей заданы следующим образом:

$$
f_1(x) = w_1 x + b_1, \quad f_2(x) = w_2 x + b_2, \quad g(x) = \sigma(v x + c)
$$

где $\sigma(z) = \frac{1}{1 + e^{-z}}$ — сигмоидальная функция активации, а $w_1, w_2, b_1, b_2, v, c$ — параметры модели.

Выход всей модели MoE определяется как:

$$
y(x) = g(x) f_1(x) + (1 - g(x)) f_2(x)
$$

\textbf{Вопрос:} Предположим, что при $x = 0$ мы наблюдаем, что $y(0) = 1$, $f_1(0) = 1$, $f_2(0) = 3$. Найдите значение $g(0)$.

\textbf{Решение:}

Из условия задачи при $x = 0$ имеем:

$$
y(0) = g(0) \cdot f_1(0) + (1 - g(0)) \cdot f_2(0) = g(0) \cdot 1 + (1 - g(0)) \cdot 3
$$

Подставляем известное значение $y(0) = 1$:

$$
g(0) \cdot 1 + (1 - g(0)) \cdot 3 = 1
$$
$$
g(0) + 3 - 3g(0) = 1
$$
$$
-2g(0) + 3 = 1
$$
$$
-2g(0) = -2
$$
$$
g(0) = 1
$$

Таким образом, $g(0) = 1$.

\section{Блендинг}
Ансамблирование (ensemble methods) --- способ улучшить точность и устойчивость предсказаний за счёт комбинирования нескольких моделей машинного обучения. Отдельно взятая модель может иметь свои слабые стороны и распределённые ошибки, в то время как ансамбль (например, суммарное взвешенное предсказание нескольких базовых моделей) позволяет компенсировать ошибки отдельных участников и повысить итоговое качество.

Среди распространённых методов ансамблирования выделяют:
\begin{itemize}
    \item \textbf{Баггинг (Bagging)}: параллельное обучение нескольких независимых моделей с последующим усреднением или голосованием;
    \item \textbf{Бустинг (Boosting)}: последовательное обучение ``слабых'' моделей, где каждая следующая фокусируется на ошибках предыдущей;
    \item \textbf{Стэкинг (Stacking)}: построение ``мета-модели'' на предиктах базовых алгоритмов, обученных в $k$-fold-схеме;
    \item \textbf{Блендинг (Blending)}: способ объединения моделей, близкий к стэкингу, но использующий простое деление обучающих данных на train/hold-out, где hold-out выборка служит для определения весов или правил объединения.
\end{itemize}

В данной статье мы сосредоточимся на \textbf{блендинге} и рассмотрим его теоретические аспекты.

\section{Метод блендинга: формулировка}
\subsection{Общая схема}
В \textbf{blending} обучающую выборку делят на две части:
\begin{enumerate}
    \item \textbf{Основная (train)} --- для обучения базовых моделей;
    \item \textbf{Hold-out} --- небольшое подмножество, \textit{не} участвующее в обучении базовых моделей.
\end{enumerate}

Далее:
\begin{enumerate}
    \item Тренируем базовые модели на \textbf{train} части.
    \item Для каждого объекта из \textbf{hold-out} получаем предсказания от каждой базовой модели.
    \item По предсказаниям (например, по MSE или логистической потере) подбираем оптимальные веса $w_i$ или параметры для объединения.  
\end{enumerate}

Финальное предсказание --- это взвешенная комбинация выходов базовых моделей:
\[
\hat{y} = \sum_{i=1}^n w_i \cdot \hat{y}_i.
\]

\textbf{Отличие от стэкинга} в том, что здесь чаще всего берётся \textbf{одна} hold-out выборка, а не $k$ фолдов, и итоговая ``мета-модель'' упрощена до простой линейной (или другой) схемы, которая не обучается на кросс-валидации и не использует всю обучающую выборку для ``второго уровня''.

\section{Теоретические аспекты блендинга}
\subsection{Байесовская интерпретация и усреднение гипотез}
С позиции байесовского подхода, каждая модель $M_i$ может рассматриваться как гипотеза с апостериорной вероятностью $p(M_i \mid D)$, где $D$ --- обучающие данные. Если наши модели дают предсказания вероятности $P(y=1 \mid x, M_i)$, то оптимальным в смысле минимизации байесовского риска считается байесовское усреднение по всем гипотезам:
\[
p(y=1 \mid x, D) = \sum_{i=1}^n p(M_i \mid D) \, P(y=1 \mid x, M_i).
\]
Блендинг может рассматриваться как \textit{эмпирическое} приближение этого байесовского усреднения, где веса $w_i$ являются оценками $p(M_i \mid D_{holdout})$ на hold-out выборке.

\subsection{Оптимизация весов: линейная регрессия или минимизация ошибки}
На \textbf{hold-out} выборке $\{(x_j, y_j)\}$, $j=1,\ldots,m$, мы имеем предсказания базовых моделей $ \{\hat{y}_{1j}, \hat{y}_{2j}, \dots, \hat{y}_{nj}\}$ для $n$ моделей. Цель --- найти вектор весов $\mathbf{w} = (w_1, \ldots, w_n)^\top$, который минимизирует заданный функционал ошибки $L$ (MSE, MAE, логистическая потеря и пр.):
\[
\mathbf{w}^* = \arg\min_{\mathbf{w}} \sum_{j=1}^m L\Bigl(y_j,\ \sum_{i=1}^n w_i \hat{y}_{ij}\Bigr).
\]
В самом простом случае с MSE эта задача сводится к линейной регрессии (без регуляризации или с L2-регуляризацией). Однако можно решать её прямыми методами оптимизации (например, градиентными методами) --- при желании, даже нелинейно (но тогда это ближе к ``мини-стэкингу'').

\subsection{Корреляция ошибок и выигрыш от блендинга}
Теоретический выигрыш от ансамблирования (в частности, блендинга) связан с \textbf{корреляцией ошибок} базовых моделей. Если две модели сильно коррелированы по ошибкам, их усреднение мало что даст. Если же они допускают разные ошибки (низкая корреляция остаточных ошибок), то их комбинация может существенно улучшить итог.

\subsection{Bias-Variance разложение для ансамбля}
В рамках bias-variance разложения (применяя для простоты MSE-подход), можно показать, что \textbf{дисперсия} ансамбля может уменьшаться при условии, что ошибки различных базовых моделей не совпадают. При этом совместный \textit{bias} ансамбля может быть ниже или ниже/сопоставим с bias отдельно взятой модели, в зависимости от того, как подбираются веса. 

\subsection{Проблема ``утечки данных'' (data leakage)}
\textbf{Data leakage} возникает, когда hold-out выборка не изолирована от данных, которые использовались для обучения базовых моделей. Если те же данные фигурируют и при обучении, и при подборе весов, ансамбль может ``запомнить'' специфические паттерны, характерные для train. Это приводит к \textit{оптимистичной} оценке на hold-out и ухудшению генерализации на настоящем тесте. Правильная схема blending предполагает, что hold-out часть полностью не пересекается с train, и базовые модели никогда не видят hold-out при обучении.

\subsection{Ограничения блендинга}
\begin{itemize}
    \item \textbf{Неэффективное использование данных}: часть выборки ``простаивает'' в hold-out, не используется при обучении базовых моделей.
    \item \textbf{Малая hold-out выборка}: низкая точность оценки оптимальных весов, особенно если базовых моделей много.
    \item \textbf{Нет глубокой интеграции}: обычно используется простая схема (линейная комбинация) и одна разделённая выборка, что может быть менее гибким, чем полноценный стэкинг с $k$-fold и сложной мета-моделью.
\end{itemize}

\section{Теоретические задачи и их решения}

Ниже представлены три \textbf{теоретические} задачи, углубляющиеся в аспекты блендинга.

\subsection{Теоретическая Задача 1: Байесовская трактовка усреднения моделей}
Допустим, у нас есть набор моделей $\{M_1, ..., M_n\}$, каждая из которых оценивает вероятность события $P(y=1 \mid x)$ в задаче бинарной классификации. Покажите, что простое взвешенное усреднение предсказаний моделей может быть приближением к байесовскому усреднению гипотез. Каким образом оцениваются веса $\{w_i\}$ с байесовской точки зрения?

\begin{itemize}
    \item В байесовском подходе модель $M_i$ рассматривается как гипотеза с некоторой апостериорной вероятностью $p(M_i \mid D)$.
    \item Финальное предсказание апостериорной вероятности события --- это взвешенная сумма предсказаний базовых гипотез, где в качестве весов выступают $p(M_i\mid D)$.
    \item В реальной практике мы не имеем явного байесовского вывода для $p(M_i\mid D)$, но можем приблизить эти вероятности путём оптимизации весов на hold-out (например, минимизируя логистическую потерю). Фактически $w_i$ служат оценкой ``доверия'' к гипотезе $M_i$.
\end{itemize}

\subsection{Теоретическая Задача 2: Обоснование полезности блендинга через уменьшение дисперсии}
Рассмотрите две модели $M_1$ и $M_2$, прогнозирующие вещественную переменную (регрессия). Пусть ошибки каждой модели представляют собой случайные величины $\epsilon_1$ и $\epsilon_2$ с нулевым средним. Покажите, что дисперсия ансамбля (при простом равновесном усреднении) убывает, если ошибки $\epsilon_1$ и $\epsilon_2$ не идеально коррелированы.

\begin{itemize}
    \item Финальное предсказание блендинга: $\hat{y}_{ens} = \frac{1}{2}\hat{y}_1 + \frac{1}{2}\hat{y}_2$.
    \item Ошибка ансамбля: $\epsilon_{ens} = \frac{1}{2}\epsilon_1 + \frac{1}{2}\epsilon_2$.
    \item Дисперсия ошибки ансамбля: 
    \[
    \mathrm{Var}(\epsilon_{ens}) = \mathrm{Var}\left(\frac{\epsilon_1}{2} + \frac{\epsilon_2}{2}\right) = \frac{1}{4}\mathrm{Var}(\epsilon_1) + \frac{1}{4}\mathrm{Var}(\epsilon_2) + \frac{1}{2}\mathrm{Cov}(\epsilon_1,\epsilon_2).
    \]
    \item Если $\rho$ --- коэффициент корреляции между $\epsilon_1$ и $\epsilon_2$, то
    \[
    \mathrm{Var}(\epsilon_{ens}) = \frac{1}{4}\bigl(\mathrm{Var}(\epsilon_1) + \mathrm{Var}(\epsilon_2) + 2\rho\sqrt{\mathrm{Var}(\epsilon_1)\mathrm{Var}(\epsilon_2)}\bigr).
    \]
    \item При $\rho<1$ данная величина меньше $\frac{1}{2}(\mathrm{Var}(\epsilon_1) + \mathrm{Var}(\epsilon_2))$, то есть дисперсия снижается. Максимальный выигрыш достигается, когда корреляция отрицательна или нулевая.
\end{itemize}

\subsection{Теоретическая Задача 3: Использование hold-out при малом количестве данных}
Пусть у вас есть небольшой датасет объёмом $N$, и вы решили применить блендинг: отложить $\alpha N$ объектов как hold-out. Каковы потенциальные проблемы при слишком большой или слишком маленькой доле $\alpha$? Как это может сказаться на результатах ансамбля?

\begin{itemize}
    \item \textbf{Слишком маленькая hold-out} (малая $\alpha$): веса $w_i$ будут определены неустойчиво (большая дисперсия оценки), возможна пере/недо-настройка весов.  
    \item \textbf{Слишком большая hold-out} (большая $\alpha$): уменьшается объём train для базовых моделей, они становятся менее обученными (большая ошибка моделей), итоговое качество ансамбля может упасть.
    \item Есть компромисс: $\alpha$ не должна быть слишком мала, чтобы надёжно оценить веса, но и не слишком велика, чтобы не ``обеднять'' обучение базовых моделей.
    \item При крайне маленьких объёмах данных классический блендинг может быть неэффективен; альтернативой является полноценный стэкинг с $k$-fold, где весь объём задействован более рационально.
\end{itemize}
